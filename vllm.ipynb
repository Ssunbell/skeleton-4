{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Objectives**\n",
                "\n",
                "1. ì‹¤ìŠµëª… : vllmìœ¼ë¡œ ì„œë¹™í•˜ê¸°ê¸°\n",
                "2. í•µì‹¬ ì£¼ì œ:\n",
                "    1. vllm í•µì‹¬ ê°œë… ì´í•´(PagedAttention, KV Cache)\n",
                "    2. vLLM ì„œë¹™ ì½”ë“œ ë° LoRA adapter ì‚¬ìš©ìš©\n",
                "    3. Transformers vs vLLM ì„±ëŠ¥ ë¹„êµ\n",
                "3. í•™ìŠµ ëª©í‘œ :\n",
                "    1. vllmì˜ í•µì‹¬ ê°œë…ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n",
                "    2. vllmì˜ ì„œë¹™ ì½”ë“œë¥¼ ì´í•´í•˜ê³  ëª¨ë¸ì„ ì„œë¹™í•  ìˆ˜ ìˆë‹¤.\n",
                "4. í•™ìŠµ ê°œë…: í‚¤ì›Œë“œëª… :\n",
                "    1. page attention\n",
                "    2. \n",
                "    3. QLoRA fine-tuning\n",
                "5. í•™ìŠµ ë°©í–¥ :\n",
                "  - llm ëª¨ë¸ì„ ì„œë¹™í•˜ëŠ”ë° ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ê³  ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ vllm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\n",
                "  - ì‹¤ìŠµ ì½”ë“œëŠ” ì¡°êµê°€ ì§ì ‘ êµ¬í˜„í•œ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.\n",
                "  - í•´ë‹¹ ì‹¤ìŠµì€ vllmì„ ì‚¬ìš©í–ˆì„ë•Œ ì†ë„ ì°¨ì´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Prerequisites**\n",
                "```\n",
                "numpy==2.1.0\n",
                "pandas==2.2.3\n",
                "transformers==4.56.0\n",
                "torch==2.8.0+cu126\n",
                "accelerate==1.10.1\n",
                "bitsandbytes==0.49.1\n",
                "datasets==4.0.0\n",
                "peft==0.17.1\n",
                "vllm==0.11.0\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ë„¤, ì´ë²ˆì—ëŠ” ì‹œë‹ˆì–´ ì—”ì§€ë‹ˆì–´ë‚˜ ì•„í‚¤í…íŠ¸ê°€ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì‘ì„±í•˜ê±°ë‚˜ íŒ€ ë‚´ ê¸°ìˆ  ì„¸ë¯¸ë‚˜ë¥¼ ì§„í–‰í•  ë•Œ ì‚¬ìš©í•˜ëŠ”, **ì „ë¬¸ì ì´ê³  ë°€ë„ ë†’ì€ í†¤**ìœ¼ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ë¹„ìœ ëŠ” ì¤„ì´ê³  ê¸°ìˆ ì  ìš©ì–´(Technical Terminology)ë¥¼ ëª…í™•íˆ ì‚¬ìš©í•˜ì—¬ ì‹ ë¢°ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤.\n",
                "\n",
                "---\n",
                "\n",
                "## 1: vLLM ì•„í‚¤í…ì²˜ ë° í•µì‹¬ ì›ë¦¬\n",
                "        \n",
                "### 1.1 ë„ì… ë°°ê²½ (Why vLLM?)\n",
                "\n",
                "ê¸°ì¡´ HuggingFace Transformers ê¸°ë°˜ ì„œë¹™ ë°©ì‹ì€ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë³‘ëª©(Bottleneck)ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
                "\n",
                "* **ë©”ëª¨ë¦¬ ë‹¨í¸í™”(Fragmentation)**: KV Cacheë¥¼ ì—°ì†ëœ ë©”ëª¨ë¦¬ ê³µê°„ì— í• ë‹¹í•´ì•¼ í•˜ë¯€ë¡œ, ì‹¤ì œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì˜ˆì•½ ë©”ëª¨ë¦¬(Internal Fragmentation)ë¡œ ì¸í•œ VRAM ë‚­ë¹„ê°€ ì‹¬ê°í•©ë‹ˆë‹¤.\n",
                "* **ë‚®ì€ ì²˜ë¦¬ëŸ‰(Throughput)**: Static Batching ë°©ì‹ì˜ ë¹„íš¨ìœ¨ì„±ìœ¼ë¡œ ì¸í•´ ë™ì‹œ ì ‘ì† ì²˜ë¦¬ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.\n",
                "\n",
                "**vLLMì˜ ì†”ë£¨ì…˜:**\n",
                "\n",
                "* **PagedAttention**: OSì˜ ê°€ìƒ ë©”ëª¨ë¦¬(Virtual Memory) ê¸°ë²•ì„ ì°¨ìš©í•˜ì—¬ KV Cacheë¥¼ ë¹„ì—°ì†ì  ë¸”ë¡ìœ¼ë¡œ ê´€ë¦¬, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.\n",
                "* **Continuous Batching**: ìš”ì²­ì´ ë“¤ì–´ì˜¤ëŠ” ì¦‰ì‹œ ë™ì ìœ¼ë¡œ ë°°ì¹˜ì— í¬í•¨ì‹œì¼œ GPU ê°€ë™ë¥ (Utilization)ì„ ë†’ì…ë‹ˆë‹¤.\n",
                "\n",
                "### 1.2 í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ 1: PagedAttention\n",
                "\n",
                "PagedAttentionì€ vLLM ì„±ëŠ¥ í–¥ìƒì˜ í•µì‹¬ìœ¼ë¡œ, ìš´ì˜ì²´ì œì˜ í˜ì´ì§•(Paging) ì‹œìŠ¤í…œê³¼ ë™ì¼í•œ ì›ë¦¬ì…ë‹ˆë‹¤.\n",
                "\n",
                "**Legacy ë°©ì‹ (Contiguous Allocation)**:\n",
                "\n",
                "* ìš”ì²­ ì‹œ ìµœëŒ€ í† í° ê¸¸ì´(Max Context Length)ë§Œí¼ì˜ ì—°ì†ëœ ë©”ëª¨ë¦¬ë¥¼ ë¯¸ë¦¬ í™•ë³´í•©ë‹ˆë‹¤.\n",
                "* ìƒì„±ë˜ì§€ ì•Šì€ ë¯¸ë˜ì˜ í† í°ì„ ìœ„í•œ ê³µê°„ê¹Œì§€ ì ìœ í•˜ë¯€ë¡œ **ë©”ëª¨ë¦¬ ë‚­ë¹„(Waste)**ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
                "\n",
                "```\n",
                "Physical Memory: [Token 1...Token N | Reserved (Unused) Space]\n",
                "-> ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë° OOM(Out of Memory)ì˜ ì£¼ì›ì¸\n",
                "\n",
                "```\n",
                "\n",
                "**PagedAttention ë°©ì‹ (Non-contiguous Allocation)**:\n",
                "\n",
                "* KV Cacheë¥¼ ê³ ì • í¬ê¸°ì˜ 'ë¸”ë¡(Block)' ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
                "* ë¬¼ë¦¬ì ìœ¼ë¡œ ë–¨ì–´ì§„ ë©”ëª¨ë¦¬ ê³µê°„ì´ë¼ë„ **Block Table**ì„ í†µí•´ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ì†ëœ ê²ƒì²˜ëŸ¼ ë§¤í•‘í•©ë‹ˆë‹¤.\n",
                "* í•„ìš”í•  ë•Œë§Œ ë¸”ë¡ì„ ë™ì  í• ë‹¹í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ë‚­ë¹„ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤(Near-zero waste).\n",
                "\n",
                "```\n",
                "Block 0 (Physical): [Token 0-3]\n",
                "Block 1 (Physical): [Token 4-7]\n",
                "Block Table (Logical): {Seq_A: [Block 0, Block 1, ...]}\n",
                "-> ë¹„ì—°ì†ì  í• ë‹¹ ê°€ëŠ¥, ìœ ì—°í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
                "\n",
                "```\n",
                "\n",
                "### 1.3 í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ 2: KV Cache ìµœì í™”\n",
                "\n",
                "LLMì€ ìê¸°íšŒê·€(Autoregressive) ëª¨ë¸ì´ë¯€ë¡œ, ì´ì „ í† í°ë“¤ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
                "\n",
                "**KV Cache ë¶€ì¬ ì‹œ (Redundant Computation)**:\n",
                "\n",
                "* ë§¤ ìŠ¤í…ë§ˆë‹¤ ì´ì „ ì‹œì ()ê¹Œì§€ì˜ ëª¨ë“  í† í°ì— ëŒ€í•´ Attention ì—°ì‚°ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
                "* ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì—°ì‚°ëŸ‰ì´ $O(N^2)$ë¡œ ì¦ê°€í•˜ì—¬ Latencyê°€ ê¸‰ê²©íˆ ë‚˜ë¹ ì§‘ë‹ˆë‹¤.\n",
                "\n",
                "**KV Cache ì ìš© ì‹œ (Memory-Space Tradeoff)**:\n",
                "\n",
                "* ì´ì „ í† í°ë“¤ì˜ Key, Value ë²¡í„°ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ VRAMì— ìºì‹±í•´ë‘¡ë‹ˆë‹¤.\n",
                "* í˜„ì¬ ì‹œì ()ì˜ í† í°ë§Œ ì—°ì‚°í•˜ê³ , ê³¼ê±° ë°ì´í„°ëŠ” ìºì‹œë¥¼ ì°¸ì¡°(Lookup)í•˜ì—¬ ì—°ì‚° ë³µì¡ë„ë¥¼ ì„ í˜•ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.\n",
                "\n",
                "**Critical Issue**:\n",
                "\n",
                "* KV CacheëŠ” ëª¨ë¸ ì‚¬ì´ì¦ˆì™€ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë¹„ë¡€í•˜ì—¬ ë§¤ìš° í° VRAMì„ ì ìœ í•©ë‹ˆë‹¤.\n",
                "* ê²°êµ­ **\"ì œí•œëœ GPU ë©”ëª¨ë¦¬ì— ì–¼ë§ˆë‚˜ ë§ì€ KV Cacheë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ê²¨ ë„£ëŠëƒ\"**ê°€ ì„œë¹™ ì„±ëŠ¥(Throughput)ì˜ ê²°ì •ì  ìš”ì†Œê°€ ë˜ë©°, ì´ê²ƒì´ vLLMì´ PagedAttentionì„ ë„ì…í•œ ê¸°ìˆ ì  ë°°ê²½ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 01-19 23:58:20 [__init__.py:216] Automatically detected platform cuda.\n",
                        "PyTorch ë²„ì „: 2.8.0+cu126\n",
                        "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
                        "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
                    ]
                }
            ],
            "source": [
                "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
                "import os\n",
                "import time\n",
                "import torch\n",
                "from vllm import LLM, SamplingParams\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
                "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.4: PagedAttention ë©”ì»¤ë‹ˆì¦˜ ì‹œë®¬ë ˆì´ì…˜\n",
                "\n",
                "ì´ë¡ ì  ê°œë…ì„ êµ¬ì²´í™”í•˜ê¸° ìœ„í•´, Python í™˜ê²½ì—ì„œ **PagedAttentionì˜ ë©”ëª¨ë¦¬ í• ë‹¹ ë¡œì§**ì„ ì‹œë®¬ë ˆì´ì…˜í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°€ìƒ ë©”ëª¨ë¦¬ ì£¼ì†Œê°€ ì‹¤ì œ GPU ë©”ëª¨ë¦¬ì— ë§¤í•‘ë˜ëŠ” ê³¼ì •ì„ ì½”ë“œ ë ˆë²¨ì—ì„œ ê²€ì¦í•©ë‹ˆë‹¤.\n",
                "\n",
                "### ì‹œë®¬ë ˆì´í„° í•µì‹¬ ì»´í¬ë„ŒíŠ¸ (Simulator Components)\n",
                "\n",
                "* **Logical Memory (Virtual Address Space)**\n",
                "  * ëª¨ë¸ì´ ì¸ì‹í•˜ëŠ” ë…¼ë¦¬ì ì¸ í† í° ì‹œí€€ìŠ¤ì…ë‹ˆë‹¤.\n",
                "  * ì‚¬ìš©ì ì…ì¥ì—ì„œëŠ” ë°ì´í„°ê°€ ì—°ì†ì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* **Physical Memory (Physical Address Space)**\n",
                "  * ì‹¤ì œ KV Cache ë°ì´í„°ê°€ ì ì¬ë˜ëŠ” ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ ê³µê°„(VRAM)ì…ë‹ˆë‹¤.\n",
                "  * ë¸”ë¡(Block) ë‹¨ìœ„ë¡œ ë¹„ì—°ì†ì ìœ¼ë¡œ í• ë‹¹ ë° ê´€ë¦¬ë©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* **Block Table (Address Translation)**\n",
                "  * Logical Blockì„ Physical Blockìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë§¤í•‘ í…Œì´ë¸”ì…ë‹ˆë‹¤.\n",
                "  * OSì˜ í˜ì´ì§€ í…Œì´ë¸”(Page Table)ê³¼ ë™ì¼í•œ ì—­í• ì„ ìˆ˜í–‰í•˜ì—¬ ë©”ëª¨ë¦¬ ì°¸ì¡°ë¥¼ ì¤‘ê°œí•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… PagedAttention ì‹œë®¬ë ˆì´í„° í´ë˜ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "import random\n",
                "from typing import List, Dict, Optional\n",
                "\n",
                "class PhysicalBlock:\n",
                "    \"\"\"\n",
                "    ë¬¼ë¦¬ì ì¸ ë©”ëª¨ë¦¬ ë¸”ë¡ (GPUì˜ ì‘ì€ ì¡°ê°)\n",
                "    ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ í•˜ë‚˜ì˜ ë¸”ë¡ì— 4ê°œì˜ í† í°ì„ ì €ì¥í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤ (BLOCK_SIZE = 4).\n",
                "    \"\"\"\n",
                "    def __init__(self, block_id: int, block_size: int = 4):\n",
                "        self.block_id = block_id\n",
                "        self.size = block_size\n",
                "        self.data = [None] * block_size  # ì²˜ìŒì—” ë¹„ì–´ìˆìŒ\n",
                "        self.filled_count = 0\n",
                "\n",
                "    def append(self, token: str) -> bool:\n",
                "        if self.filled_count < self.size:\n",
                "            self.data[self.filled_count] = token\n",
                "            self.filled_count += 1\n",
                "            return True  # ì €ì¥ ì„±ê³µ\n",
                "        return False  # ê½‰ ì°¨ì„œ ì‹¤íŒ¨\n",
                "\n",
                "    def __repr__(self):\n",
                "        return f\"[Block {self.block_id}]: {self.data}\"\n",
                "\n",
                "class VLLMMemoryManager:\n",
                "    \"\"\"\n",
                "    vLLMì˜ í•µì‹¬ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì (OS ì—­í• )\n",
                "    \"\"\"\n",
                "    def __init__(self, total_blocks: int = 16, block_size: int = 4):\n",
                "        self.block_size = block_size\n",
                "        # 1. ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ ê³µê°„ ìƒì„± (GPU VRAM)\n",
                "        self.physical_memory = [PhysicalBlock(i, block_size) for i in range(total_blocks)]\n",
                "        \n",
                "        # 2. ì‚¬ìš© ê°€ëŠ¥í•œ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ (Free List)\n",
                "        # ì‹¤ì œë¡œëŠ” ë¬¼ë¦¬ì ìœ¼ë¡œ ë–¨ì–´ì ¸ ìˆì–´ë„ ìƒê´€ì—†ìŒì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì„ìŠµë‹ˆë‹¤.\n",
                "        self.free_blocks = list(range(total_blocks))\n",
                "        random.shuffle(self.free_blocks) \n",
                "        \n",
                "        # 3. ìš”ì²­ë³„ ë¸”ë¡ ë§¤í•‘ í…Œì´ë¸” (Request ID -> List[Physical Block IDs])\n",
                "        self.block_tables: Dict[str, List[int]] = {}\n",
                "\n",
                "    def allocate_block(self) -> Optional[int]:\n",
                "        \"\"\"ë¹ˆ ë¬¼ë¦¬ ë¸”ë¡ í•˜ë‚˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
                "        if not self.free_blocks:\n",
                "            return None # OOM (Out of Memory)\n",
                "        return self.free_blocks.pop()\n",
                "\n",
                "    def append_token(self, request_id: str, token: str):\n",
                "        \"\"\"\n",
                "        í•µì‹¬ ë¡œì§: í† í°ì„ ìƒì„±í•´ì„œ KV Cacheì— ì¶”ê°€í•˜ëŠ” ê³¼ì •\n",
                "        \"\"\"\n",
                "        # 1. í•´ë‹¹ ìš”ì²­ì˜ ë¸”ë¡ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±\n",
                "        if request_id not in self.block_tables:\n",
                "            print(f\"--- [New Request] '{request_id}' ì‹œì‘ ---\")\n",
                "            self.block_tables[request_id] = []\n",
                "\n",
                "        # 2. í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë§ˆì§€ë§‰ ë¬¼ë¦¬ ë¸”ë¡ í™•ì¸\n",
                "        current_block_id = None\n",
                "        if self.block_tables[request_id]:\n",
                "            current_block_id = self.block_tables[request_id][-1]\n",
                "        \n",
                "        # 3. ë¸”ë¡ì´ ì—†ê±°ë‚˜ ê½‰ ì°¼ìœ¼ë©´, ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ í• ë‹¹ (PagedAttentionì˜ í•µì‹¬!)\n",
                "        if current_block_id is None or \\\n",
                "           self.physical_memory[current_block_id].filled_count >= self.block_size:\n",
                "            \n",
                "            new_block_id = self.allocate_block()\n",
                "            if new_block_id is None:\n",
                "                raise Exception(\"GPU Memory Full!\")\n",
                "            \n",
                "            self.block_tables[request_id].append(new_block_id)\n",
                "            current_block_id = new_block_id\n",
                "            print(f\"ğŸ‘‰ [Alloc] ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ {current_block_id}ë²ˆ í• ë‹¹ë¨\")\n",
                "\n",
                "        # 4. ë¬¼ë¦¬ ë¸”ë¡ì— ë°ì´í„° ì €ì¥\n",
                "        self.physical_memory[current_block_id].append(token)\n",
                "        print(f\"   [Write] í† í° '{token}' -> ë¬¼ë¦¬ ë¸”ë¡ {current_block_id}ë²ˆì— ì €ì¥\")\n",
                "\n",
                "    def print_state(self, request_id: str):\n",
                "        \"\"\"í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ ì‹œê°í™”í•´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.\"\"\"\n",
                "        print(f\"\\nğŸ“Š [{request_id}] ì˜ PagedAttention ìƒíƒœ\")\n",
                "        table = self.block_tables.get(request_id, [])\n",
                "        \n",
                "        # 1. Logical View (ì‚¬ìš©ìê°€ ë³´ëŠ” ë¬¸ì¥)\n",
                "        logical_text = []\n",
                "        for block_id in table:\n",
                "            block = self.physical_memory[block_id]\n",
                "            logical_text.extend([t for t in block.data if t is not None])\n",
                "        print(f\"  1) ë…¼ë¦¬ì  ë·° (Logical): {logical_text}\")\n",
                "\n",
                "        # 2. Block Table (ë§¤í•‘ ì •ë³´)\n",
                "        print(f\"  2) ë¸”ë¡ í…Œì´ë¸” (Mapping): {table}\")\n",
                "\n",
                "        # 3. Physical View (ì‹¤ì œ ì €ì¥ ìœ„ì¹˜)\n",
                "        print(f\"  3) ë¬¼ë¦¬ì  ë·° (Physical):\")\n",
                "        for block_id in table:\n",
                "            print(f\"     {self.physical_memory[block_id]}\")\n",
                "        print(\"-\" * 50)\n",
                "\n",
                "print(\"âœ… PagedAttention ì‹œë®¬ë ˆì´í„° í´ë˜ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸš€ í† í° ìƒì„± ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\n",
                        "\n",
                        "--- [New Request] 'User_A' ì‹œì‘ ---\n",
                        "ğŸ‘‰ [Alloc] ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ 5ë²ˆ í• ë‹¹ë¨\n",
                        "   [Write] í† í° 'Deep' -> ë¬¼ë¦¬ ë¸”ë¡ 5ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'Learn' -> ë¬¼ë¦¬ ë¸”ë¡ 5ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'ing' -> ë¬¼ë¦¬ ë¸”ë¡ 5ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'is' -> ë¬¼ë¦¬ ë¸”ë¡ 5ë²ˆì— ì €ì¥\n",
                        "ğŸ‘‰ [Alloc] ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ 2ë²ˆ í• ë‹¹ë¨\n",
                        "   [Write] í† í° 'very' -> ë¬¼ë¦¬ ë¸”ë¡ 2ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'fun' -> ë¬¼ë¦¬ ë¸”ë¡ 2ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'to' -> ë¬¼ë¦¬ ë¸”ë¡ 2ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'study' -> ë¬¼ë¦¬ ë¸”ë¡ 2ë²ˆì— ì €ì¥\n",
                        "\n",
                        "ğŸ“Š [User_A] ì˜ PagedAttention ìƒíƒœ\n",
                        "  1) ë…¼ë¦¬ì  ë·° (Logical): ['Deep', 'Learn', 'ing', 'is', 'very', 'fun', 'to', 'study']\n",
                        "  2) ë¸”ë¡ í…Œì´ë¸” (Mapping): [5, 2]\n",
                        "  3) ë¬¼ë¦¬ì  ë·° (Physical):\n",
                        "     [Block 5]: ['Deep', 'Learn', 'ing', 'is']\n",
                        "     [Block 2]: ['very', 'fun', 'to', 'study']\n",
                        "--------------------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "# PagedAttention ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\n",
                "# LLMì´ í•œ í† í°ì”© ìƒì„±í•˜ëŠ” ìƒí™©ì„ í‰ë‚´ëƒ…ë‹ˆë‹¤\n",
                "\n",
                "# 1. ë§¤ë‹ˆì € ì´ˆê¸°í™” (Block Size = 4)\n",
                "vllm_manager = VLLMMemoryManager(total_blocks=10, block_size=4)\n",
                "\n",
                "# 2. ë¬¸ì¥ ìƒì„± ì‹œë®¬ë ˆì´ì…˜\n",
                "req_id = \"User_A\"\n",
                "tokens = [\"Deep\", \"Learn\", \"ing\", \"is\", \"very\", \"fun\", \"to\", \"study\"]\n",
                "\n",
                "print(\"ğŸš€ í† í° ìƒì„± ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\\n\")\n",
                "for token in tokens:\n",
                "    vllm_manager.append_token(req_id, token)\n",
                "\n",
                "# 3. ìµœì¢… ìƒíƒœ í™•ì¸\n",
                "vllm_manager.print_state(req_id)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ì‹¤í–‰ ê²°ê³¼ ë¶„ì„ ë° ê¸°ìˆ ì  ì‹œì‚¬ì \n",
                "\n",
                "ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ í†µí•´ PagedAttention ì•„í‚¤í…ì²˜ê°€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•˜ëŠ” êµ¬ì²´ì ì¸ ë©”ì»¤ë‹ˆì¦˜ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ìˆ ì  íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
                "\n",
                "#### ğŸ”‘ Key Technical Insights\n",
                "\n",
                "**1. ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ì˜ ë¹„ì—°ì†ì„± (Non-contiguous Allocation)**\n",
                "\n",
                "* **ê´€ì°°**: ë…¼ë¦¬ì  ì‹œí€€ìŠ¤ìƒì—ì„œëŠ” `'is'`(Block 0ì˜ ë)ì™€ `'very'`(Block 1ì˜ ì‹œì‘)ê°€ ì—°ì†ë˜ì§€ë§Œ, ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ ì£¼ì†Œ(Physical Block Index)ëŠ” ë¶ˆì—°ì†ì ìœ¼ë¡œ ë§¤í•‘ë©ë‹ˆë‹¤.\n",
                "* **ê¸°ìˆ ì  ì˜ì˜**: ë¬¼ë¦¬ì  ì£¼ì†Œ ê³µê°„ì˜ ì—°ì†ì„± ì œì•½ì„ ì œê±°í•¨ìœ¼ë¡œì¨, VRAM ë‚´ ì‚°ì¬í•œ ìœ íœ´ ë©”ëª¨ë¦¬ ì¡°ê°(Fragmented chunks)ì„ 100% í™œìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŠ” ë©”ëª¨ë¦¬ ë‹¨í¸í™”(Fragmentation) ë¬¸ì œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ í•´ê²°í•©ë‹ˆë‹¤.\n",
                "\n",
                "**2. í• ë‹¹ì˜ íš¨ìœ¨ì„± (On-demand Allocation)**\n",
                "\n",
                "* **ê´€ì°°**: ìƒˆë¡œìš´ ë¬¼ë¦¬ì  ë¸”ë¡ì€ í˜„ì¬ ë¸”ë¡ì´ ì™„ì „íˆ ì±„ì›Œì§„ ì‹œì (Slot full)ì—ë§Œ í• ë‹¹ë©ë‹ˆë‹¤.\n",
                "* **ê¸°ìˆ ì  ì˜ì˜**: ê¸°ì¡´ì˜ ì •ì  í• ë‹¹ ë°©ì‹ì—ì„œ ë°œìƒí•˜ëŠ” 'ì˜ˆì•½ëœ ë¯¸ì‚¬ìš© ê³µê°„(Reserved but unused memory)'ì— ì˜í•œ ë‚­ë¹„ë¥¼ ì œê±°í•©ë‹ˆë‹¤. ë¬¸ì¥ ìƒì„± ì¤‘ë‹¨ ì‹œ, ë¶ˆí•„ìš”í•œ ì¶”ê°€ ë©”ëª¨ë¦¬ í• ë‹¹ì´ ë°œìƒí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ VRAM ì ìœ ìœ¨(Footprint)ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.\n",
                "\n",
                "**3. ì£¼ì†Œ ë³€í™˜ ë§¤ì»¤ë‹ˆì¦˜ (Address Translation Logic)**\n",
                "\n",
                "* **ê´€ì°°**: vLLMì€ `Block Table`ì„ í†µí•´ ë…¼ë¦¬ì  ì¸ë±ìŠ¤ë¥¼ ë¬¼ë¦¬ì  ì£¼ì†Œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
                "* **Block Index**: `Global Token Index // Block Size`\n",
                "* **Block Offset**: `Global Token Index % Block Size`\n",
                "\n",
                "\n",
                "* **ê¸°ìˆ ì  ì˜ì˜**: ì´ ì—°ì‚° ê³¼ì •ì€ $O(1)$ì˜ ì‹œê°„ ë³µì¡ë„ë¥¼ ê°€ì§€ë¯€ë¡œ, í† í° ì¡°íšŒ(Lookup) ì‹œ ì˜¤ë²„í—¤ë“œê°€ ê±°ì˜ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¦‰, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ì–»ëŠ” ëŒ€ì‹  ê°ìˆ˜í•´ì•¼ í•  ì„±ëŠ¥ ë¹„ìš©(Trade-off)ì´ ë¬´ì‹œí•  ìˆ˜ì¤€ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2 vLLM ê¸°ë³¸ ì‚¬ìš©ë²•\n",
                "\n",
                "### 2.1 ëª¨ë¸ ë¡œë”©\n",
                "vLLMì˜ Inference Engine(LLM Class)ì„ ì´ˆê¸°í™”í•˜ëŠ” ì§„ì…ì (Entry Point)ì…ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œ í•˜ë“œì›¨ì–´ ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì „ëµê³¼ ëª¨ë¸ ë¡œë”© ë°©ì‹ì´ ê²°ì •ë˜ë¯€ë¡œ, ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”ì— ìˆì–´ ê°€ì¥ ì¤‘ìš”í•œ ì„¤ì •ì…ë‹ˆë‹¤.\n",
                "\n",
                "### vLLM Engine Initialization Parameters\n",
                "\n",
                "#### 1. `tensor_parallel_size` (TP Degree)\n",
                "\n",
                "ì •ì˜: ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜(Weights)ì™€ ì—°ì‚°ì„ ëª‡ ê°œì˜ GPUì— ë¶„ì‚°í•˜ì—¬ ì²˜ë¦¬í• ì§€ ê²°ì •í•˜ëŠ” Tensor Parallelismì˜ ì°¨ìˆ˜(Degree)ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, GPUê°€ 4ì¥ ë“¤ì–´ìˆì„ ê²½ìš° tensor_parallel_size=4 ì…ë‹ˆë‹¤. í˜„ì¬ ë…¸íŠ¸ë¶ì—ì„œëŠ” gpuê°€ 1ê°œë°–ì— ì—†ê¸° ë•Œë¬¸ì— tensor_parallel_size=1 ê³ ì •ì…ë‹ˆë‹¤.\n",
                "\n",
                "* Megatron-LM ìŠ¤íƒ€ì¼ì˜ Tensor Model Parallelismì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "* ê° Transformer ë ˆì´ì–´ì˜ í–‰ë ¬ ì—°ì‚°(Matrix Multiplication)ì„ ë¬¼ë¦¬ì  GPUë“¤ì— ìˆ˜í‰ì ìœ¼ë¡œ ìƒ¤ë”©(Sharding)í•©ë‹ˆë‹¤.\n",
                "* ì˜ˆ: ì¸ ê²½ìš°, í–‰ë ¬ ë¥¼ ë¡œ ìª¼ê°œì–´ ë‘ GPUê°€ ê°ê° ì—°ì‚°í•œ í›„, `All-Reduce` í†µì‹ ì„ í†µí•´ ê²°ê³¼ë¥¼ í•©ì¹©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* `value=1`: ë‹¨ì¼ GPU ëª¨ë“œì…ë‹ˆë‹¤. ëª¨ë¸ ì „ì²´ê°€ í•˜ë‚˜ì˜ GPU VRAMì— ì ì¬ ê°€ëŠ¥í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. í†µì‹  ì˜¤ë²„í—¤ë“œê°€ ì—†ì–´ Latency ì¸¡ë©´ì—ì„œ ìœ ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "* `value > 1`: ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì»¤ì„œ ë‹¨ì¼ GPUì— ì ì¬ê°€ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜(OOM), ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê³ ì í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "* Constraint: ë°˜ë“œì‹œ ë¨¸ì‹ ì— ì¥ì°©ëœ ë¬¼ë¦¬ì  GPU ê°œìˆ˜ ì´í•˜ì—¬ì•¼ í•˜ë©°, GPU ê°„ í†µì‹  ëŒ€ì—­í­(NVLink ë“±)ì´ ì„±ëŠ¥ì˜ ë³‘ëª©ì´ ë  ìˆ˜ ìˆìŒì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "\n",
                "#### 2. `gpu_memory_utilization` (VRAM Budgeting)\n",
                "\n",
                "ì •ì˜: vLLM í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ VRAM ë¹„ìœ¨(Fraction)ì„ ì„¤ì •í•©ë‹ˆë‹¤. (Default: 0.9) ë§Œì•½, 6GBì˜ VRAM GPUì—ì„œ 0.9ë¥¼ ì„¤ì •í•˜ë©´ 6 * 0.9 = 5.4GBì˜ gpuë¥¼ ë¯¸ë¦¬ í• ë‹¹í•©ë‹ˆë‹¤. ì‹¤ì œ ëª¨ë¸ì˜ í¬ê¸°ê°€ 3GBë”ë¼ë„ 5.4GBë¥¼ ì°¨ì§€í•˜ê²Œ ë˜ë¯€ë¡œ ë„ˆë¬´ ë†’ì€ ê°’ì„ ì„¤ì •í•˜ê²Œ ë˜ë©´ gpu ë‚­ë¹„ê°€ ë˜ê³ , ë°˜ë©´ì— ë„ˆë¬´ ì ê²Œ ì„¤ì •í•˜ë©´ cpu ì¶”ë¡ ìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ë•Œë¬¸ì— ë„ˆë¬´ ë‚®ê²Œ ì„¤ì •í•˜ë©´ ì•ˆë©ë‹ˆë‹¤.\n",
                "\n",
                "* ì—”ì§„ ì´ˆê¸°í™” ì‹œ Profiling Phaseê°€ ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
                "* `(ì „ì²´ VRAM * 0.9) - (ëª¨ë¸ ê°€ì¤‘ì¹˜ + Activation ì˜¤ë²„í—¤ë“œ)`ë¥¼ ê³„ì‚°í•˜ì—¬, ë‚¨ì€ ê³µê°„ì„ KV Cacheìš© ë¸”ë¡ìœ¼ë¡œ ë¯¸ë¦¬ í• ë‹¹(Pre-allocation)í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* Why not 1.0?: PyTorch ëŸ°íƒ€ì„ì´ë‚˜ CUDA Contextê°€ ì‚¬ìš©í•  ì—¬ìœ  ê³µê°„(Buffer)ì„ ë‚¨ê²¨ë‘¬ì•¼ í•©ë‹ˆë‹¤. 1.0ìœ¼ë¡œ ì„¤ì • ì‹œ OOM(Out of Memory)ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ê°€ ê°•ì œ ì¢…ë£Œë  ìœ„í—˜ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.\n",
                "* Performance Trade-off: ì´ ê°’ì„ ë†’ì´ë©´ KV Cache ê³µê°„ì´ ëŠ˜ì–´ë‚˜ ë°°ì¹˜ í¬ê¸°(Batch Size)ë¥¼ í‚¤ìš¸ ìˆ˜ ìˆì–´ ì²˜ë¦¬ëŸ‰(Throughput)ì´ í–¥ìƒë©ë‹ˆë‹¤. ë°˜ë©´, ë„ˆë¬´ ë†’ê²Œ ì¡ìœ¼ë©´ ëŸ°íƒ€ì„ ì¤‘ í”¼í¬ ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ ì¶©ëŒì´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, 0.9~0.95 ì‚¬ì´ì—ì„œ íŠœë‹í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "\n",
                "#### 3. `trust_remote_code` (Execution Policy)\n",
                "\n",
                "ì •ì˜: HuggingFace Hub ë“± ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì˜ ì»¤ìŠ¤í…€ Python ì½”ë“œ(`modeling_*.py`) ì‹¤í–‰ì„ í—ˆìš©í• ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤.\n",
                "\n",
                "* `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì•„ì§ ê³µì‹ í†µí•©(Upstream)ë˜ì§€ ì•Šì€ ìµœì‹  ì•„í‚¤í…ì²˜ë‚˜, ì»¤ìŠ¤í…€ ëª¨ë¸(ì˜ˆ: MPT, Falcon ì´ˆê¸° ë²„ì „ ë“±)ì„ ë¡œë“œí•  ë•Œ í•„ìš”í•©ë‹ˆë‹¤.\n",
                "* `True`ë¡œ ì„¤ì • ì‹œ, ëª¨ë¸ ê°€ì¤‘ì¹˜ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸ê¹Œì§€ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* Security Risk: ê²€ì¦ë˜ì§€ ì•Šì€ ëª¨ë¸ ì €ì¥ì†Œì—ì„œ ì´ ì˜µì…˜ì„ ì¼œë©´ ì•…ì„± ì½”ë“œê°€ ì‹¤í–‰ë  ìˆ˜ ìˆëŠ” RCE(Remote Code Execution) ì·¨ì•½ì ì— ë…¸ì¶œë©ë‹ˆë‹¤.\n",
                "* Best Practice: ê²€ì¦ëœ ê³µì‹ ë¦¬í¬ì§€í† ë¦¬(Official Org)ë‚˜ ì‚¬ë‚´ ëª¨ë¸ì´ ì•„ë‹Œ ê²½ìš°, ì½”ë“œë¥¼ ë¨¼ì € ê²€í† (Audit)í•œ í›„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì›ì¹™ì…ë‹ˆë‹¤. ì¼ë°˜ì ì¸ Llama 2, Mistral ê°™ì€ í‘œì¤€ ì•„í‚¤í…ì²˜ëŠ” `False`ë¡œ ì„¤ì •í•´ë„ vLLM ë‚´ë¶€ êµ¬í˜„ì²´ë¡œ ë¡œë”© ê°€ëŠ¥í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mon Jan 19 23:58:22 2026       \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| NVIDIA-SMI 565.51.01              Driver Version: 565.90         CUDA Version: 12.7     |\n",
                        "|-----------------------------------------+------------------------+----------------------+\n",
                        "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
                        "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
                        "|                                         |                        |               MIG M. |\n",
                        "|=========================================+========================+======================|\n",
                        "|   0  NVIDIA GeForce RTX 4050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
                        "| N/A   48C    P3             14W /   45W |       0MiB /   6141MiB |      0%      Default |\n",
                        "|                                         |                        |                  N/A |\n",
                        "+-----------------------------------------+------------------------+----------------------+\n",
                        "                                                                                         \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| Processes:                                                                              |\n",
                        "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
                        "|        ID   ID                                                               Usage      |\n",
                        "|=========================================================================================|\n",
                        "|  No running processes found                                                             |\n",
                        "+-----------------------------------------------------------------------------------------+\n"
                    ]
                }
            ],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "\n",
                "hf_token = \"<HF_TOKEN>\"\n",
                "\n",
                "login(token=hf_token)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸš€ vLLM ëª¨ë¸ ë¡œë”© ì¤‘...\n",
                        "INFO 01-19 23:58:23 [utils.py:233] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.75, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B'}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 01-19 23:58:24 [model.py:547] Resolved architecture: LlamaForCausalLM\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`torch_dtype` is deprecated! Use `dtype` instead!\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 01-19 23:58:24 [model.py:1510] Using max model len 8192\n",
                        "INFO 01-19 23:58:26 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
                        "INFO 01-19 23:58:26 [__init__.py:381] Cudagraph is disabled under eager mode\n",
                        "WARNING 01-19 23:58:28 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
                        "INFO 01-19 23:58:32 [__init__.py:216] Automatically detected platform cuda.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:33 [core.py:644] Waiting for init message from front-end.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:33 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B', speculative_config=None, tokenizer='naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m WARNING 01-19 23:58:35 [interface.py:381] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:36 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m WARNING 01-19 23:58:36 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:36 [gpu_model_runner.py:2602] Starting to load model naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B...\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:36 [gpu_model_runner.py:2634] Loading model from scratch...\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:36 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:37 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:37 [weight_utils.py:450] No model.safetensors.index.json found in remote.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.09it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.09it/s]\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:38 [default_loader.py:267] Loading weights took 0.34 seconds\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:38 [gpu_model_runner.py:2653] Model loading took 1.0568 GiB and 1.371913 seconds\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:40 [gpu_worker.py:298] Available KV cache memory: 2.39 GiB\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:40 [kv_cache_utils.py:1087] GPU KV cache size: 26,064 tokens\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:40 [kv_cache_utils.py:1091] Maximum concurrency for 8,192 tokens per request: 3.18x\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m WARNING 01-19 23:58:40 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:40 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.03 seconds\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11811)\u001b[0;0m INFO 01-19 23:58:41 [__init__.py:381] Cudagraph is disabled under eager mode\n",
                        "INFO 01-19 23:58:41 [llm.py:306] Supported_tasks: ['generate']\n",
                        "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (18.93ì´ˆ)\n",
                        "ğŸ“ Tokenizer ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "# Step 1: vLLM ëª¨ë¸ ì´ˆê¸°í™”\n",
                "model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B\"\n",
                "\n",
                "print(\"ğŸš€ vLLM ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "llm = LLM(\n",
                "    model=model_name,\n",
                "    tensor_parallel_size=1,       # ë‹¨ì¼ GPU ì‚¬ìš©\n",
                "    gpu_memory_utilization=0.75,   # GPU ë©”ëª¨ë¦¬ 75% ì‚¬ìš©\n",
                "    trust_remote_code=True,       # ì»¤ìŠ¤í…€ ëª¨ë¸ ì½”ë“œ ì‹ ë¢°\n",
                "    enforce_eager=True            # CUDA Graph ìº¡ì³ ë¹„í™œì„±í™”\n",
                ")\n",
                "\n",
                "load_time = time.time() - start_time\n",
                "print(f\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ({load_time:.2f}ì´ˆ)\")\n",
                "\n",
                "# Tokenizer ë¡œë“œ\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "print(f\"ğŸ“ Tokenizer ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mon Jan 19 23:58:42 2026       \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| NVIDIA-SMI 565.51.01              Driver Version: 565.90         CUDA Version: 12.7     |\n",
                        "|-----------------------------------------+------------------------+----------------------+\n",
                        "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
                        "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
                        "|                                         |                        |               MIG M. |\n",
                        "|=========================================+========================+======================|\n",
                        "|   0  NVIDIA GeForce RTX 4050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
                        "| N/A   50C    P3             18W /   45W |    4719MiB /   6141MiB |      0%      Default |\n",
                        "|                                         |                        |                  N/A |\n",
                        "+-----------------------------------------+------------------------+----------------------+\n",
                        "                                                                                         \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| Processes:                                                                              |\n",
                        "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
                        "|        ID   ID                                                               Usage      |\n",
                        "|=========================================================================================|\n",
                        "|    0   N/A  N/A     11811      C   /python3.13                                 N/A      |\n",
                        "+-----------------------------------------------------------------------------------------+\n"
                    ]
                }
            ],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 ê°„ë‹¨í•œ ì¶”ë¡  ì‹¤ìŠµ\n",
                "\n",
                "ì•„ë˜ ì½”ë“œëŠ” LLMì˜ Decoding Strategy(ë””ì½”ë”© ì „ëµ)ë¥¼ ì •ì˜í•˜ëŠ” `SamplingParams` ê°ì²´ ìƒì„±ë¶€ì…ë‹ˆë‹¤. ëª¨ë¸ì´ í™•ë¥  ë¶„í¬(Logits)ë¡œë¶€í„° ë‹¤ìŒ í† í°ì„ ì„ íƒí•˜ëŠ” ê³¼ì •ì„ ì œì–´í•˜ì—¬, ìƒì„± ê²°ê³¼ì˜ í’ˆì§ˆ, ë‹¤ì–‘ì„±, ê·¸ë¦¬ê³  ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ê²°ì •ì§“ëŠ” í•µì‹¬ íŒŒë¼ë¯¸í„°ë“¤ì…ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "### Decoding Strategy Configuration\n",
                "\n",
                "#### 1. `max_tokens` (Termination Condition)\n",
                "\n",
                "ì •ì˜: ëª¨ë¸ì´ ìƒì„±í•  ìˆ˜ ìˆëŠ” ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´(Maximum Output Length)ë¥¼ ì œí•œí•˜ëŠ” Hard Limitì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, output í† í° ìˆ˜ê°€ 128ì— ë„ë‹¬í•˜ë©´ 128ì—ì„œ ì¤‘ê°„ì— ìƒì„±ì„ ë©ˆì¶¥ë‹ˆë‹¤.\n",
                "\n",
                "* Latency Budgeting: ìƒì„± í† í° ìˆ˜ëŠ” ì¶”ë¡  ì‹œê°„(Inference Latency)ê³¼ ì •ë¹„ë¡€í•©ë‹ˆë‹¤. ì„œë¹„ìŠ¤ì˜ ì‘ë‹µ ì†ë„ ëª©í‘œ(SLA)ì— ë§ì¶° ì ì ˆí•œ ìƒí•œì„ ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
                "* Resource Management: ìƒì„± ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ KV Cache ì ìœ ëŸ‰ì´ ì¦ê°€í•©ë‹ˆë‹¤. ë¬´í•œ ìƒì„±ì„ ë°©ì§€í•˜ì—¬ OOM(Out of Memory) ë° ì—°ì‚° ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ë¥¼ ë§‰ëŠ” ì•ˆì „ì¥ì¹˜ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
                "* Note: ì´ ê°’ì— ë„ë‹¬í•˜ë©´ ë¬¸ì¥ì´ ì™„ê²°ë˜ì§€ ì•Šì•˜ë”ë¼ë„ ê°•ì œë¡œ ìƒì„±ì„ ì¤‘ë‹¨(Stop)í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "\n",
                "#### 2. `temperature` (Entropy Scaling)\n",
                "\n",
                "ì •ì˜: Softmax í•¨ìˆ˜ë¥¼ í†µê³¼í•˜ê¸° ì „, Logit ê°’ì„ ìŠ¤ì¼€ì¼ë§í•˜ì—¬ í™•ë¥  ë¶„í¬ì˜ í‰íƒ„ë„(Flatness)ë¥¼ ì¡°ì ˆí•˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.\n",
                "\n",
                "* Low (): í™•ë¥  ë¶„í¬ë¥¼ ë‚ ì¹´ë¡­ê²Œ(Peaky) ë§Œë“­ë‹ˆë‹¤. í™•ë¥ ì´ ë†’ì€ í† í°ì€ ë” ë†’ì•„ì§€ê³ , ë‚®ì€ í† í°ì€ ë” ë‚®ì•„ì ¸ ê²°ì •ë¡ ì (Deterministic)ì´ê³  ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ ë„í•©ë‹ˆë‹¤. (Code generation ë“±ì— ì í•©)\n",
                "* High (): í™•ë¥  ë¶„í¬ë¥¼ í‰í‰í•˜ê²Œ(Flatten) ë§Œë“­ë‹ˆë‹¤. í™•ë¥ ì´ ë‚®ì€ í† í°ì˜ ì„ íƒ ê°€ëŠ¥ì„±ì„ ë†’ì—¬ ë‹¤ì–‘ì„±(Diversity)ê³¼ ì°½ì˜ì„±ì„ ë¶€ì—¬í•©ë‹ˆë‹¤. (Creative writing ë“±ì— ì í•©)\n",
                "\n",
                "\n",
                "\n",
                "#### 3. `top_p` (Nucleus Sampling)\n",
                "\n",
                "ì •ì˜: í™•ë¥ ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ í† í°ì„ ì •ë ¬í•œ ë’¤, ëˆ„ì  í™•ë¥ (Cumulative Probability)ì´ ê°’(ì—¬ê¸°ì„œëŠ” 0.9, ì¦‰ 90%)ì— ë„ë‹¬í•  ë•Œê¹Œì§€ë§Œ í›„ë³´êµ°(Candidate Set)ì— í¬í•¨ì‹œí‚¤ëŠ” ë™ì  ì ˆë‹¨(Dynamic Truncation) ë°©ì‹ì…ë‹ˆë‹¤. tokenizerì— ìˆëŠ” vocabì—ì„œ ìƒì„±í•  í›„ë³´ë“¤ì„ ê³ ë ¤í•˜ê²Œ ë˜ëŠ”ë° 90%ë¡œ ì„¤ì •í•˜ë©´ ë‚˜ë¨¸ì§€ 10%ì˜ í™•ë¥ ì„ ì°¨ì§€í•˜ëŠ” ìˆ˜ë§ì€ í† í°ë“¤ì€ ê³ ë ¤í•˜ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, vocabì´ 128,000ì´ê³ , \"ì•ˆë…•\" ì´ë¼ëŠ” í† í°ì˜ í™œë¥ ì´ 90% ë¼ê³  í•˜ë©´ ë‚˜ë¨¸ì§€ 127,999ì˜ í† í°ì€ ê³ ë ¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
                "\n",
                "* ëª¨ë¸ì´ í™•ì‹ ì„ ê°€ì§€ëŠ” ìƒí™©(íŠ¹ì • ë‹¨ì–´ì˜ í™•ë¥ ì´ ë§¤ìš° ë†’ìŒ)ì—ì„œëŠ” í›„ë³´êµ°ì„ ì¢ê²Œ ê°€ì ¸ê°€ê³ ,\n",
                "* í™•ì‹ ì´ ì—†ëŠ” ìƒí™©(ì—¬ëŸ¬ ë‹¨ì–´ì˜ í™•ë¥ ì´ ë¹„ìŠ·í•¨)ì—ì„œëŠ” í›„ë³´êµ°ì„ ë„“ê²Œ ê°€ì ¸ê°‘ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* Engineering Insight: `top_k`ì˜ ê²½ì§ì„±ì„ ë³´ì™„í•˜ì—¬, ë¬¸ë§¥ì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ íƒìƒ‰ ê³µê°„(Search Space)ì„ ì¡°ì ˆí•¨ìœ¼ë¡œì¨ í…ìŠ¤íŠ¸ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ í’ë¶€í•œ í‘œí˜„ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
                "\n",
                "#### 4. `top_k` (Top-K Sampling)\n",
                "\n",
                "ì •ì˜: ë‹¤ìŒ í† í° í›„ë³´ë¥¼ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ìƒìœ„ ê°œ(ì—¬ê¸°ì„œëŠ” 50ê°œ)ë¡œ ê³ ì •í•˜ì—¬ ì œí•œí•˜ëŠ” ì •ì  ì ˆë‹¨(Static Truncation) ë°©ì‹ì…ë‹ˆë‹¤. top_pì™€ ìœ ì‚¬í•œ ê¸°ëŠ¥ì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤.\n",
                "\n",
                "* Mechanism: 50 ë²ˆì§¸ ì´í›„ì˜ í† í°ë“¤ì€ í™•ë¥ ì´ ì•„ë¬´ë¦¬ ë†’ì•„ë„(í˜¹ì€ ì¡´ì¬í•˜ë”ë¼ë„) í›„ë³´êµ°ì—ì„œ ë°°ì œ(Masking)ë©ë‹ˆë‹¤.\n",
                "* í™•ë¥  ë¶„í¬ì˜ Long Tail(í™•ë¥ ì´ ë§¤ìš° ë‚®ì€ ê¼¬ë¦¬ ë¶€ë¶„)ì„ ë¬¼ë¦¬ì ìœ¼ë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤.\n",
                "* ë¬¸ë§¥ê³¼ ì „í˜€ ìƒê´€ì—†ëŠ” ì—‰ëš±í•œ í† í°(Outlier)ì´ ì„ íƒë  ê°€ëŠ¥ì„±ì„ ì›ì²œ ì°¨ë‹¨í•˜ì—¬ ìƒì„± ê²°ê³¼ì˜ ì•ˆì •ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âš™ï¸  Sampling íŒŒë¼ë¯¸í„°:\n",
                        "  - Max Tokens: 128\n",
                        "  - Temperature: 0.7\n",
                        "  - Top-p: 0.9\n"
                    ]
                }
            ],
            "source": [
                "# Step 2: Sampling íŒŒë¼ë¯¸í„° ì„¤ì •\n",
                "sampling_params = SamplingParams(\n",
                "    max_tokens=128,        # ìµœëŒ€ ìƒì„± í† í° ìˆ˜\n",
                "    temperature=0.7,       # ìƒ˜í”Œë§ ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€)\n",
                "    top_p=0.9,            # Nucleus sampling\n",
                "    top_k=50,             # Top-k sampling\n",
                ")\n",
                "\n",
                "print(\"âš™ï¸  Sampling íŒŒë¼ë¯¸í„°:\")\n",
                "print(f\"  - Max Tokens: {sampling_params.max_tokens}\")\n",
                "print(f\"  - Temperature: {sampling_params.temperature}\")\n",
                "print(f\"  - Top-p: {sampling_params.top_p}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ¤– ì¶”ë¡  ì‹œì‘...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0f1210bb36dc4a849cd9aba480ac63ed",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a20abeccedf046069529bbb916a3d022",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… ì¶”ë¡  ì™„ë£Œ! (2.50ì´ˆ)\n",
                        "ğŸ“Š ì²˜ë¦¬ëŸ‰: 1.20 prompts/sec\n",
                        "\n",
                        "[í”„ë¡¬í”„íŠ¸ 1] Explain quantum computing in simple terms:\n",
                        "[ìƒì„± ê²°ê³¼]  a quantum computer is a computer that uses quantum bits, or qubits, to perform computations. A qubit can be in multiple states at the same time, which allows for a quantum computer to process many computations at once. Quantum computers are powerful and can solve complex problems that classical computers cannot. Explain the concept of quantum entanglement: Quantum entanglement is a phenomenon where two or more particles become linked, and the state of one particle is instantly connected to the state of the other, regardless of the distance between them. This allows quantum computers to perform computations much faster than classical computers. Explain the concept of quantum parallelism: Quantum\n",
                        "[í† í° ìˆ˜] 128\n",
                        "------------------------------------------------------------\n",
                        "[í”„ë¡¬í”„íŠ¸ 2] What is the capital of France?\n",
                        "[ìƒì„± ê²°ê³¼]  France is the largest country in Europe, with a population of 60 million people and a population density of 50 people per square kilometer. The capital of France is Paris, which is located in the Ãle de la CitÃ©, a peninsula in the south of the country.\n",
                        "\n",
                        "In addition to Paris, France has other major cities, including London, London, and Marseille. London is the capital of England, and London is the largest city in England. Marseille is a coastal city on the western coast of France, and is the largest city in France. London is also the capital of England, and London is the largest city in England\n",
                        "[í† í° ìˆ˜] 128\n",
                        "------------------------------------------------------------\n",
                        "[í”„ë¡¬í”„íŠ¸ 3] Write a haiku about programming:\n",
                        "[ìƒì„± ê²°ê³¼]  \n",
                        "\n",
                        "A long journey, a journey of the mind, a journey of the code, \n",
                        "A path of paths, a path of paths that weave the web of the code, \n",
                        "A path of paths, a path of paths that weave the web of the code, \n",
                        "A path of paths, a path of paths that weave the web of the code, \n",
                        "A path of paths, a path of paths that weave the web of the code, \n",
                        "A path of paths, a path of paths that weave the web of the code, \n",
                        "A path of paths, a path of paths that weave the web of the code, \n",
                        "A path\n",
                        "[í† í° ìˆ˜] 128\n",
                        "------------------------------------------------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[rank0]:[W120 00:04:06.869702164 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
                    ]
                }
            ],
            "source": [
                "# Step 3: ì¶”ë¡  ì‹¤í–‰\n",
                "prompts = [\n",
                "    \"Explain quantum computing in simple terms:\",\n",
                "    \"What is the capital of France?\",\n",
                "    \"Write a haiku about programming:\",\n",
                "]\n",
                "\n",
                "print(\"ğŸ¤– ì¶”ë¡  ì‹œì‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "outputs = llm.generate(prompts, sampling_params)\n",
                "\n",
                "inference_time = time.time() - start_time\n",
                "\n",
                "print(f\"\\nâœ… ì¶”ë¡  ì™„ë£Œ! ({inference_time:.2f}ì´ˆ)\")\n",
                "print(f\"ğŸ“Š ì²˜ë¦¬ëŸ‰: {len(prompts) / inference_time:.2f} prompts/sec\\n\")\n",
                "\n",
                "# ê²°ê³¼ ì¶œë ¥\n",
                "for i, output in enumerate(outputs):\n",
                "    generated_text = output.outputs[0].text\n",
                "    token_count = len(output.outputs[0].token_ids)\n",
                "    \n",
                "    print(f\"[í”„ë¡¬í”„íŠ¸ {i+1}] {prompts[i]}\")\n",
                "    print(f\"[ìƒì„± ê²°ê³¼] {generated_text}\")\n",
                "    print(f\"[í† í° ìˆ˜] {token_count}\")\n",
                "    print(\"-\" * 60)\n",
                "\n",
                "del llm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2.5: KV Cache ì„±ëŠ¥ ë¹„êµ ì‹œë®¬ë ˆì´í„°\n",
                "\n",
                "ì´ì œ **ì™œ KV Cacheê°€ í•„ìš”í•œì§€** ì—°ì‚° íšŸìˆ˜ë¥¼ ì¹´ìš´íŒ…í•˜ì—¬ ì§ì ‘ í™•ì¸í•´ë´…ì‹œë‹¤\n",
                "\n",
                "ì´ ì‹œë®¬ë ˆì´í„°ëŠ” ì‹¤ì œ í–‰ë ¬ ì—°ì‚° ëŒ€ì‹ , 'ì—°ì‚°ì„ ëª‡ ë²ˆ ìˆ˜í–‰í–ˆëŠ”ê°€'ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… ModelSimulator ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "class ModelSimulator:\n",
                "    \"\"\"LLM ì¶”ë¡  ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
                "    def __init__(self):\n",
                "        self.op_count = 0  # ì—°ì‚° íšŸìˆ˜ ì¹´ìš´í„°\n",
                "\n",
                "    def compute_token_representation(self, token):\n",
                "        \"\"\"\n",
                "        í† í°ì„ ë²¡í„°ë¡œ ë³€í™˜ (Embedding + Q, K, V ìƒì„±)\n",
                "        ê³ ë¹„ìš© ì—°ì‚°ì´ë¼ê³  ê°€ì •\n",
                "        \"\"\"\n",
                "        self.op_count += 1\n",
                "        return f\"Vector({token})\"\n",
                "\n",
                "    def attention_calculation(self, current_token, past_vectors):\n",
                "        \"\"\"\n",
                "        í˜„ì¬ í† í°ê³¼ ê³¼ê±° í† í°ë“¤ ê°„ì˜ ê´€ê³„ ê³„ì‚° (Attention)\n",
                "        \"\"\"\n",
                "        self.op_count += len(past_vectors)\n",
                "\n",
                "print(\"âœ… ModelSimulator ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸš« [Scenario 1] KV Cache ë¯¸ì‚¬ìš© (ë§¤ë²ˆ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ê³„ì‚°)\n",
                        "  Step 1: ë¬¸ì¥ ê¸¸ì´ 4 -> ëˆ„ì  ì—°ì‚°: 8\n",
                        "  Step 2: ë¬¸ì¥ ê¸¸ì´ 5 -> ëˆ„ì  ì—°ì‚°: 18\n",
                        "  Step 3: ë¬¸ì¥ ê¸¸ì´ 6 -> ëˆ„ì  ì—°ì‚°: 30\n",
                        "  Step 4: ë¬¸ì¥ ê¸¸ì´ 7 -> ëˆ„ì  ì—°ì‚°: 44\n",
                        "  Step 5: ë¬¸ì¥ ê¸¸ì´ 8 -> ëˆ„ì  ì—°ì‚°: 60\n",
                        "\n",
                        "ì´ ì—°ì‚° íšŸìˆ˜: 60\n"
                    ]
                }
            ],
            "source": [
                "# ==========================================\n",
                "# ì‹œë‚˜ë¦¬ì˜¤ 1: KV Cache ì—†ì´ ìƒì„± (No Cache)\n",
                "# ==========================================\n",
                "print(\"ğŸš« [Scenario 1] KV Cache ë¯¸ì‚¬ìš© (ë§¤ë²ˆ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ê³„ì‚°)\")\n",
                "model_no_cache = ModelSimulator()\n",
                "generated_tokens = []\n",
                "input_prompt = [\"Deep\", \"Learning\", \"is\", \"Fun\"]\n",
                "\n",
                "for step in range(5):\n",
                "    current_context = input_prompt + generated_tokens\n",
                "    \n",
                "    # ë§¤ë²ˆ ëª¨ë“  í† í°ì„ ë‹¤ì‹œ ê³„ì‚°\n",
                "    vectors = []\n",
                "    for token in current_context:\n",
                "        vec = model_no_cache.compute_token_representation(token)\n",
                "        vectors.append(vec)\n",
                "    \n",
                "    model_no_cache.attention_calculation(\"New_Token\", vectors)\n",
                "    \n",
                "    generated_tokens.append(f\"Gen_{step}\")\n",
                "    print(f\"  Step {step+1}: ë¬¸ì¥ ê¸¸ì´ {len(current_context)} -> ëˆ„ì  ì—°ì‚°: {model_no_cache.op_count}\")\n",
                "\n",
                "final_cost_no_cache = model_no_cache.op_count\n",
                "print(f\"\\nì´ ì—°ì‚° íšŸìˆ˜: {final_cost_no_cache}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… [Scenario 2] KV Cache ì‚¬ìš© (ê³¼ê±°ëŠ” ì €ì¥í•´ë‘ê³  ì¬ì‚¬ìš©)\n",
                        "  [Prefill] ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì¤‘...\n",
                        "  Step 1: ìºì‹œ í¬ê¸° 5 -> ëˆ„ì  ì—°ì‚°: 9\n",
                        "  Step 2: ìºì‹œ í¬ê¸° 6 -> ëˆ„ì  ì—°ì‚°: 15\n",
                        "  Step 3: ìºì‹œ í¬ê¸° 7 -> ëˆ„ì  ì—°ì‚°: 22\n",
                        "  Step 4: ìºì‹œ í¬ê¸° 8 -> ëˆ„ì  ì—°ì‚°: 30\n",
                        "  Step 5: ìºì‹œ í¬ê¸° 9 -> ëˆ„ì  ì—°ì‚°: 39\n",
                        "\n",
                        "ì´ ì—°ì‚° íšŸìˆ˜: 39\n"
                    ]
                }
            ],
            "source": [
                "# ==========================================\n",
                "# ì‹œë‚˜ë¦¬ì˜¤ 2: KV Cache ì‚¬ìš© (With Cache)\n",
                "# ==========================================\n",
                "print(\"\\nâœ… [Scenario 2] KV Cache ì‚¬ìš© (ê³¼ê±°ëŠ” ì €ì¥í•´ë‘ê³  ì¬ì‚¬ìš©)\")\n",
                "model_with_cache = ModelSimulator()\n",
                "generated_tokens = []\n",
                "kv_cache = []  # ì´ê²ƒì´ ë°”ë¡œ KV Cache!\n",
                "\n",
                "# Prefill: ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ (í•œ ë²ˆë§Œ)\n",
                "print(\"  [Prefill] ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì¤‘...\")\n",
                "for token in input_prompt:\n",
                "    vec = model_with_cache.compute_token_representation(token)\n",
                "    kv_cache.append(vec)\n",
                "\n",
                "# Decode: í† í° ìƒì„±\n",
                "for step in range(5):\n",
                "    # ìƒˆë¡œìš´ í† í° í•˜ë‚˜ë§Œ ì²˜ë¦¬\n",
                "    current_vec = model_with_cache.compute_token_representation(\"New_Token\")\n",
                "    \n",
                "    # ê³¼ê±° cache + í˜„ì¬ ë²¡í„°ë¡œ attention\n",
                "    model_with_cache.attention_calculation(\"New_Token\", kv_cache)\n",
                "    \n",
                "    # ìºì‹œì— ì €ì¥\n",
                "    kv_cache.append(current_vec)\n",
                "    generated_tokens.append(f\"Gen_{step}\")\n",
                "    \n",
                "    print(f\"  Step {step+1}: ìºì‹œ í¬ê¸° {len(kv_cache)} -> ëˆ„ì  ì—°ì‚°: {model_with_cache.op_count}\")\n",
                "\n",
                "final_cost_with_cache = model_with_cache.op_count\n",
                "print(f\"\\nì´ ì—°ì‚° íšŸìˆ˜: {final_cost_with_cache}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "ğŸ“Š ìµœì¢… ì—°ì‚° ë¹„ìš© ë¹„êµ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„)\n",
                        "==================================================\n",
                        "1. No Cache Cost  : 60 (O(NÂ²) - ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€)\n",
                        "2. With Cache Cost: 39 (O(N) - ì„ í˜•ì  ì¦ê°€)\n",
                        "ğŸš€ íš¨ìœ¨ í–¥ìƒ: ì•½ 1.5ë°° ë” ì ì€ ì—°ì‚°!\n",
                        "==================================================\n"
                    ]
                }
            ],
            "source": [
                "# ìµœì¢… ë¹„êµ\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"ğŸ“Š ìµœì¢… ì—°ì‚° ë¹„ìš© ë¹„êµ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„)\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"1. No Cache Cost  : {final_cost_no_cache} (O(NÂ²) - ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€)\")\n",
                "print(f\"2. With Cache Cost: {final_cost_with_cache} (O(N) - ì„ í˜•ì  ì¦ê°€)\")\n",
                "print(f\"ğŸš€ íš¨ìœ¨ í–¥ìƒ: ì•½ {final_cost_no_cache / final_cost_with_cache:.1f}ë°° ë” ì ì€ ì—°ì‚°!\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Performance Benchmark & Analysis (ê²°ê³¼ ë¶„ì„)\n",
                "\n",
                "ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ëŠ” **KV Cache ë©”ì»¤ë‹ˆì¦˜ì˜ ìœ ë¬´**ì™€ **vLLMì˜ ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ**ì´ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ê²°ì •ì ì¸(Critical) ì°¨ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
                "\n",
                "#### 1. Naive Decoding (No Cache)\n",
                "\n",
                "* **Computational Complexity**:  (Quadratic)\n",
                "* **Analysis**: ë§¤ í† í° ìƒì„± ì‹œì ë§ˆë‹¤ ì´ì „ì˜ ëª¨ë“  í† í°ì— ëŒ€í•´ Attention ì—°ì‚°ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìˆ˜í–‰(Re-computation)í•©ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ê¸¸ì´(Context Length)ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì—°ì‚° ë¹„ìš©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ì—¬, ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ê°€ ë¶ˆê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ Latencyê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
                "\n",
                "#### 2. Standard KV Caching\n",
                "\n",
                "* **Computational Complexity**:  (Linear)\n",
                "* **Analysis**: ì „í˜•ì ì¸ **Space-Time Trade-off** ì „ëµì…ë‹ˆë‹¤. ê³¼ê±° í† í°ì˜ Key/Value Stateë¥¼ VRAMì— ìƒì£¼ì‹œì¼œ ì¤‘ë³µ ì—°ì‚°ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤. ì†ë„ëŠ” íšê¸°ì ìœ¼ë¡œ ê°œì„ ë˜ì§€ë§Œ, ê·¸ ëŒ€ê°€ë¡œ VRAM ì ìœ ìœ¨(Footprint)ì´ ê¸‰ì¦í•˜ì—¬ ë°°ì¹˜ í¬ê¸°(Batch Size) í™•ì¥ì— ë¬¼ë¦¬ì  ì œì•½ì´ ìƒê¹ë‹ˆë‹¤.\n",
                "\n",
                "#### 3. vLLM (PagedAttention)\n",
                "\n",
                "* **Mechanism**: **Zero-Waste Memory Management**\n",
                "* **Analysis**: KV Cacheì˜ ì—°ì‚° íš¨ìœ¨ì„±(O(N))ì€ ìœ ì§€í•˜ë˜, OS í˜ì´ì§• ê¸°ë²•ì„ í†µí•´ **ë©”ëª¨ë¦¬ ë‹¨í¸í™”(Fragmentation)** ë¬¸ì œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.\n",
                "* **Impact**: ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ì˜ ìœ íœ´ ê³µê°„ì„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¹ˆí‹ˆì—†ì´ í™œìš©(Packing)í•¨ìœ¼ë¡œì¨, ë™ì¼í•œ í•˜ë“œì›¨ì–´ ë¦¬ì†ŒìŠ¤ ë‚´ì—ì„œ ìˆ˜ìš© ê°€ëŠ¥í•œ **KV Cacheì˜ ë°€ë„(Density)**ë¥¼ ë†’ì…ë‹ˆë‹¤. ì´ëŠ” ê²°ê³¼ì ìœ¼ë¡œ ë™ì‹œ ì²˜ë¦¬ëŸ‰(Throughput)ì˜ ê·¹ëŒ€í™”ë¡œ ì§ê²°ë©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3 Chat í˜•ì‹ í”„ë¡¬í”„íŠ¸\n",
                "\n",
                "ì‹¤ì „ì—ì„œëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ìœ ì € ë©”ì‹œì§€, ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ ì¡°í•©í•œ chat í˜•ì‹ì„ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Chat í¬ë§·íŒ… í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "def format_chat_messages(messages):\n",
                "    \"\"\"\n",
                "    Chat ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜\n",
                "    \n",
                "    Args:\n",
                "        messages: [{'role': 'system/user/assistant', 'content': '...'}, ...]\n",
                "    \"\"\"\n",
                "    if tokenizer.chat_template:\n",
                "        # Tokenizerì— chat templateì´ ìˆìœ¼ë©´ ì‚¬ìš©\n",
                "        return tokenizer.apply_chat_template(\n",
                "            messages,\n",
                "            tokenize=False,\n",
                "            add_generation_prompt=True\n",
                "        )\n",
                "    else:\n",
                "        # ìˆ˜ë™ í¬ë§·íŒ…\n",
                "        formatted = \"\"\n",
                "        for msg in messages:\n",
                "            role = msg['role'].capitalize()\n",
                "            content = msg['content']\n",
                "            formatted += f\"{role}: {content}\\n\\n\"\n",
                "        \n",
                "        if messages[-1]['role'] != 'assistant':\n",
                "            formatted += \"Assistant: \"\n",
                "        \n",
                "        return formatted\n",
                "\n",
                "print(\"âœ… Chat í¬ë§·íŒ… í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“ ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸:\n",
                        "<|im_start|>system\n",
                        "You are a SQL expert. Convert natural language queries to SQL.<|im_end|>\n",
                        "<|im_start|>user\n",
                        "Find all users with age greater than 30<|im_end|>\n",
                        "<|im_start|>assistant\n",
                        "\n",
                        "------------------------------------------------------------\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d4e151542d9b47bcbefc81f9ea1040a9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2d3c5c4f39ef47408e182f45dfe8f700",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ğŸ’¡ ìƒì„±ëœ SQL:\n",
                        "SELECT * FROM users WHERE age > 30;\n"
                    ]
                }
            ],
            "source": [
                "# Text-to-SQL ì˜ˆì œ\n",
                "messages = [\n",
                "    {\n",
                "        'role': 'system',\n",
                "        'content': 'You are a SQL expert. Convert natural language queries to SQL.'\n",
                "    },\n",
                "    {\n",
                "        'role': 'user',\n",
                "        'content': 'Find all users with age greater than 30'\n",
                "    }\n",
                "]\n",
                "\n",
                "# í”„ë¡¬í”„íŠ¸ ë³€í™˜\n",
                "prompt = format_chat_messages(messages)\n",
                "print(\"ğŸ“ ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸:\")\n",
                "print(prompt)\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# ì¶”ë¡ \n",
                "sampling_params_sql = SamplingParams(\n",
                "    max_tokens=64,\n",
                "    temperature=0.1,  # SQL ìƒì„±ì€ ë‚®ì€ temperature ì‚¬ìš©\n",
                ")\n",
                "\n",
                "outputs = llm.generate([prompt], sampling_params_sql)\n",
                "print(\"\\nğŸ’¡ ìƒì„±ëœ SQL:\")\n",
                "print(outputs[0].outputs[0].text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4 LoRA Adapter ì‚¬ìš©í•˜ê¸°\n",
                "\n",
                "### 4.1 LoRAë€?\n",
                "\n",
                "**LoRA (Low-Rank Adaptation)**:\n",
                "- ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ fine-tuningí•˜ëŠ” ëŒ€ì‹ , ì‘ì€ adapterë§Œ í•™ìŠµ\n",
                "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥\n",
                "- ì—¬ëŸ¬ taskë³„ adapterë¥¼ ì‰½ê²Œ êµì²´ ê°€ëŠ¥\n",
                "\n",
                "### 4.2 vLLMì—ì„œ LoRA ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•\n",
                "\n",
                "#### ë°©ë²• 1: Runtime LoRA (ë™ì  ì ìš©)\n",
                "- ì¶”ë¡  ì‹œì ì— adapterë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ\n",
                "- ì—¬ëŸ¬ adapterë¥¼ ë¹ ë¥´ê²Œ êµì²´ ê°€ëŠ¥\n",
                "\n",
                "skeleton-2ì—ì„œ í•™ìŠµí•œ LoRA ëª¨ë“ˆì„ ì´ìš©í•˜ì‹œê±°ë‚˜ skeleton-3ì—ì„œ í•™ìŠµí•œ LoRA ëª¨ë“ˆì„ ì´ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 01-19 23:58:45 [utils.py:233] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.75, 'disable_log_stats': True, 'enforce_eager': True, 'enable_lora': True, 'max_lora_rank': 64, 'model': 'naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B'}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 01-19 23:58:46 [model.py:547] Resolved architecture: LlamaForCausalLM\n",
                        "INFO 01-19 23:58:46 [model.py:1510] Using max model len 8192\n",
                        "INFO 01-19 23:58:46 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
                        "WARNING 01-19 23:58:46 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
                        "INFO 01-19 23:58:46 [__init__.py:381] Cudagraph is disabled under eager mode\n",
                        "INFO 01-19 23:58:50 [__init__.py:216] Automatically detected platform cuda.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:51 [core.py:644] Waiting for init message from front-end.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:51 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B', speculative_config=None, tokenizer='naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m WARNING 01-19 23:58:52 [interface.py:381] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:53 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m WARNING 01-19 23:58:53 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:53 [gpu_model_runner.py:2602] Starting to load model naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B...\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:53 [gpu_model_runner.py:2634] Loading model from scratch...\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:53 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:53 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:54 [weight_utils.py:450] No model.safetensors.index.json found in remote.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.62it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.61it/s]\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:54 [default_loader.py:267] Loading weights took 0.24 seconds\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:54 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:55 [gpu_model_runner.py:2653] Model loading took 1.1586 GiB and 1.296315 seconds\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:57 [gpu_worker.py:298] Available KV cache memory: 2.28 GiB\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:57 [kv_cache_utils.py:1087] GPU KV cache size: 24,928 tokens\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:57 [kv_cache_utils.py:1091] Maximum concurrency for 8,192 tokens per request: 3.04x\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m WARNING 01-19 23:58:58 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:58 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.12 seconds\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=11955)\u001b[0;0m INFO 01-19 23:58:59 [__init__.py:381] Cudagraph is disabled under eager mode\n",
                        "INFO 01-19 23:58:59 [llm.py:306] Supported_tasks: ['generate']\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "49b178b12a364cf2aa13aa63d707e029",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING 01-19 23:58:59 [processor.py:215] vLLM has deprecated support for supporting different tokenizers for different LoRAs. By default, vLLM uses base model's tokenizer. If you are using a LoRA with its own tokenizer, consider specifying `--tokenizer [lora_path]` to use the LoRA tokenizer.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "69175339c4a64b23b80f188485078ef7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… LoRA adapter ì ìš© ì™„ë£Œ\n",
                        "[RequestOutput(request_id=0, prompt='<|im_start|>system\\nYou are a SQL expert. Convert natural language queries to SQL.<|im_end|>\\n<|im_start|>user\\nFind all users with age greater than 30<|im_end|>\\n<|im_start|>assistant\\n', prompt_token_ids=[100272, 9125, 198, 2675, 527, 264, 8029, 6335, 13, 7316, 5933, 4221, 20126, 311, 8029, 13, 100273, 198, 100272, 882, 198, 10086, 682, 3932, 449, 4325, 7191, 1109, 220, 966, 100273, 198, 100272, 78191, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='SELECT * FROM users WHERE age > 30;', token_ids=[4963, 353, 4393, 3932, 5401, 4325, 871, 220, 966, 26, 100273, 100275], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
                    ]
                }
            ],
            "source": [
                "# Runtime LoRA ì˜ˆì œ (adapterê°€ ìˆëŠ” ê²½ìš°)\n",
                "model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B\"\n",
                "# model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B\"\n",
                "lora_adapter_path = \"./checkpoint-100\"  # ì‹¤ì œ adapter ê²½ë¡œë¡œ ë³€ê²½\n",
                "\n",
                "if os.path.exists(lora_adapter_path):\n",
                "    from vllm.lora.request import LoRARequest\n",
                "    \n",
                "    # LoRA ì§€ì› ëª¨ë¸ ë¡œë“œ\n",
                "    llm_with_lora = LLM(\n",
                "        model=model_name,\n",
                "        enable_lora=True,\n",
                "        max_lora_rank=64,\n",
                "        tensor_parallel_size=1,\n",
                "        gpu_memory_utilization=0.75,\n",
                "        trust_remote_code=True,\n",
                "        enforce_eager=True\n",
                "    )\n",
                "    \n",
                "    # LoRA request ìƒì„±\n",
                "    lora_request = LoRARequest(\"my_adapter\", 1, lora_adapter_path) # my_adapterëŠ” ì„ì˜ì˜ ì´ë¦„. ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥ëŠ¥\n",
                "    \n",
                "    # ì¶”ë¡  (LoRA adapter ì ìš©)\n",
                "    outputs = llm_with_lora.generate(\n",
                "        [prompt],\n",
                "        sampling_params,\n",
                "        lora_request=lora_request\n",
                "    )\n",
                "    \n",
                "    print(\"âœ… LoRA adapter ì ìš© ì™„ë£Œ\")\n",
                "    print(outputs)\n",
                "else:\n",
                "    print(\"âš ï¸ LoRA adapter ì—†ìŒ - ê±´ë„ˆë›°ê¸°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### ë°©ë²• 2: Merged Model (ì‚¬ì „ í†µí•©)\n",
                "- LoRA weightsë¥¼ base modelì— ë¯¸ë¦¬ merge\n",
                "- ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„\n",
                "- ë°°í¬ì— ìœ ë¦¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ”„ LoRA merge ì‹œì‘...\n",
                        "âœ… Merge ì™„ë£Œ: ./merged_model\n",
                        "INFO 01-20 00:04:20 [utils.py:233] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.75, 'disable_log_stats': True, 'model': './merged_model'}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 01-20 00:04:20 [model.py:547] Resolved architecture: LlamaForCausalLM\n",
                        "INFO 01-20 00:04:20 [model.py:1510] Using max model len 8192\n",
                        "INFO 01-20 00:04:20 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
                        "INFO 01-20 00:04:24 [__init__.py:216] Automatically detected platform cuda.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:25 [core.py:644] Waiting for init message from front-end.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:25 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='./merged_model', speculative_config=None, tokenizer='./merged_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=./merged_model, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m WARNING 01-20 00:04:26 [interface.py:381] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:26 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m WARNING 01-20 00:04:27 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:27 [gpu_model_runner.py:2602] Starting to load model ./merged_model...\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:27 [gpu_model_runner.py:2634] Loading model from scratch...\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:27 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.49it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.48it/s]\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:27 [default_loader.py:267] Loading weights took 0.24 seconds\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:28 [gpu_model_runner.py:2653] Model loading took 1.0568 GiB and 0.419945 seconds\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:32 [backends.py:548] Using cache directory: /home/ssafy/.cache/vllm/torch_compile_cache/e5b9f55dde/rank_0_0/backbone for vLLM's torch.compile\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:32 [backends.py:559] Dynamo bytecode transform time: 3.33 s\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:33 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.057 s\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:33 [monitor.py:34] torch.compile takes 3.33 s in total\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:35 [gpu_worker.py:298] Available KV cache memory: 2.39 GiB\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:35 [kv_cache_utils.py:1087] GPU KV cache size: 26,064 tokens\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:35 [kv_cache_utils.py:1091] Maximum concurrency for 8,192 tokens per request: 3.18x\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 26.90it/s]\n",
                        "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 29.13it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:40 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.53 GiB\n",
                        "\u001b[1;36m(EngineCore_DP0 pid=12642)\u001b[0;0m INFO 01-20 00:04:40 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.57 seconds\n",
                        "INFO 01-20 00:04:40 [llm.py:306] Supported_tasks: ['generate']\n",
                        "âœ… Merged ëª¨ë¸ë¡œ ì¶”ë¡  ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoModelForCausalLM\n",
                "from peft import PeftModel\n",
                "\n",
                "def merge_lora_to_base(base_model_name, lora_path, output_path):\n",
                "    \"\"\"\n",
                "    LoRA adapterë¥¼ base modelì— merge\n",
                "    \"\"\"\n",
                "    print(\"ğŸ”„ LoRA merge ì‹œì‘...\")\n",
                "    \n",
                "    # 1. Base model ë¡œë“œ\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\n",
                "        base_model_name,\n",
                "        trust_remote_code=True,\n",
                "        torch_dtype=\"auto\"\n",
                "    )\n",
                "    \n",
                "    # 2. LoRA adapter ë¡œë“œ\n",
                "    model_with_lora = PeftModel.from_pretrained(base_model, lora_path)\n",
                "    \n",
                "    # 3. Merge\n",
                "    merged_model = model_with_lora.merge_and_unload()\n",
                "    \n",
                "    # 4. ì €ì¥\n",
                "    os.makedirs(output_path, exist_ok=True)\n",
                "    merged_model.save_pretrained(output_path, safe_serialization=True)\n",
                "    \n",
                "    # Tokenizerë„ ì €ì¥\n",
                "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
                "    tokenizer.save_pretrained(output_path)\n",
                "\n",
                "    del base_model\n",
                "    del model_with_lora\n",
                "    del merged_model\n",
                "    \n",
                "    print(f\"âœ… Merge ì™„ë£Œ: {output_path}\")\n",
                "    return output_path\n",
                "\n",
                "# ì˜ˆì œ (adapterê°€ ìˆëŠ” ê²½ìš°)\n",
                "if os.path.exists(lora_adapter_path):\n",
                "    if locals().get('llm_with_lora'):\n",
                "        del llm_with_lora\n",
                "\n",
                "    merged_path = merge_lora_to_base(\n",
                "        base_model_name=model_name,\n",
                "        lora_path=lora_adapter_path,\n",
                "        output_path=\"./merged_model\"\n",
                "    )\n",
                "    \n",
                "    # Merged ëª¨ë¸ë¡œ ì¶”ë¡ \n",
                "    llm_merged = LLM(\n",
                "        model=merged_path,\n",
                "        tensor_parallel_size=1,\n",
                "        gpu_memory_utilization=0.75,\n",
                "        trust_remote_code=True,\n",
                "    )\n",
                "    print(\"âœ… Merged ëª¨ë¸ë¡œ ì¶”ë¡  ì¤€ë¹„ ì™„ë£Œ\")\n",
                "else:\n",
                "    print(\"âš ï¸ LoRA adapter ì—†ìŒ - ê±´ë„ˆë›°ê¸°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 5: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ - Transformers vs vLLM\n",
                "\n",
                "vLLMì´ ì–¼ë§ˆë‚˜ ë¹ ë¥¸ì§€ ì§ì ‘ ì¸¡ì •í•´ë´…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì„¤ì •\n",
                        "  - í”„ë¡¬í”„íŠ¸ ìˆ˜: 5\n",
                        "  - Max Tokens: 64\n"
                    ]
                }
            ],
            "source": [
                "# ë²¤ì¹˜ë§ˆí¬ ì¤€ë¹„\n",
                "test_prompts = [\n",
                "    \"You are a SQL expert. Convert this to SQL: Find all users\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Count employees\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Show top 10 sales\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Delete inactive accounts\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Update user emails\",\n",
                "]\n",
                "\n",
                "max_tokens = 64\n",
                "\n",
                "print(f\"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì„¤ì •\")\n",
                "print(f\"  - í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(test_prompts)}\")\n",
                "print(f\"  - Max Tokens: {max_tokens}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Transformers ë²¤ì¹˜ë§ˆí¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ”µ Transformers ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\n",
                        "  ëª¨ë¸ ë¡œë”© ì¤‘...\n",
                        "\n",
                        "âœ… Transformers ì™„ë£Œ\n",
                        "  â±ï¸  ì´ ì‹œê°„: 9.77s\n",
                        "  âš¡ First Token: 86.84ms\n",
                        "  ğŸ”¥ Token/sec: 32.74\n",
                        "  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: 1095 MB\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "print(\"ğŸ”µ Transformers ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\")\n",
                "print(\"  ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
                "\n",
                "# ëª¨ë¸ ë¡œë“œ\n",
                "tf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tf_model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "tf_model.eval()\n",
                "\n",
                "# GPU ë©”ëª¨ë¦¬ ë¦¬ì…‹\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.reset_peak_memory_stats()\n",
                "\n",
                "# ì¶”ë¡ \n",
                "tf_start = time.time()\n",
                "tf_total_tokens = 0\n",
                "tf_first_token_latencies = []\n",
                "\n",
                "for prompt in test_prompts:\n",
                "    inputs = tf_tokenizer(prompt, return_tensors=\"pt\").to(tf_model.device)\n",
                "    \n",
                "    # First token ì¸¡ì •\n",
                "    ft_start = time.time()\n",
                "    with torch.no_grad():\n",
                "        outputs = tf_model(**inputs)\n",
                "    first_token_time = time.time() - ft_start\n",
                "    tf_first_token_latencies.append(first_token_time)\n",
                "    \n",
                "    # ì „ì²´ ìƒì„±\n",
                "    with torch.no_grad():\n",
                "        generated = tf_model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_tokens,\n",
                "            do_sample=False,\n",
                "        )\n",
                "    tf_total_tokens += generated.shape[1] - inputs.input_ids.shape[1]\n",
                "\n",
                "tf_time = time.time() - tf_start\n",
                "tf_peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0\n",
                "tf_avg_first_token = sum(tf_first_token_latencies) / len(tf_first_token_latencies)\n",
                "\n",
                "print(f\"\\nâœ… Transformers ì™„ë£Œ\")\n",
                "print(f\"  â±ï¸  ì´ ì‹œê°„: {tf_time:.2f}s\")\n",
                "print(f\"  âš¡ First Token: {tf_avg_first_token*1000:.2f}ms\")\n",
                "print(f\"  ğŸ”¥ Token/sec: {tf_total_tokens / tf_time:.2f}\")\n",
                "print(f\"  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: {tf_peak_memory:.0f} MB\")\n",
                "\n",
                "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
                "del tf_model\n",
                "del tf_tokenizer\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 vLLM ë²¤ì¹˜ë§ˆí¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ğŸŸ¢ vLLM ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5d3e3512c1784c6b8b8ea8d9d483fea4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "12795abe5ec942668cfedabe3606c6cc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cb5c972993e0412c856798296186e8b6",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/5 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "55ca65f550f84dbb88a39ceccbfba61b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… vLLM ì™„ë£Œ\n",
                        "  â±ï¸  ì´ ì‹œê°„: 0.58s\n",
                        "  âš¡ First Token: 771.25ms\n",
                        "  ğŸ”¥ Token/sec: 554.15\n",
                        "  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: 8 MB\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\nğŸŸ¢ vLLM ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\")\n",
                "\n",
                "# GPU ë©”ëª¨ë¦¬ ë¦¬ì…‹\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.reset_peak_memory_stats()\n",
                "\n",
                "# Sampling íŒŒë¼ë¯¸í„°\n",
                "benchmark_params = SamplingParams(\n",
                "    max_tokens=max_tokens,\n",
                "    temperature=0.0,  # greedy decoding\n",
                ")\n",
                "\n",
                "# First token latency ì¸¡ì •\n",
                "vllm_ft_start = time.time()\n",
                "_ = llm_merged.generate([test_prompts[0]], benchmark_params)\n",
                "vllm_first_token = time.time() - vllm_ft_start\n",
                "\n",
                "# ì „ì²´ ì¶”ë¡ \n",
                "vllm_start = time.time()\n",
                "vllm_outputs = llm_merged.generate(test_prompts, benchmark_params)\n",
                "vllm_time = time.time() - vllm_start\n",
                "\n",
                "vllm_peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0\n",
                "vllm_total_tokens = sum(len(out.outputs[0].token_ids) for out in vllm_outputs)\n",
                "\n",
                "print(f\"\\nâœ… vLLM ì™„ë£Œ\")\n",
                "print(f\"  â±ï¸  ì´ ì‹œê°„: {vllm_time:.2f}s\")\n",
                "print(f\"  âš¡ First Token: {vllm_first_token*1000:.2f}ms\")\n",
                "print(f\"  ğŸ”¥ Token/sec: {vllm_total_tokens / vllm_time:.2f}\")\n",
                "print(f\"  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: {vllm_peak_memory:.0f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 ê²°ê³¼ ë¹„êµ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "ğŸ“Š ìµœì¢… ë¹„êµ ê²°ê³¼\n",
                        "============================================================\n",
                        "\n",
                        "âš¡ First Token Latency:\n",
                        "  Transformers: 86.84ms\n",
                        "  vLLM:         771.25ms\n",
                        "  âš ï¸ ì°¨ì´: 788.1%\n",
                        "\n",
                        "ğŸ”¥ Token/sec:\n",
                        "  Transformers: 32.74 tokens/sec\n",
                        "  vLLM:         554.15 tokens/sec\n",
                        "  ğŸš€ vLLM í–¥ìƒ: 16.93x\n",
                        "\n",
                        "â±ï¸ ì´ ì¶”ë¡  ì‹œê°„:\n",
                        "  Transformers: 9.77s\n",
                        "  vLLM:         0.58s\n",
                        "  ğŸš€ ì†ë„ í–¥ìƒ: 16.93x\n",
                        "\n",
                        "ğŸ’¾ í”¼í¬ GPU ë©”ëª¨ë¦¬:\n",
                        "  Transformers: 1095 MB\n",
                        "  vLLM:         8 MB\n",
                        "  ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆê°: 1087 MB (99.3%)\n",
                        "\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"ğŸ“Š ìµœì¢… ë¹„êµ ê²°ê³¼\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# First Token Latency\n",
                "print(f\"\\nâš¡ First Token Latency:\")\n",
                "print(f\"  Transformers: {tf_avg_first_token*1000:.2f}ms\")\n",
                "print(f\"  vLLM:         {vllm_first_token*1000:.2f}ms\")\n",
                "ft_improvement = ((tf_avg_first_token - vllm_first_token) / tf_avg_first_token) * 100\n",
                "print(f\"  {'ğŸš€ ê°œì„ ' if ft_improvement > 0 else 'âš ï¸ ì°¨ì´'}: {abs(ft_improvement):.1f}%\")\n",
                "\n",
                "# Token/sec\n",
                "tf_tps = tf_total_tokens / tf_time\n",
                "vllm_tps = vllm_total_tokens / vllm_time\n",
                "print(f\"\\nğŸ”¥ Token/sec:\")\n",
                "print(f\"  Transformers: {tf_tps:.2f} tokens/sec\")\n",
                "print(f\"  vLLM:         {vllm_tps:.2f} tokens/sec\")\n",
                "print(f\"  ğŸš€ vLLM í–¥ìƒ: {vllm_tps / tf_tps:.2f}x\")\n",
                "\n",
                "# ì´ ì‹œê°„\n",
                "print(f\"\\nâ±ï¸ ì´ ì¶”ë¡  ì‹œê°„:\")\n",
                "print(f\"  Transformers: {tf_time:.2f}s\")\n",
                "print(f\"  vLLM:         {vllm_time:.2f}s\")\n",
                "print(f\"  ğŸš€ ì†ë„ í–¥ìƒ: {tf_time / vllm_time:.2f}x\")\n",
                "\n",
                "# ë©”ëª¨ë¦¬\n",
                "print(f\"\\nğŸ’¾ í”¼í¬ GPU ë©”ëª¨ë¦¬:\")\n",
                "print(f\"  Transformers: {tf_peak_memory:.0f} MB\")\n",
                "print(f\"  vLLM:         {vllm_peak_memory:.0f} MB\")\n",
                "memory_diff = tf_peak_memory - vllm_peak_memory\n",
                "if memory_diff > 0:\n",
                "    print(f\"  ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆê°: {memory_diff:.0f} MB ({memory_diff/tf_peak_memory*100:.1f}%)\")\n",
                "else:\n",
                "    print(f\"  ğŸ“ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©: {abs(memory_diff):.0f} MB ë” ì‚¬ìš©\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6: ì¢…í•© ì‹¤ìŠµ\n",
                "\n",
                "### ìµœì¢… í”„ë¡œì íŠ¸: Text-to-SQL ì‹œìŠ¤í…œ êµ¬ì¶•\n",
                "\n",
                "ë°°ìš´ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì‹¤ì „ Text-to-SQL ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ¤– ì¶”ë¡  ì‹œì‘...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8913b1f995804caaa25dc0612212e88e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ab34b89009bf48549172b0af37285b71",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… ì¶”ë¡  ì™„ë£Œ! (0.56ì´ˆ)\n",
                        "ğŸ“Š ì²˜ë¦¬ëŸ‰: 7.17 prompts/sec\n",
                        "\n",
                        "[ìƒì„± ê²°ê³¼] SELECT * FROM user WHERE age  >  30\n",
                        "[í† í° ìˆ˜] 12\n",
                        "------------------------------------------------------------\n",
                        "[ìƒì„± ê²°ê³¼] SELECT department ,  COUNT(*) FROM Employee GROUP BY department\n",
                        "[í† í° ìˆ˜] 12\n",
                        "------------------------------------------------------------\n",
                        "[ìƒì„± ê²°ê³¼] SELECT TOP 5 order_by FROM user_order ORDER BY order_by DESC;\n",
                        "[í† í° ìˆ˜] 16\n",
                        "------------------------------------------------------------\n",
                        "[ìƒì„± ê²°ê³¼] SELECT department FROM department_table WHERE budget > 100000;\n",
                        "[í† í° ìˆ˜] 13\n",
                        "------------------------------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "system_prompt = \"\"\"You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query.\"\"\"\n",
                "user_prompt = \"\"\"Given the <USER_QUERY>, generate the corresponding SQL command to retrieve the desired data, considering the query's syntax, semantics, and schema constraints.\n",
                "\n",
                "<USER_QUERY>\n",
                "{question}\n",
                "</USER_QUERY>\"\"\"\n",
                "\n",
                "test_questions = [\n",
                "    \"Find all users older than 30\",\n",
                "    \"Count employees in each department\",\n",
                "    \"Show top 5 users by order amount\",\n",
                "    \"List departments with budget over 100000\",\n",
                "]\n",
                "\n",
                "messages = []\n",
                "for i in range(len(test_questions)):\n",
                "    messages.append(\n",
                "        [\n",
                "            {\n",
                "                \"role\": \"system\",\n",
                "                \"content\": system_prompt\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": user_prompt.format(question=test_questions[i])\n",
                "            }\n",
                "        ]\n",
                "    )\n",
                "\n",
                "prompts = format_chat_messages(messages)\n",
                "\n",
                "print(\"ğŸ¤– ì¶”ë¡  ì‹œì‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "outputs = llm_merged.generate(prompts, sampling_params)\n",
                "\n",
                "inference_time = time.time() - start_time\n",
                "\n",
                "print(f\"\\nâœ… ì¶”ë¡  ì™„ë£Œ! ({inference_time:.2f}ì´ˆ)\")\n",
                "print(f\"ğŸ“Š ì²˜ë¦¬ëŸ‰: {len(prompts) / inference_time:.2f} prompts/sec\\n\")\n",
                "\n",
                "# ê²°ê³¼ ì¶œë ¥\n",
                "for i, output in enumerate(outputs):\n",
                "    generated_text = output.outputs[0].text\n",
                "    token_count = len(output.outputs[0].token_ids)\n",
                "    \n",
                "    print(f\"[ìƒì„± ê²°ê³¼] {generated_text}\")\n",
                "    print(f\"[í† í° ìˆ˜] {token_count}\")\n",
                "    print(\"-\" * 60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
