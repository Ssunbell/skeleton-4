{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Objectives**\n",
                "\n",
                "1. ì‹¤ìŠµëª… : vLLMì„ ì´ìš©í•œ íš¨ìœ¨ì ì¸ LLM ì¶”ë¡ ê³¼ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹\n",
                "2. í•µì‹¬ ì£¼ì œ:\n",
                "    1. vLLMì˜ PagedAttentionê³¼ KV Cache ê°œë… ì´í•´\n",
                "    2. LoRA adapter í†µí•© ë°©ë²• (Runtime LoRA vs Merged Model)\n",
                "    3. Transformers vs vLLM ì„±ëŠ¥ ë¹„êµ (ì†ë„, ë©”ëª¨ë¦¬, ì²˜ë¦¬ëŸ‰)\n",
                "    4. Text-to-SQL taskë¥¼ ìœ„í•œ ì‹¤ì „ ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬ì¶•\n",
                "3. í•™ìŠµ ëª©í‘œ :\n",
                "    1. vLLMì˜ PagedAttention ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•˜ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì˜ ì›ë¦¬ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\n",
                "    2. LoRA adapterë¥¼ vLLMì— í†µí•©í•˜ì—¬ ì¶”ë¡ ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n",
                "    3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ë¥¼ í†µí•´ vLLMì˜ ì¥ì ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆë‹¤.\n",
                "4. í•™ìŠµ ê°œë…: í‚¤ì›Œë“œëª… :\n",
                "    1. PagedAttention\n",
                "    2. KV Cache\n",
                "    3. vLLM\n",
                "    4. LoRA adapter\n",
                "    5. Performance Benchmarking\n",
                "5. í•™ìŠµ ë°©í–¥ :\n",
                "  - vLLMì˜ í•µì‹¬ ê¸°ìˆ ì¸ PagedAttentionì´ ì–´ë–»ê²Œ ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ”ì§€ ì‹œë®¬ë ˆì´í„°ë¥¼ í†µí•´ ì²´í—˜í•©ë‹ˆë‹¤.\n",
                "  - ì‹¤ìŠµ ì½”ë“œëŠ” ë‹¨ê³„ë³„ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, Jupyter notebookì„ í†µí•´ ì´ë¡ ê³¼ ì‹¤ìŠµì„ ë³‘í–‰í•©ë‹ˆë‹¤.\n",
                "  - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ì„ í†µí•´ ì¼ë°˜ ì¶”ë¡  ë°©ì‹ê³¼ vLLMì˜ ì°¨ì´ë¥¼ ì§ì ‘ í™•ì¸í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Prerequisites**\n",
                "```\n",
                "numpy==2.1.0\n",
                "pandas==2.2.3\n",
                "transformers==4.56.0\n",
                "torch==2.9.0+cu129\n",
                "accelerate==1.10.1\n",
                "bitsandbytes==0.49.1\n",
                "datasets==4.0.0\n",
                "peft==0.17.1\n",
                "vllm==0.13.0\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1: vLLM ì•„í‚¤í…ì²˜ ë° í•µì‹¬ ì›ë¦¬\n",
                "        \n",
                "### 1.1 ë„ì… ë°°ê²½ (Why vLLM?)\n",
                "\n",
                "ê¸°ì¡´ HuggingFace Transformers ê¸°ë°˜ ì„œë¹™ ë°©ì‹ì€ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë³‘ëª©(Bottleneck)ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
                "\n",
                "* **ë©”ëª¨ë¦¬ ë‹¨í¸í™”(Fragmentation)**: KV Cacheë¥¼ ì—°ì†ëœ ë©”ëª¨ë¦¬ ê³µê°„ì— í• ë‹¹í•´ì•¼ í•˜ë¯€ë¡œ, ì‹¤ì œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì˜ˆì•½ ë©”ëª¨ë¦¬(Internal Fragmentation)ë¡œ ì¸í•œ VRAM ë‚­ë¹„ê°€ ì‹¬ê°í•©ë‹ˆë‹¤.\n",
                "* **ë‚®ì€ ì²˜ë¦¬ëŸ‰(Throughput)**: Static Batching ë°©ì‹ì˜ ë¹„íš¨ìœ¨ì„±ìœ¼ë¡œ ì¸í•´ ë™ì‹œ ì ‘ì† ì²˜ë¦¬ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.\n",
                "\n",
                "**vLLMì˜ ì†”ë£¨ì…˜:**\n",
                "\n",
                "* **PagedAttention**: OSì˜ ê°€ìƒ ë©”ëª¨ë¦¬(Virtual Memory) ê¸°ë²•ì„ ì°¨ìš©í•˜ì—¬ KV Cacheë¥¼ ë¹„ì—°ì†ì  ë¸”ë¡ìœ¼ë¡œ ê´€ë¦¬, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.\n",
                "* **Continuous Batching**: ìš”ì²­ì´ ë“¤ì–´ì˜¤ëŠ” ì¦‰ì‹œ ë™ì ìœ¼ë¡œ ë°°ì¹˜ì— í¬í•¨ì‹œì¼œ GPU ê°€ë™ë¥ (Utilization)ì„ ë†’ì…ë‹ˆë‹¤.\n",
                "\n",
                "### 1.2 í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ 1: PagedAttention\n",
                "\n",
                "PagedAttentionì€ vLLM ì„±ëŠ¥ í–¥ìƒì˜ í•µì‹¬ìœ¼ë¡œ, ìš´ì˜ì²´ì œì˜ í˜ì´ì§•(Paging) ì‹œìŠ¤í…œê³¼ ë™ì¼í•œ ì›ë¦¬ì…ë‹ˆë‹¤.\n",
                "\n",
                "**Legacy ë°©ì‹ (Contiguous Allocation)**:\n",
                "\n",
                "* ìš”ì²­ ì‹œ ìµœëŒ€ í† í° ê¸¸ì´(Max Context Length)ë§Œí¼ì˜ ì—°ì†ëœ ë©”ëª¨ë¦¬ë¥¼ ë¯¸ë¦¬ í™•ë³´í•©ë‹ˆë‹¤.\n",
                "* ìƒì„±ë˜ì§€ ì•Šì€ ë¯¸ë˜ì˜ í† í°ì„ ìœ„í•œ ê³µê°„ê¹Œì§€ ì ìœ í•˜ë¯€ë¡œ **ë©”ëª¨ë¦¬ ë‚­ë¹„(Waste)**ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
                "\n",
                "```\n",
                "Physical Memory: [Token 1...Token N | Reserved (Unused) Space]\n",
                "-> ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë° OOM(Out of Memory)ì˜ ì£¼ì›ì¸\n",
                "\n",
                "```\n",
                "\n",
                "**PagedAttention ë°©ì‹ (Non-contiguous Allocation)**:\n",
                "\n",
                "* KV Cacheë¥¼ ê³ ì • í¬ê¸°ì˜ 'ë¸”ë¡(Block)' ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
                "* ë¬¼ë¦¬ì ìœ¼ë¡œ ë–¨ì–´ì§„ ë©”ëª¨ë¦¬ ê³µê°„ì´ë¼ë„ **Block Table**ì„ í†µí•´ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ì†ëœ ê²ƒì²˜ëŸ¼ ë§¤í•‘í•©ë‹ˆë‹¤.\n",
                "* í•„ìš”í•  ë•Œë§Œ ë¸”ë¡ì„ ë™ì  í• ë‹¹í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ë‚­ë¹„ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤(Near-zero waste).\n",
                "\n",
                "```\n",
                "Block 0 (Physical): [Token 0-3]\n",
                "Block 1 (Physical): [Token 4-7]\n",
                "Block Table (Logical): {Seq_A: [Block 0, Block 1, ...]}\n",
                "-> ë¹„ì—°ì†ì  í• ë‹¹ ê°€ëŠ¥, ìœ ì—°í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
                "\n",
                "```\n",
                "\n",
                "### 1.3 í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ 2: KV Cache ìµœì í™”\n",
                "\n",
                "LLMì€ ìê¸°íšŒê·€(Autoregressive) ëª¨ë¸ì´ë¯€ë¡œ, ì´ì „ í† í°ë“¤ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
                "\n",
                "**KV Cache ë¶€ì¬ ì‹œ (Redundant Computation)**:\n",
                "\n",
                "* ë§¤ ìŠ¤í…ë§ˆë‹¤ ì´ì „ ì‹œì ()ê¹Œì§€ì˜ ëª¨ë“  í† í°ì— ëŒ€í•´ Attention ì—°ì‚°ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
                "* ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì—°ì‚°ëŸ‰ì´ $O(N^2)$ë¡œ ì¦ê°€í•˜ì—¬ Latencyê°€ ê¸‰ê²©íˆ ë‚˜ë¹ ì§‘ë‹ˆë‹¤.\n",
                "\n",
                "**KV Cache ì ìš© ì‹œ (Memory-Space Tradeoff)**:\n",
                "\n",
                "* ì´ì „ í† í°ë“¤ì˜ Key, Value ë²¡í„°ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ VRAMì— ìºì‹±í•´ë‘¡ë‹ˆë‹¤.\n",
                "* í˜„ì¬ ì‹œì ()ì˜ í† í°ë§Œ ì—°ì‚°í•˜ê³ , ê³¼ê±° ë°ì´í„°ëŠ” ìºì‹œë¥¼ ì°¸ì¡°(Lookup)í•˜ì—¬ ì—°ì‚° ë³µì¡ë„ë¥¼ ì„ í˜•ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.\n",
                "\n",
                "**Critical Issue**:\n",
                "\n",
                "* KV CacheëŠ” ëª¨ë¸ ì‚¬ì´ì¦ˆì™€ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë¹„ë¡€í•˜ì—¬ ë§¤ìš° í° VRAMì„ ì ìœ í•©ë‹ˆë‹¤.\n",
                "* ê²°êµ­ **\"ì œí•œëœ GPU ë©”ëª¨ë¦¬ì— ì–¼ë§ˆë‚˜ ë§ì€ KV Cacheë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ê²¨ ë„£ëŠëƒ\"**ê°€ ì„œë¹™ ì„±ëŠ¥(Throughput)ì˜ ê²°ì •ì  ìš”ì†Œê°€ ë˜ë©°, ì´ê²ƒì´ vLLMì´ PagedAttentionì„ ë„ì…í•œ ê¸°ìˆ ì  ë°°ê²½ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch ë²„ì „: 2.9.0+cu129\n",
                        "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
                        "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
                    ]
                }
            ],
            "source": [
                "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
                "import os\n",
                "import time\n",
                "import torch\n",
                "from vllm import LLM, SamplingParams\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
                "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.4: PagedAttention ë©”ì»¤ë‹ˆì¦˜ ì‹œë®¬ë ˆì´ì…˜\n",
                "\n",
                "ì´ë¡ ì  ê°œë…ì„ êµ¬ì²´í™”í•˜ê¸° ìœ„í•´, Python í™˜ê²½ì—ì„œ **PagedAttentionì˜ ë©”ëª¨ë¦¬ í• ë‹¹ ë¡œì§**ì„ ì‹œë®¬ë ˆì´ì…˜í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°€ìƒ ë©”ëª¨ë¦¬ ì£¼ì†Œê°€ ì‹¤ì œ GPU ë©”ëª¨ë¦¬ì— ë§¤í•‘ë˜ëŠ” ê³¼ì •ì„ ì½”ë“œ ë ˆë²¨ì—ì„œ ê²€ì¦í•©ë‹ˆë‹¤.\n",
                "\n",
                "### ì‹œë®¬ë ˆì´í„° í•µì‹¬ ì»´í¬ë„ŒíŠ¸ (Simulator Components)\n",
                "\n",
                "* **Logical Memory (Virtual Address Space)**\n",
                "  * ëª¨ë¸ì´ ì¸ì‹í•˜ëŠ” ë…¼ë¦¬ì ì¸ í† í° ì‹œí€€ìŠ¤ì…ë‹ˆë‹¤.\n",
                "  * ì‚¬ìš©ì ì…ì¥ì—ì„œëŠ” ë°ì´í„°ê°€ ì—°ì†ì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* **Physical Memory (Physical Address Space)**\n",
                "  * ì‹¤ì œ KV Cache ë°ì´í„°ê°€ ì ì¬ë˜ëŠ” ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ ê³µê°„(VRAM)ì…ë‹ˆë‹¤.\n",
                "  * ë¸”ë¡(Block) ë‹¨ìœ„ë¡œ ë¹„ì—°ì†ì ìœ¼ë¡œ í• ë‹¹ ë° ê´€ë¦¬ë©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* **Block Table (Address Translation)**\n",
                "  * Logical Blockì„ Physical Blockìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë§¤í•‘ í…Œì´ë¸”ì…ë‹ˆë‹¤.\n",
                "  * OSì˜ í˜ì´ì§€ í…Œì´ë¸”(Page Table)ê³¼ ë™ì¼í•œ ì—­í• ì„ ìˆ˜í–‰í•˜ì—¬ ë©”ëª¨ë¦¬ ì°¸ì¡°ë¥¼ ì¤‘ê°œí•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… PagedAttention ì‹œë®¬ë ˆì´í„° í´ë˜ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "import random\n",
                "from typing import List, Dict, Optional\n",
                "\n",
                "class PhysicalBlock:\n",
                "    \"\"\"\n",
                "    ë¬¼ë¦¬ì ì¸ ë©”ëª¨ë¦¬ ë¸”ë¡ (GPUì˜ ì‘ì€ ì¡°ê°)\n",
                "    ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ í•˜ë‚˜ì˜ ë¸”ë¡ì— 4ê°œì˜ í† í°ì„ ì €ì¥í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤ (BLOCK_SIZE = 4).\n",
                "    \"\"\"\n",
                "    def __init__(self, block_id: int, block_size: int = 4):\n",
                "        self.block_id = block_id\n",
                "        self.size = block_size\n",
                "        self.data = [None] * block_size  # ì²˜ìŒì—” ë¹„ì–´ìˆìŒ\n",
                "        self.filled_count = 0\n",
                "\n",
                "    def append(self, token: str) -> bool:\n",
                "        if self.filled_count < self.size:\n",
                "            self.data[self.filled_count] = token\n",
                "            self.filled_count += 1\n",
                "            return True  # ì €ì¥ ì„±ê³µ\n",
                "        return False  # ê½‰ ì°¨ì„œ ì‹¤íŒ¨\n",
                "\n",
                "    def __repr__(self):\n",
                "        return f\"[Block {self.block_id}]: {self.data}\"\n",
                "\n",
                "class VLLMMemoryManager:\n",
                "    \"\"\"\n",
                "    vLLMì˜ í•µì‹¬ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì (OS ì—­í• )\n",
                "    \"\"\"\n",
                "    def __init__(self, total_blocks: int = 16, block_size: int = 4):\n",
                "        self.block_size = block_size\n",
                "        # 1. ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ ê³µê°„ ìƒì„± (GPU VRAM)\n",
                "        self.physical_memory = [PhysicalBlock(i, block_size) for i in range(total_blocks)]\n",
                "        \n",
                "        # 2. ì‚¬ìš© ê°€ëŠ¥í•œ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ (Free List)\n",
                "        # ì‹¤ì œë¡œëŠ” ë¬¼ë¦¬ì ìœ¼ë¡œ ë–¨ì–´ì ¸ ìˆì–´ë„ ìƒê´€ì—†ìŒì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì„ìŠµë‹ˆë‹¤.\n",
                "        self.free_blocks = list(range(total_blocks))\n",
                "        random.shuffle(self.free_blocks) \n",
                "        \n",
                "        # 3. ìš”ì²­ë³„ ë¸”ë¡ ë§¤í•‘ í…Œì´ë¸” (Request ID -> List[Physical Block IDs])\n",
                "        self.block_tables: Dict[str, List[int]] = {}\n",
                "\n",
                "    def allocate_block(self) -> Optional[int]:\n",
                "        \"\"\"ë¹ˆ ë¬¼ë¦¬ ë¸”ë¡ í•˜ë‚˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
                "        if not self.free_blocks:\n",
                "            return None # OOM (Out of Memory)\n",
                "        return self.free_blocks.pop()\n",
                "\n",
                "    def append_token(self, request_id: str, token: str):\n",
                "        \"\"\"\n",
                "        í•µì‹¬ ë¡œì§: í† í°ì„ ìƒì„±í•´ì„œ KV Cacheì— ì¶”ê°€í•˜ëŠ” ê³¼ì •\n",
                "        \"\"\"\n",
                "        # 1. í•´ë‹¹ ìš”ì²­ì˜ ë¸”ë¡ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±\n",
                "        if request_id not in self.block_tables:\n",
                "            print(f\"--- [New Request] '{request_id}' ì‹œì‘ ---\")\n",
                "            self.block_tables[request_id] = []\n",
                "\n",
                "        # 2. í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë§ˆì§€ë§‰ ë¬¼ë¦¬ ë¸”ë¡ í™•ì¸\n",
                "        current_block_id = None\n",
                "        if self.block_tables[request_id]:\n",
                "            current_block_id = self.block_tables[request_id][-1]\n",
                "        \n",
                "        # 3. ë¸”ë¡ì´ ì—†ê±°ë‚˜ ê½‰ ì°¼ìœ¼ë©´, ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ í• ë‹¹ (PagedAttentionì˜ í•µì‹¬!)\n",
                "        if current_block_id is None or \\\n",
                "           self.physical_memory[current_block_id].filled_count >= self.block_size:\n",
                "            \n",
                "            new_block_id = self.allocate_block()\n",
                "            if new_block_id is None:\n",
                "                raise Exception(\"GPU Memory Full!\")\n",
                "            \n",
                "            self.block_tables[request_id].append(new_block_id)\n",
                "            current_block_id = new_block_id\n",
                "            print(f\"ğŸ‘‰ [Alloc] ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ {current_block_id}ë²ˆ í• ë‹¹ë¨\")\n",
                "\n",
                "        # 4. ë¬¼ë¦¬ ë¸”ë¡ì— ë°ì´í„° ì €ì¥\n",
                "        self.physical_memory[current_block_id].append(token)\n",
                "        print(f\"   [Write] í† í° '{token}' -> ë¬¼ë¦¬ ë¸”ë¡ {current_block_id}ë²ˆì— ì €ì¥\")\n",
                "\n",
                "    def print_state(self, request_id: str):\n",
                "        \"\"\"í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ ì‹œê°í™”í•´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.\"\"\"\n",
                "        print(f\"\\nğŸ“Š [{request_id}] ì˜ PagedAttention ìƒíƒœ\")\n",
                "        table = self.block_tables.get(request_id, [])\n",
                "        \n",
                "        # 1. Logical View (ì‚¬ìš©ìê°€ ë³´ëŠ” ë¬¸ì¥)\n",
                "        logical_text = []\n",
                "        for block_id in table:\n",
                "            block = self.physical_memory[block_id]\n",
                "            logical_text.extend([t for t in block.data if t is not None])\n",
                "        print(f\"  1) ë…¼ë¦¬ì  ë·° (Logical): {logical_text}\")\n",
                "\n",
                "        # 2. Block Table (ë§¤í•‘ ì •ë³´)\n",
                "        print(f\"  2) ë¸”ë¡ í…Œì´ë¸” (Mapping): {table}\")\n",
                "\n",
                "        # 3. Physical View (ì‹¤ì œ ì €ì¥ ìœ„ì¹˜)\n",
                "        print(f\"  3) ë¬¼ë¦¬ì  ë·° (Physical):\")\n",
                "        for block_id in table:\n",
                "            print(f\"     {self.physical_memory[block_id]}\")\n",
                "        print(\"-\" * 50)\n",
                "\n",
                "print(\"âœ… PagedAttention ì‹œë®¬ë ˆì´í„° í´ë˜ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸš€ í† í° ìƒì„± ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\n",
                        "\n",
                        "--- [New Request] 'User_A' ì‹œì‘ ---\n",
                        "ğŸ‘‰ [Alloc] ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ 3ë²ˆ í• ë‹¹ë¨\n",
                        "   [Write] í† í° 'Deep' -> ë¬¼ë¦¬ ë¸”ë¡ 3ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'Learn' -> ë¬¼ë¦¬ ë¸”ë¡ 3ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'ing' -> ë¬¼ë¦¬ ë¸”ë¡ 3ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'is' -> ë¬¼ë¦¬ ë¸”ë¡ 3ë²ˆì— ì €ì¥\n",
                        "ğŸ‘‰ [Alloc] ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ 1ë²ˆ í• ë‹¹ë¨\n",
                        "   [Write] í† í° 'very' -> ë¬¼ë¦¬ ë¸”ë¡ 1ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'fun' -> ë¬¼ë¦¬ ë¸”ë¡ 1ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'to' -> ë¬¼ë¦¬ ë¸”ë¡ 1ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'study' -> ë¬¼ë¦¬ ë¸”ë¡ 1ë²ˆì— ì €ì¥\n",
                        "\n",
                        "ğŸ“Š [User_A] ì˜ PagedAttention ìƒíƒœ\n",
                        "  1) ë…¼ë¦¬ì  ë·° (Logical): ['Deep', 'Learn', 'ing', 'is', 'very', 'fun', 'to', 'study']\n",
                        "  2) ë¸”ë¡ í…Œì´ë¸” (Mapping): [3, 1]\n",
                        "  3) ë¬¼ë¦¬ì  ë·° (Physical):\n",
                        "     [Block 3]: ['Deep', 'Learn', 'ing', 'is']\n",
                        "     [Block 1]: ['very', 'fun', 'to', 'study']\n",
                        "--------------------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "# PagedAttention ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\n",
                "# LLMì´ í•œ í† í°ì”© ìƒì„±í•˜ëŠ” ìƒí™©ì„ í‰ë‚´ëƒ…ë‹ˆë‹¤\n",
                "\n",
                "# 1. ë§¤ë‹ˆì € ì´ˆê¸°í™” (Block Size = 4)\n",
                "vllm_manager = VLLMMemoryManager(total_blocks=10, block_size=4)\n",
                "\n",
                "# 2. ë¬¸ì¥ ìƒì„± ì‹œë®¬ë ˆì´ì…˜\n",
                "req_id = \"User_A\"\n",
                "tokens = [\"Deep\", \"Learn\", \"ing\", \"is\", \"very\", \"fun\", \"to\", \"study\"]\n",
                "\n",
                "print(\"ğŸš€ í† í° ìƒì„± ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\\n\")\n",
                "for token in tokens:\n",
                "    vllm_manager.append_token(req_id, token)\n",
                "\n",
                "# 3. ìµœì¢… ìƒíƒœ í™•ì¸\n",
                "vllm_manager.print_state(req_id)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ì‹¤í–‰ ê²°ê³¼ ë¶„ì„ ë° ê¸°ìˆ ì  ì‹œì‚¬ì \n",
                "\n",
                "ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ í†µí•´ PagedAttention ì•„í‚¤í…ì²˜ê°€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•˜ëŠ” êµ¬ì²´ì ì¸ ë©”ì»¤ë‹ˆì¦˜ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ìˆ ì  íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
                "\n",
                "#### ğŸ”‘ Key Technical Insights\n",
                "\n",
                "**1. ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ì˜ ë¹„ì—°ì†ì„± (Non-contiguous Allocation)**\n",
                "\n",
                "* **ê´€ì°°**: ë…¼ë¦¬ì  ì‹œí€€ìŠ¤ìƒì—ì„œëŠ” `'is'`(Block 0ì˜ ë)ì™€ `'very'`(Block 1ì˜ ì‹œì‘)ê°€ ì—°ì†ë˜ì§€ë§Œ, ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ ì£¼ì†Œ(Physical Block Index)ëŠ” ë¶ˆì—°ì†ì ìœ¼ë¡œ ë§¤í•‘ë©ë‹ˆë‹¤.\n",
                "* **ê¸°ìˆ ì  ì˜ì˜**: ë¬¼ë¦¬ì  ì£¼ì†Œ ê³µê°„ì˜ ì—°ì†ì„± ì œì•½ì„ ì œê±°í•¨ìœ¼ë¡œì¨, VRAM ë‚´ ì‚°ì¬í•œ ìœ íœ´ ë©”ëª¨ë¦¬ ì¡°ê°(Fragmented chunks)ì„ 100% í™œìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŠ” ë©”ëª¨ë¦¬ ë‹¨í¸í™”(Fragmentation) ë¬¸ì œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ í•´ê²°í•©ë‹ˆë‹¤.\n",
                "\n",
                "**2. í• ë‹¹ì˜ íš¨ìœ¨ì„± (On-demand Allocation)**\n",
                "\n",
                "* **ê´€ì°°**: ìƒˆë¡œìš´ ë¬¼ë¦¬ì  ë¸”ë¡ì€ í˜„ì¬ ë¸”ë¡ì´ ì™„ì „íˆ ì±„ì›Œì§„ ì‹œì (Slot full)ì—ë§Œ í• ë‹¹ë©ë‹ˆë‹¤.\n",
                "* **ê¸°ìˆ ì  ì˜ì˜**: ê¸°ì¡´ì˜ ì •ì  í• ë‹¹ ë°©ì‹ì—ì„œ ë°œìƒí•˜ëŠ” 'ì˜ˆì•½ëœ ë¯¸ì‚¬ìš© ê³µê°„(Reserved but unused memory)'ì— ì˜í•œ ë‚­ë¹„ë¥¼ ì œê±°í•©ë‹ˆë‹¤. ë¬¸ì¥ ìƒì„± ì¤‘ë‹¨ ì‹œ, ë¶ˆí•„ìš”í•œ ì¶”ê°€ ë©”ëª¨ë¦¬ í• ë‹¹ì´ ë°œìƒí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ VRAM ì ìœ ìœ¨(Footprint)ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.\n",
                "\n",
                "**3. ì£¼ì†Œ ë³€í™˜ ë§¤ì»¤ë‹ˆì¦˜ (Address Translation Logic)**\n",
                "\n",
                "* **ê´€ì°°**: vLLMì€ `Block Table`ì„ í†µí•´ ë…¼ë¦¬ì  ì¸ë±ìŠ¤ë¥¼ ë¬¼ë¦¬ì  ì£¼ì†Œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
                "* **Block Index**: `Global Token Index // Block Size`\n",
                "* **Block Offset**: `Global Token Index % Block Size`\n",
                "\n",
                "\n",
                "* **ê¸°ìˆ ì  ì˜ì˜**: ì´ ì—°ì‚° ê³¼ì •ì€ $O(1)$ì˜ ì‹œê°„ ë³µì¡ë„ë¥¼ ê°€ì§€ë¯€ë¡œ, í† í° ì¡°íšŒ(Lookup) ì‹œ ì˜¤ë²„í—¤ë“œê°€ ê±°ì˜ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¦‰, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ì–»ëŠ” ëŒ€ì‹  ê°ìˆ˜í•´ì•¼ í•  ì„±ëŠ¥ ë¹„ìš©(Trade-off)ì´ ë¬´ì‹œí•  ìˆ˜ì¤€ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2 vLLM ê¸°ë³¸ ì‚¬ìš©ë²•\n",
                "\n",
                "### 2.1 ëª¨ë¸ ë¡œë”©\n",
                "vLLMì˜ Inference Engine(LLM Class)ì„ ì´ˆê¸°í™”í•˜ëŠ” ì§„ì…ì (Entry Point)ì…ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œ í•˜ë“œì›¨ì–´ ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì „ëµê³¼ ëª¨ë¸ ë¡œë”© ë°©ì‹ì´ ê²°ì •ë˜ë¯€ë¡œ, ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”ì— ìˆì–´ ê°€ì¥ ì¤‘ìš”í•œ ì„¤ì •ì…ë‹ˆë‹¤.\n",
                "\n",
                "### vLLM Engine Initialization Parameters\n",
                "\n",
                "#### 1. `tensor_parallel_size` (TP Degree)\n",
                "\n",
                "ì •ì˜: ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜(Weights)ì™€ ì—°ì‚°ì„ ëª‡ ê°œì˜ GPUì— ë¶„ì‚°í•˜ì—¬ ì²˜ë¦¬í• ì§€ ê²°ì •í•˜ëŠ” Tensor Parallelismì˜ ì°¨ìˆ˜(Degree)ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, GPUê°€ 4ì¥ ë“¤ì–´ìˆì„ ê²½ìš° tensor_parallel_size=4 ì…ë‹ˆë‹¤. í˜„ì¬ ë…¸íŠ¸ë¶ì—ì„œëŠ” gpuê°€ 1ê°œë°–ì— ì—†ê¸° ë•Œë¬¸ì— tensor_parallel_size=1 ê³ ì •ì…ë‹ˆë‹¤.\n",
                "\n",
                "* Megatron-LM ìŠ¤íƒ€ì¼ì˜ Tensor Model Parallelismì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "* ê° Transformer ë ˆì´ì–´ì˜ í–‰ë ¬ ì—°ì‚°(Matrix Multiplication)ì„ ë¬¼ë¦¬ì  GPUë“¤ì— ìˆ˜í‰ì ìœ¼ë¡œ ìƒ¤ë”©(Sharding)í•©ë‹ˆë‹¤.\n",
                "* ì˜ˆ: ì¸ ê²½ìš°, í–‰ë ¬ ë¥¼ ë¡œ ìª¼ê°œì–´ ë‘ GPUê°€ ê°ê° ì—°ì‚°í•œ í›„, `All-Reduce` í†µì‹ ì„ í†µí•´ ê²°ê³¼ë¥¼ í•©ì¹©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* `value=1`: ë‹¨ì¼ GPU ëª¨ë“œì…ë‹ˆë‹¤. ëª¨ë¸ ì „ì²´ê°€ í•˜ë‚˜ì˜ GPU VRAMì— ì ì¬ ê°€ëŠ¥í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. í†µì‹  ì˜¤ë²„í—¤ë“œê°€ ì—†ì–´ Latency ì¸¡ë©´ì—ì„œ ìœ ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "* `value > 1`: ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì»¤ì„œ ë‹¨ì¼ GPUì— ì ì¬ê°€ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜(OOM), ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê³ ì í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "* Constraint: ë°˜ë“œì‹œ ë¨¸ì‹ ì— ì¥ì°©ëœ ë¬¼ë¦¬ì  GPU ê°œìˆ˜ ì´í•˜ì—¬ì•¼ í•˜ë©°, GPU ê°„ í†µì‹  ëŒ€ì—­í­(NVLink ë“±)ì´ ì„±ëŠ¥ì˜ ë³‘ëª©ì´ ë  ìˆ˜ ìˆìŒì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "\n",
                "#### 2. `gpu_memory_utilization` (VRAM Budgeting)\n",
                "\n",
                "ì •ì˜: vLLM í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ VRAM ë¹„ìœ¨(Fraction)ì„ ì„¤ì •í•©ë‹ˆë‹¤. (Default: 0.9) ë§Œì•½, 6GBì˜ VRAM GPUì—ì„œ 0.9ë¥¼ ì„¤ì •í•˜ë©´ 6 * 0.9 = 5.4GBì˜ gpuë¥¼ ë¯¸ë¦¬ í• ë‹¹í•©ë‹ˆë‹¤. ì‹¤ì œ ëª¨ë¸ì˜ í¬ê¸°ê°€ 3GBë”ë¼ë„ 5.4GBë¥¼ ì°¨ì§€í•˜ê²Œ ë˜ë¯€ë¡œ ë„ˆë¬´ ë†’ì€ ê°’ì„ ì„¤ì •í•˜ê²Œ ë˜ë©´ gpu ë‚­ë¹„ê°€ ë˜ê³ , ë°˜ë©´ì— ë„ˆë¬´ ì ê²Œ ì„¤ì •í•˜ë©´ cpu ì¶”ë¡ ìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ë•Œë¬¸ì— ë„ˆë¬´ ë‚®ê²Œ ì„¤ì •í•˜ë©´ ì•ˆë©ë‹ˆë‹¤.\n",
                "\n",
                "* ì—”ì§„ ì´ˆê¸°í™” ì‹œ Profiling Phaseê°€ ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
                "* `(ì „ì²´ VRAM * 0.9) - (ëª¨ë¸ ê°€ì¤‘ì¹˜ + Activation ì˜¤ë²„í—¤ë“œ)`ë¥¼ ê³„ì‚°í•˜ì—¬, ë‚¨ì€ ê³µê°„ì„ KV Cacheìš© ë¸”ë¡ìœ¼ë¡œ ë¯¸ë¦¬ í• ë‹¹(Pre-allocation)í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* Why not 1.0?: PyTorch ëŸ°íƒ€ì„ì´ë‚˜ CUDA Contextê°€ ì‚¬ìš©í•  ì—¬ìœ  ê³µê°„(Buffer)ì„ ë‚¨ê²¨ë‘¬ì•¼ í•©ë‹ˆë‹¤. 1.0ìœ¼ë¡œ ì„¤ì • ì‹œ OOM(Out of Memory)ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ê°€ ê°•ì œ ì¢…ë£Œë  ìœ„í—˜ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.\n",
                "* Performance Trade-off: ì´ ê°’ì„ ë†’ì´ë©´ KV Cache ê³µê°„ì´ ëŠ˜ì–´ë‚˜ ë°°ì¹˜ í¬ê¸°(Batch Size)ë¥¼ í‚¤ìš¸ ìˆ˜ ìˆì–´ ì²˜ë¦¬ëŸ‰(Throughput)ì´ í–¥ìƒë©ë‹ˆë‹¤. ë°˜ë©´, ë„ˆë¬´ ë†’ê²Œ ì¡ìœ¼ë©´ ëŸ°íƒ€ì„ ì¤‘ í”¼í¬ ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ ì¶©ëŒì´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, 0.9~0.95 ì‚¬ì´ì—ì„œ íŠœë‹í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "\n",
                "#### 3. `trust_remote_code` (Execution Policy)\n",
                "\n",
                "ì •ì˜: HuggingFace Hub ë“± ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì˜ ì»¤ìŠ¤í…€ Python ì½”ë“œ(`modeling_*.py`) ì‹¤í–‰ì„ í—ˆìš©í• ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤.\n",
                "\n",
                "* `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì•„ì§ ê³µì‹ í†µí•©(Upstream)ë˜ì§€ ì•Šì€ ìµœì‹  ì•„í‚¤í…ì²˜ë‚˜, ì»¤ìŠ¤í…€ ëª¨ë¸(ì˜ˆ: MPT, Falcon ì´ˆê¸° ë²„ì „ ë“±)ì„ ë¡œë“œí•  ë•Œ í•„ìš”í•©ë‹ˆë‹¤.\n",
                "* `True`ë¡œ ì„¤ì • ì‹œ, ëª¨ë¸ ê°€ì¤‘ì¹˜ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸ê¹Œì§€ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* Security Risk: ê²€ì¦ë˜ì§€ ì•Šì€ ëª¨ë¸ ì €ì¥ì†Œì—ì„œ ì´ ì˜µì…˜ì„ ì¼œë©´ ì•…ì„± ì½”ë“œê°€ ì‹¤í–‰ë  ìˆ˜ ìˆëŠ” RCE(Remote Code Execution) ì·¨ì•½ì ì— ë…¸ì¶œë©ë‹ˆë‹¤.\n",
                "* Best Practice: ê²€ì¦ëœ ê³µì‹ ë¦¬í¬ì§€í† ë¦¬(Official Org)ë‚˜ ì‚¬ë‚´ ëª¨ë¸ì´ ì•„ë‹Œ ê²½ìš°, ì½”ë“œë¥¼ ë¨¼ì € ê²€í† (Audit)í•œ í›„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì›ì¹™ì…ë‹ˆë‹¤. ì¼ë°˜ì ì¸ Llama 2, Mistral ê°™ì€ í‘œì¤€ ì•„í‚¤í…ì²˜ëŠ” `False`ë¡œ ì„¤ì •í•´ë„ vLLM ë‚´ë¶€ êµ¬í˜„ì²´ë¡œ ë¡œë”© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "#### 4. `enforce_eager` (CUDA Graph Strategy)\n",
                "\n",
                "**ì •ì˜:** vLLMì˜ í•µì‹¬ ìµœì í™” ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì¸ **CUDA Graphs** ì‚¬ìš© ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. `True`ë¡œ ì„¤ì •í•˜ë©´ CUDA Graphsë¥¼ ë„ê³ , PyTorchì˜ ê¸°ë³¸ ì‹¤í–‰ ë°©ì‹ì¸ **Eager Mode**ë¡œ ê°•ì œ ì „í™˜í•©ë‹ˆë‹¤. (Default: `False`)\n",
                "\n",
                "* **CUDA Graphsë€?:** GPU ì—°ì‚° ì»¤ë„(Kernel)ë“¤ì˜ ì‹¤í–‰ ìˆœì„œì™€ ë©”ëª¨ë¦¬ í• ë‹¹ì„ ë¯¸ë¦¬ 'ê·¸ë˜í”„' í˜•íƒœë¡œ ìº¡ì²˜(Capture)í•´ë‘ê³ , ì¶”ë¡  ì‹œì—ëŠ” ì´ ê·¸ë˜í”„ë¥¼ í†µì§¸ë¡œ ì‹¤í–‰(Replay)í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ CPUê°€ GPUì— ëª…ë ¹ì„ ë‚´ë¦¬ëŠ” ì˜¤ë²„í—¤ë“œ(Kernel Launch Overhead)ë¥¼ íšê¸°ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.\n",
                "* **Why use `True` (Disabling Graphs)?:**\n",
                "    * **Debugging:** CUDA ì—ëŸ¬ê°€ ë°œìƒí–ˆì„ ë•Œ ì •í™•í•œ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "    * **Memory Issue:** CUDA Graphë¥¼ ìº¡ì²˜í•˜ê³  ìœ ì§€í•˜ëŠ” ë°ì—ë„ ì¼ì •ëŸ‰ì˜ VRAMì´ ì†Œëª¨ë©ë‹ˆë‹¤. VRAMì´ ê·¹í•œìœ¼ë¡œ ë¶€ì¡±í•˜ì—¬ OOMì´ ë°œìƒí•  ë•Œ, ì„±ëŠ¥ ì €í•˜ë¥¼ ê°ìˆ˜í•˜ê³  ë©”ëª¨ë¦¬ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•´ ì¼¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "    * **Hardware Compatibility:** ì¼ë¶€ êµ¬í˜• GPUë‚˜ íŠ¹ì • ë“œë¼ì´ë²„ í™˜ê²½ì—ì„œ CUDA Graphsê°€ ë¶ˆì•ˆì •í•  ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "    * **Performance Impact:** `True`ë¡œ ì„¤ì •í•  ê²½ìš°, CPU ì˜¤ë²„í—¤ë“œê°€ ì¦ê°€í•˜ì—¬ íŠ¹íˆ ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆì—ì„œì˜ ì²˜ë¦¬ëŸ‰(Throughput)ê³¼ ì§€ì—° ì‹œê°„(Latency) ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” `False`(ê¸°ë³¸ê°’) ìœ ì§€ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sun Feb  1 00:59:54 2026       \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| NVIDIA-SMI 575.57.05              Driver Version: 576.57         CUDA Version: 12.9     |\n",
                        "|-----------------------------------------+------------------------+----------------------+\n",
                        "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
                        "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
                        "|                                         |                        |               MIG M. |\n",
                        "|=========================================+========================+======================|\n",
                        "|   0  NVIDIA GeForce RTX 4050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
                        "| N/A   49C    P8              2W /   78W |       0MiB /   6141MiB |      0%      Default |\n",
                        "|                                         |                        |                  N/A |\n",
                        "+-----------------------------------------+------------------------+----------------------+\n",
                        "                                                                                         \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| Processes:                                                                              |\n",
                        "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
                        "|        ID   ID                                                               Usage      |\n",
                        "|=========================================================================================|\n",
                        "|  No running processes found                                                             |\n",
                        "+-----------------------------------------------------------------------------------------+\n"
                    ]
                }
            ],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸš€ vLLM ëª¨ë¸ ë¡œë”© ì¤‘...\n",
                        "INFO 02-01 00:59:54 [utils.py:253] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.75, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'HuggingFaceTB/SmolLM2-360M-Instruct'}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-01 00:59:55 [model.py:514] Resolved architecture: LlamaForCausalLM\n",
                        "INFO 02-01 00:59:55 [model.py:1661] Using max model len 8192\n",
                        "INFO 02-01 00:59:57 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
                        "WARNING 02-01 00:59:57 [vllm.py:622] Enforce eager set, overriding optimization level to -O0\n",
                        "INFO 02-01 00:59:57 [vllm.py:722] Cudagraph is disabled under eager mode\n",
                        "WARNING 02-01 00:59:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
                        "WARNING 02-01 00:59:58 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:03 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='HuggingFaceTB/SmolLM2-360M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-360M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-360M-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:04 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.26.93.109:46351 backend=nccl\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:04 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m WARNING 02-01 01:00:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:04 [gpu_model_runner.py:3562] Starting to load model HuggingFaceTB/SmolLM2-360M-Instruct...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m /home/ssafy/skeleton-4/.venv/lib/python3.13/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m We recommend installing via `pip install torch-c-dlpack-ext`\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m   warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:06 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:08 [weight_utils.py:527] No model.safetensors.index.json found in remote.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.69it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.69it/s]\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:08 [default_loader.py:308] Loading weights took 0.23 seconds\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:08 [gpu_model_runner.py:3659] Model loading took 0.6750 GiB memory and 3.259518 seconds\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:10 [gpu_worker.py:375] Available KV cache memory: 3.33 GiB\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:10 [kv_cache_utils.py:1291] GPU KV cache size: 87,168 tokens\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:10 [kv_cache_utils.py:1296] Maximum concurrency for 8,192 tokens per request: 10.64x\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:10 [core.py:259] init engine (profile, create kv cache, warmup model) took 1.83 seconds\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m WARNING 02-01 01:00:11 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=4930)\u001b[0;0m INFO 02-01 01:00:11 [vllm.py:722] Cudagraph is disabled under eager mode\n",
                        "INFO 02-01 01:00:12 [llm.py:360] Supported tasks: ['generate']\n",
                        "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (17.68ì´ˆ)\n",
                        "ğŸ“ Tokenizer ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "# Step 1: vLLM ëª¨ë¸ ì´ˆê¸°í™”\n",
                "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
                "\n",
                "print(\"ğŸš€ vLLM ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "llm = LLM(\n",
                "    model=model_name,\n",
                "    tensor_parallel_size=1,       # ë‹¨ì¼ GPU ì‚¬ìš©\n",
                "    gpu_memory_utilization=0.75,   # GPU ë©”ëª¨ë¦¬ 75% ì‚¬ìš© (ì›ë˜ ëª¨ë¸ì˜ í¬ê¸°ëŠ” 1.5GB ì •ë„)\n",
                "    trust_remote_code=True,       # ì»¤ìŠ¤í…€ ëª¨ë¸ ì½”ë“œ ì‹ ë¢°\n",
                "    enforce_eager=True            # CUDA Graph ìº¡ì³ ë¹„í™œì„±í™”\n",
                ")\n",
                "\n",
                "load_time = time.time() - start_time\n",
                "print(f\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ({load_time:.2f}ì´ˆ)\")\n",
                "\n",
                "# Tokenizer ë¡œë“œ\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "print(f\"ğŸ“ Tokenizer ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sun Feb  1 01:00:12 2026       \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| NVIDIA-SMI 575.57.05              Driver Version: 576.57         CUDA Version: 12.9     |\n",
                        "|-----------------------------------------+------------------------+----------------------+\n",
                        "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
                        "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
                        "|                                         |                        |               MIG M. |\n",
                        "|=========================================+========================+======================|\n",
                        "|   0  NVIDIA GeForce RTX 4050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
                        "| N/A   52C    P4             22W /   78W |    4805MiB /   6141MiB |      0%      Default |\n",
                        "|                                         |                        |                  N/A |\n",
                        "+-----------------------------------------+------------------------+----------------------+\n",
                        "                                                                                         \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| Processes:                                                                              |\n",
                        "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
                        "|        ID   ID                                                               Usage      |\n",
                        "|=========================================================================================|\n",
                        "|    0   N/A  N/A            4930      C   /python3.13                           N/A      |\n",
                        "+-----------------------------------------------------------------------------------------+\n"
                    ]
                }
            ],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 ê°„ë‹¨í•œ ì¶”ë¡  ì‹¤ìŠµ\n",
                "\n",
                "ì•„ë˜ ì½”ë“œëŠ” LLMì˜ Decoding Strategy(ë””ì½”ë”© ì „ëµ)ë¥¼ ì •ì˜í•˜ëŠ” `SamplingParams` ê°ì²´ ìƒì„±ë¶€ì…ë‹ˆë‹¤. ëª¨ë¸ì´ í™•ë¥  ë¶„í¬(Logits)ë¡œë¶€í„° ë‹¤ìŒ í† í°ì„ ì„ íƒí•˜ëŠ” ê³¼ì •ì„ ì œì–´í•˜ì—¬, ìƒì„± ê²°ê³¼ì˜ í’ˆì§ˆ, ë‹¤ì–‘ì„±, ê·¸ë¦¬ê³  ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ê²°ì •ì§“ëŠ” í•µì‹¬ íŒŒë¼ë¯¸í„°ë“¤ì…ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "### Decoding Strategy Configuration\n",
                "\n",
                "#### 1. `max_tokens` (Termination Condition)\n",
                "\n",
                "ì •ì˜: ëª¨ë¸ì´ ìƒì„±í•  ìˆ˜ ìˆëŠ” ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´(Maximum Output Length)ë¥¼ ì œí•œí•˜ëŠ” Hard Limitì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, output í† í° ìˆ˜ê°€ 128ì— ë„ë‹¬í•˜ë©´ 128ì—ì„œ ì¤‘ê°„ì— ìƒì„±ì„ ë©ˆì¶¥ë‹ˆë‹¤.\n",
                "\n",
                "* Latency Budgeting: ìƒì„± í† í° ìˆ˜ëŠ” ì¶”ë¡  ì‹œê°„(Inference Latency)ê³¼ ì •ë¹„ë¡€í•©ë‹ˆë‹¤. ì„œë¹„ìŠ¤ì˜ ì‘ë‹µ ì†ë„ ëª©í‘œ(SLA)ì— ë§ì¶° ì ì ˆí•œ ìƒí•œì„ ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
                "* Resource Management: ìƒì„± ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ KV Cache ì ìœ ëŸ‰ì´ ì¦ê°€í•©ë‹ˆë‹¤. ë¬´í•œ ìƒì„±ì„ ë°©ì§€í•˜ì—¬ OOM(Out of Memory) ë° ì—°ì‚° ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ë¥¼ ë§‰ëŠ” ì•ˆì „ì¥ì¹˜ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
                "* Note: ì´ ê°’ì— ë„ë‹¬í•˜ë©´ ë¬¸ì¥ì´ ì™„ê²°ë˜ì§€ ì•Šì•˜ë”ë¼ë„ ê°•ì œë¡œ ìƒì„±ì„ ì¤‘ë‹¨(Stop)í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "\n",
                "#### 2. `temperature` (Entropy Scaling)\n",
                "\n",
                "ì •ì˜: Softmax í•¨ìˆ˜ë¥¼ í†µê³¼í•˜ê¸° ì „, Logit ê°’ì„ ìŠ¤ì¼€ì¼ë§í•˜ì—¬ í™•ë¥  ë¶„í¬ì˜ í‰íƒ„ë„(Flatness)ë¥¼ ì¡°ì ˆí•˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.\n",
                "\n",
                "* Low (): í™•ë¥  ë¶„í¬ë¥¼ ë‚ ì¹´ë¡­ê²Œ(Peaky) ë§Œë“­ë‹ˆë‹¤. í™•ë¥ ì´ ë†’ì€ í† í°ì€ ë” ë†’ì•„ì§€ê³ , ë‚®ì€ í† í°ì€ ë” ë‚®ì•„ì ¸ ê²°ì •ë¡ ì (Deterministic)ì´ê³  ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ ë„í•©ë‹ˆë‹¤. (Code generation ë“±ì— ì í•©)\n",
                "* High (): í™•ë¥  ë¶„í¬ë¥¼ í‰í‰í•˜ê²Œ(Flatten) ë§Œë“­ë‹ˆë‹¤. í™•ë¥ ì´ ë‚®ì€ í† í°ì˜ ì„ íƒ ê°€ëŠ¥ì„±ì„ ë†’ì—¬ ë‹¤ì–‘ì„±(Diversity)ê³¼ ì°½ì˜ì„±ì„ ë¶€ì—¬í•©ë‹ˆë‹¤. (Creative writing ë“±ì— ì í•©)\n",
                "\n",
                "\n",
                "\n",
                "#### 3. `top_p` (Nucleus Sampling)\n",
                "\n",
                "ì •ì˜: í™•ë¥ ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ í† í°ì„ ì •ë ¬í•œ ë’¤, ëˆ„ì  í™•ë¥ (Cumulative Probability)ì´ ê°’(ì—¬ê¸°ì„œëŠ” 0.9, ì¦‰ 90%)ì— ë„ë‹¬í•  ë•Œê¹Œì§€ë§Œ í›„ë³´êµ°(Candidate Set)ì— í¬í•¨ì‹œí‚¤ëŠ” ë™ì  ì ˆë‹¨(Dynamic Truncation) ë°©ì‹ì…ë‹ˆë‹¤. tokenizerì— ìˆëŠ” vocabì—ì„œ ìƒì„±í•  í›„ë³´ë“¤ì„ ê³ ë ¤í•˜ê²Œ ë˜ëŠ”ë° 90%ë¡œ ì„¤ì •í•˜ë©´ ë‚˜ë¨¸ì§€ 10%ì˜ í™•ë¥ ì„ ì°¨ì§€í•˜ëŠ” ìˆ˜ë§ì€ í† í°ë“¤ì€ ê³ ë ¤í•˜ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, vocabì´ 128,000ì´ê³ , \"ì•ˆë…•\" ì´ë¼ëŠ” í† í°ì˜ í™œë¥ ì´ 90% ë¼ê³  í•˜ë©´ ë‚˜ë¨¸ì§€ 127,999ì˜ í† í°ì€ ê³ ë ¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
                "\n",
                "* ëª¨ë¸ì´ í™•ì‹ ì„ ê°€ì§€ëŠ” ìƒí™©(íŠ¹ì • ë‹¨ì–´ì˜ í™•ë¥ ì´ ë§¤ìš° ë†’ìŒ)ì—ì„œëŠ” í›„ë³´êµ°ì„ ì¢ê²Œ ê°€ì ¸ê°€ê³ ,\n",
                "* í™•ì‹ ì´ ì—†ëŠ” ìƒí™©(ì—¬ëŸ¬ ë‹¨ì–´ì˜ í™•ë¥ ì´ ë¹„ìŠ·í•¨)ì—ì„œëŠ” í›„ë³´êµ°ì„ ë„“ê²Œ ê°€ì ¸ê°‘ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* Engineering Insight: `top_k`ì˜ ê²½ì§ì„±ì„ ë³´ì™„í•˜ì—¬, ë¬¸ë§¥ì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ íƒìƒ‰ ê³µê°„(Search Space)ì„ ì¡°ì ˆí•¨ìœ¼ë¡œì¨ í…ìŠ¤íŠ¸ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ í’ë¶€í•œ í‘œí˜„ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
                "\n",
                "#### 4. `top_k` (Top-K Sampling)\n",
                "\n",
                "ì •ì˜: ë‹¤ìŒ í† í° í›„ë³´ë¥¼ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ìƒìœ„ ê°œ(ì—¬ê¸°ì„œëŠ” 50ê°œ)ë¡œ ê³ ì •í•˜ì—¬ ì œí•œí•˜ëŠ” ì •ì  ì ˆë‹¨(Static Truncation) ë°©ì‹ì…ë‹ˆë‹¤. top_pì™€ ìœ ì‚¬í•œ ê¸°ëŠ¥ì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤.\n",
                "\n",
                "* Mechanism: 50 ë²ˆì§¸ ì´í›„ì˜ í† í°ë“¤ì€ í™•ë¥ ì´ ì•„ë¬´ë¦¬ ë†’ì•„ë„(í˜¹ì€ ì¡´ì¬í•˜ë”ë¼ë„) í›„ë³´êµ°ì—ì„œ ë°°ì œ(Masking)ë©ë‹ˆë‹¤.\n",
                "* í™•ë¥  ë¶„í¬ì˜ Long Tail(í™•ë¥ ì´ ë§¤ìš° ë‚®ì€ ê¼¬ë¦¬ ë¶€ë¶„)ì„ ë¬¼ë¦¬ì ìœ¼ë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤.\n",
                "* ë¬¸ë§¥ê³¼ ì „í˜€ ìƒê´€ì—†ëŠ” ì—‰ëš±í•œ í† í°(Outlier)ì´ ì„ íƒë  ê°€ëŠ¥ì„±ì„ ì›ì²œ ì°¨ë‹¨í•˜ì—¬ ìƒì„± ê²°ê³¼ì˜ ì•ˆì •ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âš™ï¸  Sampling íŒŒë¼ë¯¸í„°:\n",
                        "  - Max Tokens: 128\n",
                        "  - Temperature: 0.7\n",
                        "  - Top-p: 0.9\n"
                    ]
                }
            ],
            "source": [
                "# Step 2: Sampling íŒŒë¼ë¯¸í„° ì„¤ì •\n",
                "sampling_params = SamplingParams(\n",
                "    max_tokens=128,        # ìµœëŒ€ ìƒì„± í† í° ìˆ˜\n",
                "    temperature=0.7,       # ìƒ˜í”Œë§ ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€)\n",
                "    top_p=0.9,            # Nucleus sampling\n",
                "    top_k=50,             # Top-k sampling\n",
                ")\n",
                "\n",
                "print(\"âš™ï¸  Sampling íŒŒë¼ë¯¸í„°:\")\n",
                "print(f\"  - Max Tokens: {sampling_params.max_tokens}\")\n",
                "print(f\"  - Temperature: {sampling_params.temperature}\")\n",
                "print(f\"  - Top-p: {sampling_params.top_p}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ¤– ì¶”ë¡  ì‹œì‘...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cc9ed64e24d24708bd62cf32038ed74a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "27787ec94e1248b195ba61f94dc01439",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… ì¶”ë¡  ì™„ë£Œ! (2.91ì´ˆ)\n",
                        "ğŸ“Š ì²˜ë¦¬ëŸ‰: 1.03 prompts/sec\n",
                        "\n",
                        "[í”„ë¡¬í”„íŠ¸ 1] Explain quantum computing in simple terms:\n",
                        "[ìƒì„± ê²°ê³¼] \n",
                        "\n",
                        "Quantum computing is a type of computing that uses the principles of quantum mechanics to process information. Unlike classical computing, which uses bits to store information, quantum computing uses quantum bits, or qubits. Qubits can exist in multiple states simultaneously, which allows quantum computers to process complex problems much faster than classical computers.\n",
                        "\n",
                        "For example, a classical computer can solve a problem in a few minutes, but a quantum computer can solve the same problem in a few seconds. This is because the quantum computer uses quantum entanglement to combine the qubits, allowing the computer to process the problem in a way that classical computers cannot.\n",
                        "\n",
                        "Qub\n",
                        "[í† í° ìˆ˜] 128\n",
                        "------------------------------------------------------------\n",
                        "[í”„ë¡¬í”„íŠ¸ 2] What is the capital of France?\n",
                        "[ìƒì„± ê²°ê³¼] \n",
                        "The capital of France is Paris.\n",
                        "#### 3\n",
                        "Is the following statement true?\n",
                        "\"The capital of France is Berlin.\"\n",
                        "Yes, the capital of France is Paris.\n",
                        "#### 4\n",
                        "Is the following statement true?\n",
                        "\"The capital of France is Moscow.\"\n",
                        "No, the capital of France is Paris.\n",
                        "#### 5\n",
                        "What is the name of the largest city in France?\n",
                        "The largest city in France is Paris.\n",
                        "#### 6\n",
                        "What is the capital of Spain?\n",
                        "The capital of Spain is Madrid.\n",
                        "#### 7\n",
                        "What is the capital of Switzerland?\n",
                        "The capital\n",
                        "[í† í° ìˆ˜] 128\n",
                        "------------------------------------------------------------\n",
                        "[í”„ë¡¬í”„íŠ¸ 3] Write a haiku about programming:\n",
                        "[ìƒì„± ê²°ê³¼] \n",
                        "\n",
                        "\"Programming's a dance\n",
                        "With code and data,\n",
                        "Performing magic.\"\n",
                        "[í† í° ìˆ˜] 19\n",
                        "------------------------------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "# Step 3: ì¶”ë¡  ì‹¤í–‰\n",
                "prompts = [\n",
                "    \"Explain quantum computing in simple terms:\",\n",
                "    \"What is the capital of France?\",\n",
                "    \"Write a haiku about programming:\",\n",
                "]\n",
                "\n",
                "print(\"ğŸ¤– ì¶”ë¡  ì‹œì‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "outputs = llm.generate(prompts, sampling_params)\n",
                "\n",
                "inference_time = time.time() - start_time\n",
                "\n",
                "print(f\"\\nâœ… ì¶”ë¡  ì™„ë£Œ! ({inference_time:.2f}ì´ˆ)\")\n",
                "print(f\"ğŸ“Š ì²˜ë¦¬ëŸ‰: {len(prompts) / inference_time:.2f} prompts/sec\\n\")\n",
                "\n",
                "# ê²°ê³¼ ì¶œë ¥\n",
                "for i, output in enumerate(outputs):\n",
                "    generated_text = output.outputs[0].text\n",
                "    token_count = len(output.outputs[0].token_ids)\n",
                "    \n",
                "    print(f\"[í”„ë¡¬í”„íŠ¸ {i+1}] {prompts[i]}\")\n",
                "    print(f\"[ìƒì„± ê²°ê³¼] {generated_text}\")\n",
                "    print(f\"[í† í° ìˆ˜] {token_count}\")\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2.5: KV Cache ì„±ëŠ¥ ë¹„êµ ì‹œë®¬ë ˆì´í„°\n",
                "\n",
                "ì´ì œ **ì™œ KV Cacheê°€ í•„ìš”í•œì§€** ì—°ì‚° íšŸìˆ˜ë¥¼ ì¹´ìš´íŒ…í•˜ì—¬ ì§ì ‘ í™•ì¸í•´ë´…ì‹œë‹¤\n",
                "\n",
                "ì´ ì‹œë®¬ë ˆì´í„°ëŠ” ì‹¤ì œ í–‰ë ¬ ì—°ì‚° ëŒ€ì‹ , 'ì—°ì‚°ì„ ëª‡ ë²ˆ ìˆ˜í–‰í–ˆëŠ”ê°€'ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… ModelSimulator ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "class ModelSimulator:\n",
                "    \"\"\"LLM ì¶”ë¡  ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
                "    def __init__(self):\n",
                "        self.op_count = 0  # ì—°ì‚° íšŸìˆ˜ ì¹´ìš´í„°\n",
                "\n",
                "    def compute_token_representation(self, token):\n",
                "        \"\"\"\n",
                "        í† í°ì„ ë²¡í„°ë¡œ ë³€í™˜ (Embedding + Q, K, V ìƒì„±)\n",
                "        ê³ ë¹„ìš© ì—°ì‚°ì´ë¼ê³  ê°€ì •\n",
                "        \"\"\"\n",
                "        self.op_count += 1\n",
                "        return f\"Vector({token})\"\n",
                "\n",
                "    def attention_calculation(self, current_token, past_vectors):\n",
                "        \"\"\"\n",
                "        í˜„ì¬ í† í°ê³¼ ê³¼ê±° í† í°ë“¤ ê°„ì˜ ê´€ê³„ ê³„ì‚° (Attention)\n",
                "        \"\"\"\n",
                "        self.op_count += len(past_vectors)\n",
                "\n",
                "print(\"âœ… ModelSimulator ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸš« [Scenario 1] KV Cache ë¯¸ì‚¬ìš© (ë§¤ë²ˆ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ê³„ì‚°)\n",
                        "  Step 1: ë¬¸ì¥ ê¸¸ì´ 4 -> ëˆ„ì  ì—°ì‚°: 8\n",
                        "  Step 2: ë¬¸ì¥ ê¸¸ì´ 5 -> ëˆ„ì  ì—°ì‚°: 18\n",
                        "  Step 3: ë¬¸ì¥ ê¸¸ì´ 6 -> ëˆ„ì  ì—°ì‚°: 30\n",
                        "  Step 4: ë¬¸ì¥ ê¸¸ì´ 7 -> ëˆ„ì  ì—°ì‚°: 44\n",
                        "  Step 5: ë¬¸ì¥ ê¸¸ì´ 8 -> ëˆ„ì  ì—°ì‚°: 60\n",
                        "\n",
                        "ì´ ì—°ì‚° íšŸìˆ˜: 60\n"
                    ]
                }
            ],
            "source": [
                "# ==========================================\n",
                "# ì‹œë‚˜ë¦¬ì˜¤ 1: KV Cache ì—†ì´ ìƒì„± (No Cache)\n",
                "# ==========================================\n",
                "print(\"ğŸš« [Scenario 1] KV Cache ë¯¸ì‚¬ìš© (ë§¤ë²ˆ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ê³„ì‚°)\")\n",
                "model_no_cache = ModelSimulator()\n",
                "generated_tokens = []\n",
                "input_prompt = [\"Deep\", \"Learning\", \"is\", \"Fun\"]\n",
                "\n",
                "for step in range(5):\n",
                "    current_context = input_prompt + generated_tokens\n",
                "    \n",
                "    # ë§¤ë²ˆ ëª¨ë“  í† í°ì„ ë‹¤ì‹œ ê³„ì‚°\n",
                "    vectors = []\n",
                "    for token in current_context:\n",
                "        vec = model_no_cache.compute_token_representation(token)\n",
                "        vectors.append(vec)\n",
                "    \n",
                "    model_no_cache.attention_calculation(\"New_Token\", vectors)\n",
                "    \n",
                "    generated_tokens.append(f\"Gen_{step}\")\n",
                "    print(f\"  Step {step+1}: ë¬¸ì¥ ê¸¸ì´ {len(current_context)} -> ëˆ„ì  ì—°ì‚°: {model_no_cache.op_count}\")\n",
                "\n",
                "final_cost_no_cache = model_no_cache.op_count\n",
                "print(f\"\\nì´ ì—°ì‚° íšŸìˆ˜: {final_cost_no_cache}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… [Scenario 2] KV Cache ì‚¬ìš© (ê³¼ê±°ëŠ” ì €ì¥í•´ë‘ê³  ì¬ì‚¬ìš©)\n",
                        "  [Prefill] ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì¤‘...\n",
                        "  Step 1: ìºì‹œ í¬ê¸° 5 -> ëˆ„ì  ì—°ì‚°: 9\n",
                        "  Step 2: ìºì‹œ í¬ê¸° 6 -> ëˆ„ì  ì—°ì‚°: 15\n",
                        "  Step 3: ìºì‹œ í¬ê¸° 7 -> ëˆ„ì  ì—°ì‚°: 22\n",
                        "  Step 4: ìºì‹œ í¬ê¸° 8 -> ëˆ„ì  ì—°ì‚°: 30\n",
                        "  Step 5: ìºì‹œ í¬ê¸° 9 -> ëˆ„ì  ì—°ì‚°: 39\n",
                        "\n",
                        "ì´ ì—°ì‚° íšŸìˆ˜: 39\n"
                    ]
                }
            ],
            "source": [
                "# ==========================================\n",
                "# ì‹œë‚˜ë¦¬ì˜¤ 2: KV Cache ì‚¬ìš© (With Cache)\n",
                "# ==========================================\n",
                "print(\"\\nâœ… [Scenario 2] KV Cache ì‚¬ìš© (ê³¼ê±°ëŠ” ì €ì¥í•´ë‘ê³  ì¬ì‚¬ìš©)\")\n",
                "model_with_cache = ModelSimulator()\n",
                "generated_tokens = []\n",
                "kv_cache = []  # ì´ê²ƒì´ ë°”ë¡œ KV Cache!\n",
                "\n",
                "# Prefill: ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ (í•œ ë²ˆë§Œ)\n",
                "print(\"  [Prefill] ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì¤‘...\")\n",
                "for token in input_prompt:\n",
                "    vec = model_with_cache.compute_token_representation(token)\n",
                "    kv_cache.append(vec)\n",
                "\n",
                "# Decode: í† í° ìƒì„±\n",
                "for step in range(5):\n",
                "    # ìƒˆë¡œìš´ í† í° í•˜ë‚˜ë§Œ ì²˜ë¦¬\n",
                "    current_vec = model_with_cache.compute_token_representation(\"New_Token\")\n",
                "    \n",
                "    # ê³¼ê±° cache + í˜„ì¬ ë²¡í„°ë¡œ attention\n",
                "    model_with_cache.attention_calculation(\"New_Token\", kv_cache)\n",
                "    \n",
                "    # ìºì‹œì— ì €ì¥\n",
                "    kv_cache.append(current_vec)\n",
                "    generated_tokens.append(f\"Gen_{step}\")\n",
                "    \n",
                "    print(f\"  Step {step+1}: ìºì‹œ í¬ê¸° {len(kv_cache)} -> ëˆ„ì  ì—°ì‚°: {model_with_cache.op_count}\")\n",
                "\n",
                "final_cost_with_cache = model_with_cache.op_count\n",
                "print(f\"\\nì´ ì—°ì‚° íšŸìˆ˜: {final_cost_with_cache}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "ğŸ“Š ìµœì¢… ì—°ì‚° ë¹„ìš© ë¹„êµ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„)\n",
                        "==================================================\n",
                        "1. No Cache Cost  : 60 (O(NÂ²) - ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€)\n",
                        "2. With Cache Cost: 39 (O(N) - ì„ í˜•ì  ì¦ê°€)\n",
                        "ğŸš€ íš¨ìœ¨ í–¥ìƒ: ì•½ 1.5ë°° ë” ì ì€ ì—°ì‚°!\n",
                        "==================================================\n"
                    ]
                }
            ],
            "source": [
                "# ìµœì¢… ë¹„êµ\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"ğŸ“Š ìµœì¢… ì—°ì‚° ë¹„ìš© ë¹„êµ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„)\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"1. No Cache Cost  : {final_cost_no_cache} (O(NÂ²) - ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€)\")\n",
                "print(f\"2. With Cache Cost: {final_cost_with_cache} (O(N) - ì„ í˜•ì  ì¦ê°€)\")\n",
                "print(f\"ğŸš€ íš¨ìœ¨ í–¥ìƒ: ì•½ {final_cost_no_cache / final_cost_with_cache:.1f}ë°° ë” ì ì€ ì—°ì‚°!\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Performance Benchmark & Analysis (ê²°ê³¼ ë¶„ì„)\n",
                "\n",
                "ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ëŠ” **KV Cache ë©”ì»¤ë‹ˆì¦˜ì˜ ìœ ë¬´**ì™€ **vLLMì˜ ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ**ì´ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ê²°ì •ì ì¸(Critical) ì°¨ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
                "\n",
                "#### 1. Naive Decoding (No Cache)\n",
                "\n",
                "* **Computational Complexity**:  (Quadratic)\n",
                "* **Analysis**: ë§¤ í† í° ìƒì„± ì‹œì ë§ˆë‹¤ ì´ì „ì˜ ëª¨ë“  í† í°ì— ëŒ€í•´ Attention ì—°ì‚°ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìˆ˜í–‰(Re-computation)í•©ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ê¸¸ì´(Context Length)ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì—°ì‚° ë¹„ìš©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ì—¬, ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ê°€ ë¶ˆê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ Latencyê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
                "\n",
                "#### 2. Standard KV Caching\n",
                "\n",
                "* **Computational Complexity**:  (Linear)\n",
                "* **Analysis**: ì „í˜•ì ì¸ **Space-Time Trade-off** ì „ëµì…ë‹ˆë‹¤. ê³¼ê±° í† í°ì˜ Key/Value Stateë¥¼ VRAMì— ìƒì£¼ì‹œì¼œ ì¤‘ë³µ ì—°ì‚°ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤. ì†ë„ëŠ” íšê¸°ì ìœ¼ë¡œ ê°œì„ ë˜ì§€ë§Œ, ê·¸ ëŒ€ê°€ë¡œ VRAM ì ìœ ìœ¨(Footprint)ì´ ê¸‰ì¦í•˜ì—¬ ë°°ì¹˜ í¬ê¸°(Batch Size) í™•ì¥ì— ë¬¼ë¦¬ì  ì œì•½ì´ ìƒê¹ë‹ˆë‹¤.\n",
                "\n",
                "#### 3. vLLM (PagedAttention)\n",
                "\n",
                "* **Mechanism**: **Zero-Waste Memory Management**\n",
                "* **Analysis**: KV Cacheì˜ ì—°ì‚° íš¨ìœ¨ì„±(O(N))ì€ ìœ ì§€í•˜ë˜, OS í˜ì´ì§• ê¸°ë²•ì„ í†µí•´ **ë©”ëª¨ë¦¬ ë‹¨í¸í™”(Fragmentation)** ë¬¸ì œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.\n",
                "* **Impact**: ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ì˜ ìœ íœ´ ê³µê°„ì„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¹ˆí‹ˆì—†ì´ í™œìš©(Packing)í•¨ìœ¼ë¡œì¨, ë™ì¼í•œ í•˜ë“œì›¨ì–´ ë¦¬ì†ŒìŠ¤ ë‚´ì—ì„œ ìˆ˜ìš© ê°€ëŠ¥í•œ **KV Cacheì˜ ë°€ë„(Density)**ë¥¼ ë†’ì…ë‹ˆë‹¤. ì´ëŠ” ê²°ê³¼ì ìœ¼ë¡œ ë™ì‹œ ì²˜ë¦¬ëŸ‰(Throughput)ì˜ ê·¹ëŒ€í™”ë¡œ ì§ê²°ë©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3 Chat í˜•ì‹ í”„ë¡¬í”„íŠ¸\n",
                "\n",
                "ì‹¤ì „ì—ì„œëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ìœ ì € ë©”ì‹œì§€, ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ ì¡°í•©í•œ chat í˜•ì‹ì„ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Chat í¬ë§·íŒ… í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "def format_chat_messages(messages):\n",
                "    \"\"\"\n",
                "    Chat ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜\n",
                "    \n",
                "    Args:\n",
                "        messages: [{'role': 'system/user/assistant', 'content': '...'}, ...]\n",
                "    \"\"\"\n",
                "    if tokenizer.chat_template:\n",
                "        # Tokenizerì— chat templateì´ ìˆìœ¼ë©´ ì‚¬ìš©\n",
                "        return tokenizer.apply_chat_template(\n",
                "            messages,\n",
                "            tokenize=False,\n",
                "            add_generation_prompt=True\n",
                "        )\n",
                "    else:\n",
                "        # ìˆ˜ë™ í¬ë§·íŒ…\n",
                "        formatted = \"\"\n",
                "        for msg in messages:\n",
                "            role = msg['role'].capitalize()\n",
                "            content = msg['content']\n",
                "            formatted += f\"{role}: {content}\\n\\n\"\n",
                "        \n",
                "        if messages[-1]['role'] != 'assistant':\n",
                "            formatted += \"Assistant: \"\n",
                "        \n",
                "        return formatted\n",
                "\n",
                "print(\"âœ… Chat í¬ë§·íŒ… í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“ ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸:\n",
                        "<|im_start|>system\n",
                        "You are a SQL expert. Convert natural language queries to SQL.<|im_end|>\n",
                        "<|im_start|>user\n",
                        "Find all users with age greater than 30<|im_end|>\n",
                        "<|im_start|>assistant\n",
                        "\n",
                        "------------------------------------------------------------\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0ea3548c0733492a9b978e815798b31b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7d52a4e76aca40e39f4ba88dc352ae57",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[rank0]:[W201 01:00:16.976771580 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ğŸ’¡ ìƒì„±ëœ SQL:\n",
                        "SELECT * FROM users WHERE age > 30;\n"
                    ]
                }
            ],
            "source": [
                "# Text-to-SQL ì˜ˆì œ\n",
                "messages = [\n",
                "    {\n",
                "        'role': 'system',\n",
                "        'content': 'You are a SQL expert. Convert natural language queries to SQL.'\n",
                "    },\n",
                "    {\n",
                "        'role': 'user',\n",
                "        'content': 'Find all users with age greater than 30'\n",
                "    }\n",
                "]\n",
                "\n",
                "# í”„ë¡¬í”„íŠ¸ ë³€í™˜\n",
                "prompt = format_chat_messages(messages)\n",
                "print(\"ğŸ“ ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸:\")\n",
                "print(prompt)\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# ì¶”ë¡ \n",
                "sampling_params_sql = SamplingParams(\n",
                "    max_tokens=64,\n",
                "    temperature=0.1,  # SQL ìƒì„±ì€ ë‚®ì€ temperature ì‚¬ìš©\n",
                ")\n",
                "\n",
                "outputs = llm.generate([prompt], sampling_params_sql)\n",
                "\n",
                "del llm\n",
                "print(\"\\nğŸ’¡ ìƒì„±ëœ SQL:\")\n",
                "print(outputs[0].outputs[0].text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4 LoRA Adapter ì‚¬ìš©í•˜ê¸°\n",
                "\n",
                "### 4.1 LoRAë€?\n",
                "\n",
                "**LoRA (Low-Rank Adaptation)**:\n",
                "- ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ fine-tuningí•˜ëŠ” ëŒ€ì‹ , ì‘ì€ adapterë§Œ í•™ìŠµ\n",
                "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥\n",
                "- ì—¬ëŸ¬ taskë³„ adapterë¥¼ ì‰½ê²Œ êµì²´ ê°€ëŠ¥\n",
                "\n",
                "### 4.2 vLLMì—ì„œ LoRA ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•\n",
                "\n",
                "#### ë°©ë²• 1: Runtime LoRA (ë™ì  ì ìš©)\n",
                "- ì¶”ë¡  ì‹œì ì— adapterë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ\n",
                "- ì—¬ëŸ¬ adapterë¥¼ ë¹ ë¥´ê²Œ êµì²´ ê°€ëŠ¥\n",
                "\n",
                "skeleton-2ì—ì„œ í•™ìŠµí•œ LoRA ëª¨ë“ˆì„ ì´ìš©í•˜ì‹œê±°ë‚˜ skeleton-3ì—ì„œ í•™ìŠµí•œ LoRA ëª¨ë“ˆì„ ì´ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-01 01:00:16 [utils.py:253] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.75, 'disable_log_stats': True, 'enforce_eager': True, 'enable_lora': True, 'max_lora_rank': 64, 'model': 'HuggingFaceTB/SmolLM2-360M-Instruct'}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-01 01:00:17 [model.py:514] Resolved architecture: LlamaForCausalLM\n",
                        "INFO 02-01 01:00:17 [model.py:1661] Using max model len 8192\n",
                        "INFO 02-01 01:00:17 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
                        "WARNING 02-01 01:00:17 [vllm.py:622] Enforce eager set, overriding optimization level to -O0\n",
                        "INFO 02-01 01:00:17 [vllm.py:722] Cudagraph is disabled under eager mode\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:21 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='HuggingFaceTB/SmolLM2-360M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-360M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-360M-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:22 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.26.93.109:47255 backend=nccl\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:22 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m WARNING 02-01 01:00:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:23 [gpu_model_runner.py:3562] Starting to load model HuggingFaceTB/SmolLM2-360M-Instruct...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m /home/ssafy/skeleton-4/.venv/lib/python3.13/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m We recommend installing via `pip install torch-c-dlpack-ext`\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m   warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:25 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:26 [weight_utils.py:527] No model.safetensors.index.json found in remote.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.98it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.97it/s]\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:26 [default_loader.py:308] Loading weights took 0.27 seconds\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:26 [punica_selector.py:20] Using PunicaWrapperGPU.\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:27 [gpu_model_runner.py:3659] Model loading took 0.7521 GiB memory and 3.339144 seconds\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:28 [gpu_worker.py:375] Available KV cache memory: 3.25 GiB\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:29 [kv_cache_utils.py:1291] GPU KV cache size: 85,168 tokens\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:29 [kv_cache_utils.py:1296] Maximum concurrency for 8,192 tokens per request: 10.40x\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:29 [core.py:259] init engine (profile, create kv cache, warmup model) took 1.68 seconds\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m WARNING 02-01 01:00:30 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m INFO 02-01 01:00:30 [vllm.py:722] Cudagraph is disabled under eager mode\n",
                        "INFO 02-01 01:00:30 [llm.py:360] Supported tasks: ['generate']\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "44850b2ea6444e06babdd70a14934291",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING 02-01 01:00:30 [input_processor.py:250] vLLM has deprecated support for supporting different tokenizers for different LoRAs. By default, vLLM uses base model's tokenizer. If you are using a LoRA with its own tokenizer, consider specifying `--tokenizer [lora_path]` to use the LoRA tokenizer.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e0c06e8a6beb4fbdae198a4f23489f04",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;36m(EngineCore_DP0 pid=5103)\u001b[0;0m WARNING 02-01 01:00:30 [utils.py:250] Using default LoRA kernel configs\n",
                        "âœ… LoRA adapter ì ìš© ì™„ë£Œ\n",
                        "[RequestOutput(request_id=0, prompt='<|im_start|>system\\nYou are a SQL expert. Convert natural language queries to SQL.<|im_end|>\\n<|im_start|>user\\nFind all users with age greater than 30<|im_end|>\\n<|im_start|>assistant\\n', prompt_token_ids=[1, 9690, 198, 2683, 359, 253, 15142, 4507, 30, 29490, 1782, 1789, 18795, 288, 15142, 30, 2, 198, 1, 4093, 198, 11933, 511, 3629, 351, 1850, 2852, 670, 216, 35, 32, 2, 198, 1, 520, 9531, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='SELECT name \\nFROM users \\nWHERE age > 30', token_ids=[23428, 1462, 216, 198, 46510, 3629, 216, 198, 14779, 14298, 1850, 2986, 216, 35, 32, 2], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
                    ]
                }
            ],
            "source": [
                "# Runtime LoRA ì˜ˆì œ (adapterê°€ ìˆëŠ” ê²½ìš°)\n",
                "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
                "lora_adapter_path = \"./checkpoint-100\"  # ì‹¤ì œ adapter ê²½ë¡œë¡œ ë³€ê²½\n",
                "\n",
                "if os.path.exists(lora_adapter_path):\n",
                "    from vllm.lora.request import LoRARequest\n",
                "    \n",
                "    # LoRA ì§€ì› ëª¨ë¸ ë¡œë“œ\n",
                "    llm_with_lora = LLM(\n",
                "        model=model_name,\n",
                "        enable_lora=True,\n",
                "        max_lora_rank=64,\n",
                "        tensor_parallel_size=1,\n",
                "        gpu_memory_utilization=0.75,\n",
                "        trust_remote_code=True,\n",
                "        enforce_eager=True\n",
                "    )\n",
                "    \n",
                "    # LoRA request ìƒì„±\n",
                "    lora_request = LoRARequest(\"my_adapter\", 1, lora_adapter_path) # my_adapterëŠ” ì„ì˜ì˜ ì´ë¦„. ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥ëŠ¥\n",
                "    \n",
                "    # ì¶”ë¡  (LoRA adapter ì ìš©)\n",
                "    outputs = llm_with_lora.generate(\n",
                "        [prompt],\n",
                "        sampling_params,\n",
                "        lora_request=lora_request\n",
                "    )\n",
                "    \n",
                "    print(\"âœ… LoRA adapter ì ìš© ì™„ë£Œ\")\n",
                "    print(outputs)\n",
                "else:\n",
                "    print(\"âš ï¸ LoRA adapter ì—†ìŒ - ê±´ë„ˆë›°ê¸°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### ë°©ë²• 2: Merged Model (ì‚¬ì „ í†µí•©)\n",
                "- LoRA weightsë¥¼ base modelì— ë¯¸ë¦¬ merge\n",
                "- ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„\n",
                "- ë°°í¬ì— ìœ ë¦¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[rank0]:[W201 01:00:32.436326660 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ”„ LoRA merge ì‹œì‘...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`torch_dtype` is deprecated! Use `dtype` instead!\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Merge ì™„ë£Œ: ./merged_model\n",
                        "INFO 02-01 01:00:36 [utils.py:253] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.75, 'disable_log_stats': True, 'model': './merged_model'}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-01 01:00:36 [model.py:514] Resolved architecture: LlamaForCausalLM\n",
                        "INFO 02-01 01:00:36 [model.py:1661] Using max model len 8192\n",
                        "INFO 02-01 01:00:36 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:41 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='./merged_model', speculative_config=None, tokenizer='./merged_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=./merged_model, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:41 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.26.93.109:52725 backend=nccl\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:41 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m WARNING 02-01 01:00:42 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:42 [gpu_model_runner.py:3562] Starting to load model ./merged_model...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m /home/ssafy/skeleton-4/.venv/lib/python3.13/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m We recommend installing via `pip install torch-c-dlpack-ext`\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m   warnings.warn(\n",
                        "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:44 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.97it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.97it/s]\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:44 [default_loader.py:308] Loading weights took 0.18 seconds\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:45 [gpu_model_runner.py:3659] Model loading took 0.6750 GiB memory and 1.939831 seconds\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:50 [backends.py:643] Using cache directory: /home/ssafy/.cache/vllm/torch_compile_cache/45e4fc4647/rank_0_0/backbone for vLLM's torch.compile\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:50 [backends.py:703] Dynamo bytecode transform time: 4.36 s\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:56 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 1.022 s\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:56 [monitor.py:34] torch.compile takes 5.39 s in total\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:57 [gpu_worker.py:375] Available KV cache memory: 3.33 GiB\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:57 [kv_cache_utils.py:1291] GPU KV cache size: 87,168 tokens\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:00:57 [kv_cache_utils.py:1296] Maximum concurrency for 8,192 tokens per request: 10.64x\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:02<00:00, 17.58it/s]\n",
                        "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:02<00:00, 14.69it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:01:04 [gpu_model_runner.py:4587] Graph capturing finished in 7 secs, took 0.49 GiB\n",
                        "\u001b[0;36m(EngineCore_DP0 pid=5294)\u001b[0;0m INFO 02-01 01:01:04 [core.py:259] init engine (profile, create kv cache, warmup model) took 18.75 seconds\n",
                        "INFO 02-01 01:01:04 [llm.py:360] Supported tasks: ['generate']\n",
                        "âœ… Merged ëª¨ë¸ë¡œ ì¶”ë¡  ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoModelForCausalLM\n",
                "from peft import PeftModel\n",
                "\n",
                "\n",
                "\n",
                "def merge_lora_to_base(base_model_name, lora_path, output_path):\n",
                "    \"\"\"\n",
                "    LoRA adapterë¥¼ base modelì— merge\n",
                "    \"\"\"\n",
                "    print(\"ğŸ”„ LoRA merge ì‹œì‘...\")\n",
                "    \n",
                "    # 1. Base model ë¡œë“œ\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\n",
                "        base_model_name,\n",
                "        trust_remote_code=True,\n",
                "        torch_dtype=\"auto\",\n",
                "    )\n",
                "    \n",
                "    # 2. LoRA adapter ë¡œë“œ\n",
                "    model_with_lora = PeftModel.from_pretrained(base_model, lora_path)\n",
                "    \n",
                "    # 3. Merge\n",
                "    merged_model = model_with_lora.merge_and_unload()\n",
                "    \n",
                "    # 4. ì €ì¥\n",
                "    os.makedirs(output_path, exist_ok=True)\n",
                "    merged_model.save_pretrained(output_path, safe_serialization=True)\n",
                "    \n",
                "    # Tokenizerë„ ì €ì¥\n",
                "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
                "    tokenizer.save_pretrained(output_path)\n",
                "\n",
                "    del base_model\n",
                "    del model_with_lora\n",
                "    del merged_model\n",
                "    \n",
                "    print(f\"âœ… Merge ì™„ë£Œ: {output_path}\")\n",
                "    return output_path\n",
                "\n",
                "# ì˜ˆì œ (adapterê°€ ìˆëŠ” ê²½ìš°)\n",
                "if os.path.exists(lora_adapter_path):\n",
                "    if locals().get('llm_with_lora'):\n",
                "        del llm_with_lora\n",
                "\n",
                "    merged_path = merge_lora_to_base(\n",
                "        base_model_name=model_name,\n",
                "        lora_path=lora_adapter_path,\n",
                "        output_path=\"./merged_model\"\n",
                "    )\n",
                "    \n",
                "    # Merged ëª¨ë¸ë¡œ ì¶”ë¡ \n",
                "    llm_merged = LLM(\n",
                "        model=merged_path,\n",
                "        tensor_parallel_size=1,\n",
                "        gpu_memory_utilization=0.75,\n",
                "        trust_remote_code=True,\n",
                "    )\n",
                "    print(\"âœ… Merged ëª¨ë¸ë¡œ ì¶”ë¡  ì¤€ë¹„ ì™„ë£Œ\")\n",
                "else:\n",
                "    print(\"âš ï¸ LoRA adapter ì—†ìŒ - ê±´ë„ˆë›°ê¸°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 5: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ - Transformers vs vLLM\n",
                "\n",
                "vLLMì´ ì–¼ë§ˆë‚˜ ë¹ ë¥¸ì§€ ì§ì ‘ ì¸¡ì •í•´ë´…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì„¤ì •\n",
                        "  - í”„ë¡¬í”„íŠ¸ ìˆ˜: 5\n",
                        "  - Max Tokens: 64\n"
                    ]
                }
            ],
            "source": [
                "# ë²¤ì¹˜ë§ˆí¬ ì¤€ë¹„\n",
                "test_prompts = [\n",
                "    \"You are a SQL expert. Convert this to SQL: Find all users\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Count employees\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Show top 10 sales\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Delete inactive accounts\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Update user emails\",\n",
                "]\n",
                "\n",
                "max_tokens = 64\n",
                "\n",
                "print(f\"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì„¤ì •\")\n",
                "print(f\"  - í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(test_prompts)}\")\n",
                "print(f\"  - Max Tokens: {max_tokens}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Transformers ë²¤ì¹˜ë§ˆí¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ”µ Transformers ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\n",
                        "  ëª¨ë¸ ë¡œë”© ì¤‘...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`torch_dtype` is deprecated! Use `dtype` instead!\n",
                        "[2026-02-01 01:01:05] INFO modeling.py:1004: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… Transformers ì™„ë£Œ\n",
                        "  â±ï¸  ì´ ì‹œê°„: 5.02s\n",
                        "  âš¡ First Token: 98.86ms\n",
                        "  ğŸ”¥ Token/sec: 23.30\n",
                        "  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: 6134 MB\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "import pynvml\n",
                "\n",
                "def get_gpu_memory_usage():\n",
                "    \"\"\"NVIDIA GPUì—ì„œ ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¸¡ì • (nvidia-smiì™€ ë™ì¼)\"\"\"\n",
                "    pynvml.nvmlInit()\n",
                "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
                "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
                "    pynvml.nvmlShutdown()\n",
                "    return info.used / 1024 / 1024  # MB ë‹¨ìœ„\n",
                "\n",
                "print(\"ğŸ”µ Transformers ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\")\n",
                "print(\"  ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
                "\n",
                "# ëª¨ë¸ ë¡œë“œ\n",
                "tf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tf_model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "tf_model.eval()\n",
                "\n",
                "# ì¶”ë¡ \n",
                "tf_start = time.time()\n",
                "tf_total_tokens = 0\n",
                "tf_first_token_latencies = []\n",
                "\n",
                "for prompt in test_prompts:\n",
                "    inputs = tf_tokenizer(prompt, return_tensors=\"pt\").to(tf_model.device)\n",
                "    \n",
                "    # First token ì¸¡ì •\n",
                "    ft_start = time.time()\n",
                "    with torch.no_grad():\n",
                "        outputs = tf_model(**inputs)\n",
                "    first_token_time = time.time() - ft_start\n",
                "    tf_first_token_latencies.append(first_token_time)\n",
                "    \n",
                "    # ì „ì²´ ìƒì„±\n",
                "    with torch.no_grad():\n",
                "        generated = tf_model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_tokens,\n",
                "            do_sample=False,\n",
                "        )\n",
                "    tf_total_tokens += generated.shape[1] - inputs.input_ids.shape[1]\n",
                "\n",
                "tf_time = time.time() - tf_start\n",
                "tf_peak_memory = get_gpu_memory_usage()  # NVIDIA GPU ì‹¤ì œ ë©”ëª¨ë¦¬ ì¸¡ì •\n",
                "tf_avg_first_token = sum(tf_first_token_latencies) / len(tf_first_token_latencies)\n",
                "\n",
                "print(f\"\\nâœ… Transformers ì™„ë£Œ\")\n",
                "print(f\"  â±ï¸  ì´ ì‹œê°„: {tf_time:.2f}s\")\n",
                "print(f\"  âš¡ First Token: {tf_avg_first_token*1000:.2f}ms\")\n",
                "print(f\"  ğŸ”¥ Token/sec: {tf_total_tokens / tf_time:.2f}\")\n",
                "print(f\"  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: {tf_peak_memory:.0f} MB\")\n",
                "\n",
                "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
                "del tf_model\n",
                "del tf_tokenizer\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 vLLM ë²¤ì¹˜ë§ˆí¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ğŸŸ¢ vLLM ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "50e9630aa767496eb5a861b3717d362d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5bfefc7966324213a0f8a742f4e11eec",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f164bec6cccf42e0ae1bbca4712567d9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/5 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2a67eb4d2f4e40ff96bdfe94e9f31aa2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… vLLM ì™„ë£Œ\n",
                        "  â±ï¸  ì´ ì‹œê°„: 0.42s\n",
                        "  âš¡ First Token: 137.75ms\n",
                        "  ğŸ”¥ Token/sec: 212.99\n",
                        "  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: 6022 MB\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\nğŸŸ¢ vLLM ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\")\n",
                "\n",
                "# Sampling íŒŒë¼ë¯¸í„°\n",
                "benchmark_params = SamplingParams(\n",
                "    max_tokens=max_tokens,\n",
                "    temperature=0.0,  # greedy decoding\n",
                ")\n",
                "\n",
                "# First token latency ì¸¡ì •\n",
                "vllm_ft_start = time.time()\n",
                "_ = llm_merged.generate([test_prompts[0]], benchmark_params)\n",
                "vllm_first_token = time.time() - vllm_ft_start\n",
                "\n",
                "# ì „ì²´ ì¶”ë¡ \n",
                "vllm_start = time.time()\n",
                "vllm_outputs = llm_merged.generate(test_prompts, benchmark_params)\n",
                "vllm_time = time.time() - vllm_start\n",
                "\n",
                "vllm_peak_memory = get_gpu_memory_usage()  # NVIDIA GPU ì‹¤ì œ ë©”ëª¨ë¦¬ ì¸¡ì •\n",
                "vllm_total_tokens = sum(len(out.outputs[0].token_ids) for out in vllm_outputs)\n",
                "\n",
                "print(f\"\\nâœ… vLLM ì™„ë£Œ\")\n",
                "print(f\"  â±ï¸  ì´ ì‹œê°„: {vllm_time:.2f}s\")\n",
                "print(f\"  âš¡ First Token: {vllm_first_token*1000:.2f}ms\")\n",
                "print(f\"  ğŸ”¥ Token/sec: {vllm_total_tokens / vllm_time:.2f}\")\n",
                "print(f\"  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: {vllm_peak_memory:.0f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 ê²°ê³¼ ë¹„êµ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "ğŸ“Š ìµœì¢… ë¹„êµ ê²°ê³¼\n",
                        "============================================================\n",
                        "\n",
                        "âš¡ First Token Latency:\n",
                        "  Transformers: 98.86ms\n",
                        "  vLLM:         137.75ms\n",
                        "  âš ï¸ ì°¨ì´: 39.3%\n",
                        "\n",
                        "ğŸ”¥ Token/sec:\n",
                        "  Transformers: 23.30 tokens/sec\n",
                        "  vLLM:         212.99 tokens/sec\n",
                        "  ğŸš€ vLLM í–¥ìƒ: 9.14x\n",
                        "\n",
                        "â±ï¸ ì´ ì¶”ë¡  ì‹œê°„:\n",
                        "  Transformers: 5.02s\n",
                        "  vLLM:         0.42s\n",
                        "  ğŸš€ ì†ë„ í–¥ìƒ: 11.88x\n",
                        "\n",
                        "ğŸ’¾ í”¼í¬ GPU ë©”ëª¨ë¦¬:\n",
                        "  Transformers: 6134 MB\n",
                        "  vLLM:         6022 MB\n",
                        "  ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆê°: 112 MB (1.8%)\n",
                        "\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"ğŸ“Š ìµœì¢… ë¹„êµ ê²°ê³¼\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# First Token Latency\n",
                "print(f\"\\nâš¡ First Token Latency:\")\n",
                "print(f\"  Transformers: {tf_avg_first_token*1000:.2f}ms\")\n",
                "print(f\"  vLLM:         {vllm_first_token*1000:.2f}ms\")\n",
                "ft_improvement = ((tf_avg_first_token - vllm_first_token) / tf_avg_first_token) * 100\n",
                "print(f\"  {'ğŸš€ ê°œì„ ' if ft_improvement > 0 else 'âš ï¸ ì°¨ì´'}: {abs(ft_improvement):.1f}%\")\n",
                "\n",
                "# Token/sec\n",
                "tf_tps = tf_total_tokens / tf_time\n",
                "vllm_tps = vllm_total_tokens / vllm_time\n",
                "print(f\"\\nğŸ”¥ Token/sec:\")\n",
                "print(f\"  Transformers: {tf_tps:.2f} tokens/sec\")\n",
                "print(f\"  vLLM:         {vllm_tps:.2f} tokens/sec\")\n",
                "print(f\"  ğŸš€ vLLM í–¥ìƒ: {vllm_tps / tf_tps:.2f}x\")\n",
                "\n",
                "# ì´ ì‹œê°„\n",
                "print(f\"\\nâ±ï¸ ì´ ì¶”ë¡  ì‹œê°„:\")\n",
                "print(f\"  Transformers: {tf_time:.2f}s\")\n",
                "print(f\"  vLLM:         {vllm_time:.2f}s\")\n",
                "print(f\"  ğŸš€ ì†ë„ í–¥ìƒ: {tf_time / vllm_time:.2f}x\")\n",
                "\n",
                "# ë©”ëª¨ë¦¬\n",
                "print(f\"\\nğŸ’¾ í”¼í¬ GPU ë©”ëª¨ë¦¬:\")\n",
                "print(f\"  Transformers: {tf_peak_memory:.0f} MB\")\n",
                "print(f\"  vLLM:         {vllm_peak_memory:.0f} MB\")\n",
                "memory_diff = tf_peak_memory - vllm_peak_memory\n",
                "if memory_diff > 0:\n",
                "    print(f\"  ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆê°: {memory_diff:.0f} MB ({memory_diff/tf_peak_memory*100:.1f}%)\")\n",
                "else:\n",
                "    print(f\"  ğŸ“ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©: {abs(memory_diff):.0f} MB ë” ì‚¬ìš©\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6: ì¢…í•© ì‹¤ìŠµ\n",
                "\n",
                "### ìµœì¢… í”„ë¡œì íŠ¸: Text-to-SQL ì‹œìŠ¤í…œ êµ¬ì¶•\n",
                "\n",
                "ë°°ìš´ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì‹¤ì „ Text-to-SQL ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ¤– ì¶”ë¡  ì‹œì‘...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "31ac60c4ad864f12b6a50330f39ac884",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c5227a3692a74fc2828728339818b3ba",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… ì¶”ë¡  ì™„ë£Œ! (0.19ì´ˆ)\n",
                        "ğŸ“Š ì²˜ë¦¬ëŸ‰: 21.51 prompts/sec\n",
                        "\n",
                        "[ìƒì„± ê²°ê³¼] SELECT COUNT(*) FROM pets;\n",
                        "[í† í° ìˆ˜] 9\n",
                        "------------------------------------------------------------\n",
                        "[ìƒì„± ê²°ê³¼] SELECT COUNT(*) FROM employee WHERE department = 'Department A'\n",
                        "[í† í° ìˆ˜] 15\n",
                        "------------------------------------------------------------\n",
                        "[ìƒì„± ê²°ê³¼] SELECT COUNT(*) FROM airlines\n",
                        "[í† í° ìˆ˜] 8\n",
                        "------------------------------------------------------------\n",
                        "[ìƒì„± ê²°ê³¼] SELECT * FROM `department` WHERE `budget` > 100000\n",
                        "[í† í° ìˆ˜] 19\n",
                        "------------------------------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "system_prompt = \"\"\"You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query.\"\"\"\n",
                "user_prompt = \"\"\"Given the <USER_QUERY>, generate the corresponding SQL command to retrieve the desired data, considering the query's syntax, semantics, and schema constraints.\n",
                "\n",
                "<USER_QUERY>\n",
                "{question}\n",
                "</USER_QUERY>\"\"\"\n",
                "\n",
                "test_questions = [\n",
                "    \"How many different types of pet are there?\",\n",
                "    \"Count employees in each department\",\n",
                "    \"How many airlines do we have?\",\n",
                "    \"List departments with budget over 100000\",\n",
                "]\n",
                "\n",
                "messages = []\n",
                "for i in range(len(test_questions)):\n",
                "    messages.append(\n",
                "        [\n",
                "            {\n",
                "                \"role\": \"system\",\n",
                "                \"content\": system_prompt\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": user_prompt.format(question=test_questions[i])\n",
                "            }\n",
                "        ]\n",
                "    )\n",
                "\n",
                "prompts = format_chat_messages(messages)\n",
                "\n",
                "print(\"ğŸ¤– ì¶”ë¡  ì‹œì‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "outputs = llm_merged.generate(prompts, sampling_params)\n",
                "\n",
                "inference_time = time.time() - start_time\n",
                "\n",
                "print(f\"\\nâœ… ì¶”ë¡  ì™„ë£Œ! ({inference_time:.2f}ì´ˆ)\")\n",
                "print(f\"ğŸ“Š ì²˜ë¦¬ëŸ‰: {len(prompts) / inference_time:.2f} prompts/sec\\n\")\n",
                "\n",
                "# ê²°ê³¼ ì¶œë ¥\n",
                "for i, output in enumerate(outputs):\n",
                "    generated_text = output.outputs[0].text\n",
                "    token_count = len(output.outputs[0].token_ids)\n",
                "    \n",
                "    print(f\"[ìƒì„± ê²°ê³¼] {generated_text}\")\n",
                "    print(f\"[í† í° ìˆ˜] {token_count}\")\n",
                "    print(\"-\" * 60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
