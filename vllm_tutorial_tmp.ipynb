{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# vLLM ì™„ë²½ ê°€ì´ë“œ: ì´ë¡ ë¶€í„° ì‹¤ì „ê¹Œì§€\n",
                "\n",
                "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” vLLMì„ ì´ìš©í•œ íš¨ìœ¨ì ì¸ LLM ì¶”ë¡ ì„ ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
                "\n",
                "## í•™ìŠµ ëª©í‘œ\n",
                "1. âœ… vLLMì˜ í•µì‹¬ ê°œë… ì´í•´ (PagedAttention, KV Cache)\n",
                "2. âœ… vLLM ê¸°ë³¸ ì‚¬ìš©ë²• ì‹¤ìŠµ\n",
                "3. âœ… LoRA adapter í†µí•© ë°©ë²•\n",
                "4. âœ… Transformers vs vLLM ì„±ëŠ¥ ë¹„êµ\n",
                "5. âœ… ì‹¤ì „ ì‘ìš© (Text-to-SQL)\n",
                "\n",
                "## ì‚¬ì „ ì¤€ë¹„\n",
                "```bash\n",
                "pip install vllm==0.13.0 transformers torch peft\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 1: vLLM ê¸°ì´ˆ ê°œë…\n",
                "\n",
                "### 1.1 ì™œ vLLMì¸ê°€?\n",
                "\n",
                "ì¼ë°˜ì ì¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë¬¸ì œì :\n",
                "- **ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨**: KV Cacheë¥¼ ì—°ì†ëœ ë©”ëª¨ë¦¬ì— ì €ì¥ â†’ ë‹¨í¸í™” ë°œìƒ\n",
                "- **ëŠë¦° ì†ë„**: ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë¹„íš¨ìœ¨ì \n",
                "- **ë‚®ì€ ì²˜ë¦¬ëŸ‰**: ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ ì œí•œ\n",
                "\n",
                "vLLMì˜ í•´ê²°ì±…:\n",
                "- **PagedAttention**: OSì˜ ê°€ìƒ ë©”ëª¨ë¦¬ì²˜ëŸ¼ KV Cacheë¥¼ ë¸”ë¡ ë‹¨ìœ„ë¡œ ê´€ë¦¬\n",
                "- **ì—°ì† ë°°ì¹­**: ìš”ì²­ì„ ë™ì ìœ¼ë¡œ ë°°ì¹­í•˜ì—¬ ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”\n",
                "- **ìµœì í™”ëœ CUDA ì»¤ë„**: GPU í™œìš©ë„ í–¥ìƒ\n",
                "\n",
                "### 1.2 í•µì‹¬ ê°œë…: PagedAttention\n",
                "\n",
                "```\n",
                "[ê¸°ì¡´ ë°©ì‹]\n",
                "KV Cache: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  â† ë¯¸ë¦¬ í° ë©”ëª¨ë¦¬ í• ë‹¹, ë‚­ë¹„ ë°œìƒ\n",
                "\n",
                "[PagedAttention]\n",
                "Block 0: [â–ˆâ–ˆâ–ˆâ–ˆ]\n",
                "Block 3: [â–ˆâ–ˆâ–ˆâ–ˆ]  â† í•„ìš”í•  ë•Œë§Œ í• ë‹¹, ë¹„ì—°ì†ì  ì €ì¥ ê°€ëŠ¥\n",
                "Block 7: [â–ˆâ–ˆâ–‘â–‘]\n",
                "Block Table: [0, 3, 7]  â† ë§¤í•‘ í…Œì´ë¸”ë¡œ ê´€ë¦¬\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
                "import os\n",
                "import time\n",
                "import torch\n",
                "from vllm import LLM, SamplingParams\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")\n",
                "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
                "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 2: vLLM ê¸°ë³¸ ì‚¬ìš©ë²•\n",
                "\n",
                "### 2.1 ëª¨ë¸ ë¡œë”©"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: vLLM ëª¨ë¸ ì´ˆê¸°í™”\n",
                "model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B\"\n",
                "\n",
                "print(\"ğŸš€ vLLM ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "llm = LLM(\n",
                "    model=model_name,\n",
                "    tensor_parallel_size=1,       # ë‹¨ì¼ GPU ì‚¬ìš©\n",
                "    gpu_memory_utilization=0.9,   # GPU ë©”ëª¨ë¦¬ 90% ì‚¬ìš©\n",
                "    trust_remote_code=True,       # ì»¤ìŠ¤í…€ ëª¨ë¸ ì½”ë“œ ì‹ ë¢°\n",
                ")\n",
                "\n",
                "load_time = time.time() - start_time\n",
                "print(f\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ({load_time:.2f}ì´ˆ)\")\n",
                "\n",
                "# Tokenizer ë¡œë“œ\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "print(f\"ğŸ“ Tokenizer ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 ê°„ë‹¨í•œ ì¶”ë¡  ì‹¤ìŠµ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2: Sampling íŒŒë¼ë¯¸í„° ì„¤ì •\n",
                "sampling_params = SamplingParams(\n",
                "    max_tokens=128,        # ìµœëŒ€ ìƒì„± í† í° ìˆ˜\n",
                "    temperature=0.7,       # ìƒ˜í”Œë§ ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€)\n",
                "    top_p=0.9,            # Nucleus sampling\n",
                "    top_k=50,             # Top-k sampling\n",
                ")\n",
                "\n",
                "print(\"âš™ï¸  Sampling íŒŒë¼ë¯¸í„°:\")\n",
                "print(f\"  - Max Tokens: {sampling_params.max_tokens}\")\n",
                "print(f\"  - Temperature: {sampling_params.temperature}\")\n",
                "print(f\"  - Top-p: {sampling_params.top_p}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3: ì¶”ë¡  ì‹¤í–‰\n",
                "prompts = [\n",
                "    \"Explain quantum computing in simple terms:\",\n",
                "    \"What is the capital of France?\",\n",
                "    \"Write a haiku about programming:\",\n",
                "]\n",
                "\n",
                "print(\"ğŸ¤– ì¶”ë¡  ì‹œì‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "outputs = llm.generate(prompts, sampling_params)\n",
                "\n",
                "inference_time = time.time() - start_time\n",
                "\n",
                "print(f\"\\nâœ… ì¶”ë¡  ì™„ë£Œ! ({inference_time:.2f}ì´ˆ)\")\n",
                "print(f\"ğŸ“Š ì²˜ë¦¬ëŸ‰: {len(prompts) / inference_time:.2f} prompts/sec\\n\")\n",
                "\n",
                "# ê²°ê³¼ ì¶œë ¥\n",
                "for i, output in enumerate(outputs):\n",
                "    generated_text = output.outputs[0].text\n",
                "    token_count = len(output.outputs[0].token_ids)\n",
                "    \n",
                "    print(f\"[í”„ë¡¬í”„íŠ¸ {i+1}] {prompts[i]}\")\n",
                "    print(f\"[ìƒì„± ê²°ê³¼] {generated_text}\")\n",
                "    print(f\"[í† í° ìˆ˜] {token_count}\")\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ’¡ ì‹¤ìŠµ ë¬¸ì œ 1\n",
                "\n",
                "**Q1**: `temperature`ë¥¼ 0.1, 0.7, 1.5ë¡œ ë°”ê¿”ê°€ë©° ê²°ê³¼ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”.\n",
                "- ì–´ë–¤ ì°¨ì´ê°€ ìˆë‚˜ìš”?\n",
                "- ì–´ë–¤ ìƒí™©ì— ì–´ë–¤ ê°’ì„ ì‚¬ìš©í•˜ë©´ ì¢‹ì„ê¹Œìš”?\n",
                "\n",
                "**Q2**: `max_tokens`ë¥¼ 32, 64, 256ìœ¼ë¡œ ë³€ê²½í•˜ê³  ì¶”ë¡  ì‹œê°„ì„ ì¸¡ì •í•´ë³´ì„¸ìš”.\n",
                "- í† í° ìˆ˜ì™€ ì¶”ë¡  ì‹œê°„ì˜ ê´€ê³„ëŠ”?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ì‹¤ìŠµ ê³µê°„: ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 3: Chat í˜•ì‹ í”„ë¡¬í”„íŠ¸\n",
                "\n",
                "ì‹¤ì „ì—ì„œëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ìœ ì € ë©”ì‹œì§€, ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ ì¡°í•©í•œ chat í˜•ì‹ì„ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_chat_messages(messages):\n",
                "    \"\"\"\n",
                "    Chat ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜\n",
                "    \n",
                "    Args:\n",
                "        messages: [{'role': 'system/user/assistant', 'content': '...'}, ...]\n",
                "    \"\"\"\n",
                "    if tokenizer.chat_template:\n",
                "        # Tokenizerì— chat templateì´ ìˆìœ¼ë©´ ì‚¬ìš©\n",
                "        return tokenizer.apply_chat_template(\n",
                "            messages,\n",
                "            tokenize=False,\n",
                "            add_generation_prompt=True\n",
                "        )\n",
                "    else:\n",
                "        # ìˆ˜ë™ í¬ë§·íŒ…\n",
                "        formatted = \"\"\n",
                "        for msg in messages:\n",
                "            role = msg['role'].capitalize()\n",
                "            content = msg['content']\n",
                "            formatted += f\"{role}: {content}\\n\\n\"\n",
                "        \n",
                "        if messages[-1]['role'] != 'assistant':\n",
                "            formatted += \"Assistant: \"\n",
                "        \n",
                "        return formatted\n",
                "\n",
                "print(\"âœ… Chat í¬ë§·íŒ… í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Text-to-SQL ì˜ˆì œ\n",
                "messages = [\n",
                "    {\n",
                "        'role': 'system',\n",
                "        'content': 'You are a SQL expert. Convert natural language queries to SQL.'\n",
                "    },\n",
                "    {\n",
                "        'role': 'user',\n",
                "        'content': 'Find all users with age greater than 30'\n",
                "    }\n",
                "]\n",
                "\n",
                "# í”„ë¡¬í”„íŠ¸ ë³€í™˜\n",
                "prompt = format_chat_messages(messages)\n",
                "print(\"ğŸ“ ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸:\")\n",
                "print(prompt)\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# ì¶”ë¡ \n",
                "sampling_params_sql = SamplingParams(\n",
                "    max_tokens=64,\n",
                "    temperature=0.1,  # SQL ìƒì„±ì€ ë‚®ì€ temperature ì‚¬ìš©\n",
                ")\n",
                "\n",
                "outputs = llm.generate([prompt], sampling_params_sql)\n",
                "print(\"\\nğŸ’¡ ìƒì„±ëœ SQL:\")\n",
                "print(outputs[0].outputs[0].text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ’¡ ì‹¤ìŠµ ë¬¸ì œ 2\n",
                "\n",
                "ë‹¤ìŒ ìì—°ì–´ ì§ˆì˜ë¥¼ SQLë¡œ ë³€í™˜í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ê³  ì‹¤í–‰í•´ë³´ì„¸ìš”:\n",
                "\n",
                "1. \"Show top 10 products by sales\"\n",
                "2. \"Count employees in each department\"\n",
                "3. \"Find customers who haven't made a purchase in 6 months\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ì‹¤ìŠµ ê³µê°„\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 4: LoRA Adapter ì‚¬ìš©í•˜ê¸°\n",
                "\n",
                "### 4.1 LoRAë€?\n",
                "\n",
                "**LoRA (Low-Rank Adaptation)**:\n",
                "- ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ fine-tuningí•˜ëŠ” ëŒ€ì‹ , ì‘ì€ adapterë§Œ í•™ìŠµ\n",
                "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥\n",
                "- ì—¬ëŸ¬ taskë³„ adapterë¥¼ ì‰½ê²Œ êµì²´ ê°€ëŠ¥\n",
                "\n",
                "### 4.2 vLLMì—ì„œ LoRA ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•\n",
                "\n",
                "#### ë°©ë²• 1: Runtime LoRA (ë™ì  ì ìš©)\n",
                "- ì¶”ë¡  ì‹œì ì— adapterë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ\n",
                "- ì—¬ëŸ¬ adapterë¥¼ ë¹ ë¥´ê²Œ êµì²´ ê°€ëŠ¥"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Runtime LoRA ì˜ˆì œ (adapterê°€ ìˆëŠ” ê²½ìš°)\n",
                "lora_adapter_path = \"./lora_adapter\"  # ì‹¤ì œ adapter ê²½ë¡œë¡œ ë³€ê²½\n",
                "\n",
                "if os.path.exists(lora_adapter_path):\n",
                "    from vllm.lora.request import LoRARequest\n",
                "    \n",
                "    # LoRA ì§€ì› ëª¨ë¸ ë¡œë“œ\n",
                "    llm_with_lora = LLM(\n",
                "        model=model_name,\n",
                "        enable_lora=True,\n",
                "        max_lora_rank=64,\n",
                "        tensor_parallel_size=1,\n",
                "        gpu_memory_utilization=0.9,\n",
                "        trust_remote_code=True,\n",
                "    )\n",
                "    \n",
                "    # LoRA request ìƒì„±\n",
                "    lora_request = LoRARequest(\"my_adapter\", 1, lora_adapter_path)\n",
                "    \n",
                "    # ì¶”ë¡  (LoRA adapter ì ìš©)\n",
                "    outputs = llm_with_lora.generate(\n",
                "        [prompt],\n",
                "        sampling_params,\n",
                "        lora_request=lora_request\n",
                "    )\n",
                "    \n",
                "    print(\"âœ… LoRA adapter ì ìš© ì™„ë£Œ\")\n",
                "else:\n",
                "    print(\"âš ï¸ LoRA adapter ì—†ìŒ - ê±´ë„ˆë›°ê¸°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### ë°©ë²• 2: Merged Model (ì‚¬ì „ í†µí•©)\n",
                "- LoRA weightsë¥¼ base modelì— ë¯¸ë¦¬ merge\n",
                "- ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„\n",
                "- ë°°í¬ì— ìœ ë¦¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM\n",
                "from peft import PeftModel\n",
                "\n",
                "def merge_lora_to_base(base_model_name, lora_path, output_path):\n",
                "    \"\"\"\n",
                "    LoRA adapterë¥¼ base modelì— merge\n",
                "    \"\"\"\n",
                "    print(\"ğŸ”„ LoRA merge ì‹œì‘...\")\n",
                "    \n",
                "    # 1. Base model ë¡œë“œ\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\n",
                "        base_model_name,\n",
                "        trust_remote_code=True,\n",
                "        torch_dtype=\"auto\"\n",
                "    )\n",
                "    \n",
                "    # 2. LoRA adapter ë¡œë“œ\n",
                "    model_with_lora = PeftModel.from_pretrained(base_model, lora_path)\n",
                "    \n",
                "    # 3. Merge\n",
                "    merged_model = model_with_lora.merge_and_unload()\n",
                "    \n",
                "    # 4. ì €ì¥\n",
                "    os.makedirs(output_path, exist_ok=True)\n",
                "    merged_model.save_pretrained(output_path, safe_serialization=True)\n",
                "    \n",
                "    # Tokenizerë„ ì €ì¥\n",
                "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
                "    tokenizer.save_pretrained(output_path)\n",
                "    \n",
                "    print(f\"âœ… Merge ì™„ë£Œ: {output_path}\")\n",
                "    return output_path\n",
                "\n",
                "# ì˜ˆì œ (adapterê°€ ìˆëŠ” ê²½ìš°)\n",
                "if os.path.exists(lora_adapter_path):\n",
                "    merged_path = merge_lora_to_base(\n",
                "        base_model_name=model_name,\n",
                "        lora_path=lora_adapter_path,\n",
                "        output_path=\"./merged_model\"\n",
                "    )\n",
                "    \n",
                "    # Merged ëª¨ë¸ë¡œ ì¶”ë¡ \n",
                "    llm_merged = LLM(\n",
                "        model=merged_path,\n",
                "        tensor_parallel_size=1,\n",
                "        gpu_memory_utilization=0.9,\n",
                "        trust_remote_code=True,\n",
                "    )\n",
                "    print(\"âœ… Merged ëª¨ë¸ë¡œ ì¶”ë¡  ì¤€ë¹„ ì™„ë£Œ\")\n",
                "else:\n",
                "    print(\"âš ï¸ LoRA adapter ì—†ìŒ - ê±´ë„ˆë›°ê¸°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 5: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ - Transformers vs vLLM\n",
                "\n",
                "vLLMì´ ì–¼ë§ˆë‚˜ ë¹ ë¥¸ì§€ ì§ì ‘ ì¸¡ì •í•´ë´…ì‹œë‹¤!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë²¤ì¹˜ë§ˆí¬ ì¤€ë¹„\n",
                "test_prompts = [\n",
                "    \"You are a SQL expert. Convert this to SQL: Find all users\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Count employees\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Show top 10 sales\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Delete inactive accounts\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Update user emails\",\n",
                "]\n",
                "\n",
                "max_tokens = 64\n",
                "\n",
                "print(f\"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì„¤ì •\")\n",
                "print(f\"  - í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(test_prompts)}\")\n",
                "print(f\"  - Max Tokens: {max_tokens}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Transformers ë²¤ì¹˜ë§ˆí¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "print(\"ğŸ”µ Transformers ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\")\n",
                "print(\"  ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
                "\n",
                "# ëª¨ë¸ ë¡œë“œ\n",
                "tf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tf_model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "tf_model.eval()\n",
                "\n",
                "# GPU ë©”ëª¨ë¦¬ ë¦¬ì…‹\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.reset_peak_memory_stats()\n",
                "\n",
                "# ì¶”ë¡ \n",
                "tf_start = time.time()\n",
                "tf_total_tokens = 0\n",
                "tf_first_token_latencies = []\n",
                "\n",
                "for prompt in test_prompts:\n",
                "    inputs = tf_tokenizer(prompt, return_tensors=\"pt\").to(tf_model.device)\n",
                "    \n",
                "    # First token ì¸¡ì •\n",
                "    ft_start = time.time()\n",
                "    with torch.no_grad():\n",
                "        outputs = tf_model(**inputs)\n",
                "    first_token_time = time.time() - ft_start\n",
                "    tf_first_token_latencies.append(first_token_time)\n",
                "    \n",
                "    # ì „ì²´ ìƒì„±\n",
                "    with torch.no_grad():\n",
                "        generated = tf_model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_tokens,\n",
                "            do_sample=False,\n",
                "        )\n",
                "    tf_total_tokens += generated.shape[1] - inputs.input_ids.shape[1]\n",
                "\n",
                "tf_time = time.time() - tf_start\n",
                "tf_peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0\n",
                "tf_avg_first_token = sum(tf_first_token_latencies) / len(tf_first_token_latencies)\n",
                "\n",
                "print(f\"\\nâœ… Transformers ì™„ë£Œ\")\n",
                "print(f\"  â±ï¸  ì´ ì‹œê°„: {tf_time:.2f}s\")\n",
                "print(f\"  âš¡ First Token: {tf_avg_first_token*1000:.2f}ms\")\n",
                "print(f\"  ğŸ”¥ Token/sec: {tf_total_tokens / tf_time:.2f}\")\n",
                "print(f\"  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: {tf_peak_memory:.0f} MB\")\n",
                "\n",
                "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
                "del tf_model\n",
                "del tf_tokenizer\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 vLLM ë²¤ì¹˜ë§ˆí¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nğŸŸ¢ vLLM ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\")\n",
                "\n",
                "# GPU ë©”ëª¨ë¦¬ ë¦¬ì…‹\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.reset_peak_memory_stats()\n",
                "\n",
                "# Sampling íŒŒë¼ë¯¸í„°\n",
                "benchmark_params = SamplingParams(\n",
                "    max_tokens=max_tokens,\n",
                "    temperature=0.0,  # greedy decoding\n",
                ")\n",
                "\n",
                "# First token latency ì¸¡ì •\n",
                "vllm_ft_start = time.time()\n",
                "_ = llm.generate([test_prompts[0]], benchmark_params)\n",
                "vllm_first_token = time.time() - vllm_ft_start\n",
                "\n",
                "# ì „ì²´ ì¶”ë¡ \n",
                "vllm_start = time.time()\n",
                "vllm_outputs = llm.generate(test_prompts, benchmark_params)\n",
                "vllm_time = time.time() - vllm_start\n",
                "\n",
                "vllm_peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0\n",
                "vllm_total_tokens = sum(len(out.outputs[0].token_ids) for out in vllm_outputs)\n",
                "\n",
                "print(f\"\\nâœ… vLLM ì™„ë£Œ\")\n",
                "print(f\"  â±ï¸  ì´ ì‹œê°„: {vllm_time:.2f}s\")\n",
                "print(f\"  âš¡ First Token: {vllm_first_token*1000:.2f}ms\")\n",
                "print(f\"  ğŸ”¥ Token/sec: {vllm_total_tokens / vllm_time:.2f}\")\n",
                "print(f\"  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: {vllm_peak_memory:.0f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 ê²°ê³¼ ë¹„êµ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"ğŸ“Š ìµœì¢… ë¹„êµ ê²°ê³¼\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# First Token Latency\n",
                "print(f\"\\nâš¡ First Token Latency:\")\n",
                "print(f\"  Transformers: {tf_avg_first_token*1000:.2f}ms\")\n",
                "print(f\"  vLLM:         {vllm_first_token*1000:.2f}ms\")\n",
                "ft_improvement = ((tf_avg_first_token - vllm_first_token) / tf_avg_first_token) * 100\n",
                "print(f\"  {'ğŸš€ ê°œì„ ' if ft_improvement > 0 else 'âš ï¸ ì°¨ì´'}: {abs(ft_improvement):.1f}%\")\n",
                "\n",
                "# Token/sec\n",
                "tf_tps = tf_total_tokens / tf_time\n",
                "vllm_tps = vllm_total_tokens / vllm_time\n",
                "print(f\"\\nğŸ”¥ Token/sec:\")\n",
                "print(f\"  Transformers: {tf_tps:.2f} tokens/sec\")\n",
                "print(f\"  vLLM:         {vllm_tps:.2f} tokens/sec\")\n",
                "print(f\"  ğŸš€ vLLM í–¥ìƒ: {vllm_tps / tf_tps:.2f}x\")\n",
                "\n",
                "# ì´ ì‹œê°„\n",
                "print(f\"\\nâ±ï¸ ì´ ì¶”ë¡  ì‹œê°„:\")\n",
                "print(f\"  Transformers: {tf_time:.2f}s\")\n",
                "print(f\"  vLLM:         {vllm_time:.2f}s\")\n",
                "print(f\"  ğŸš€ ì†ë„ í–¥ìƒ: {tf_time / vllm_time:.2f}x\")\n",
                "\n",
                "# ë©”ëª¨ë¦¬\n",
                "print(f\"\\nğŸ’¾ í”¼í¬ GPU ë©”ëª¨ë¦¬:\")\n",
                "print(f\"  Transformers: {tf_peak_memory:.0f} MB\")\n",
                "print(f\"  vLLM:         {vllm_peak_memory:.0f} MB\")\n",
                "memory_diff = tf_peak_memory - vllm_peak_memory\n",
                "if memory_diff > 0:\n",
                "    print(f\"  ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆê°: {memory_diff:.0f} MB ({memory_diff/tf_peak_memory*100:.1f}%)\")\n",
                "else:\n",
                "    print(f\"  ğŸ“ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©: {abs(memory_diff):.0f} MB ë” ì‚¬ìš©\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ’¡ ë¶„ì„ í¬ì¸íŠ¸\n",
                "\n",
                "ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ë³´ë©´:\n",
                "\n",
                "1. **First Token Latency**: vLLMì´ ë” ë¹ ë¥¸ ì´ìœ ëŠ”?\n",
                "   - ìµœì í™”ëœ CUDA ì»¤ë„\n",
                "   - íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
                "\n",
                "2. **Token/sec**: vLLMì´ ì••ë„ì ì¸ ì´ìœ ëŠ”?\n",
                "   - PagedAttentionì˜ íš¨ìœ¨ì„±\n",
                "   - ì—°ì† ë°°ì¹­ (Continuous Batching)\n",
                "   - KV Cache ì¬ì‚¬ìš©\n",
                "\n",
                "3. **ë©”ëª¨ë¦¬**: ì™œ ì ˆê°ë˜ëŠ”ê°€?\n",
                "   - ë™ì  ë©”ëª¨ë¦¬ í• ë‹¹\n",
                "   - ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€\n",
                "   - ë¸”ë¡ ë‹¨ìœ„ ê´€ë¦¬\n",
                "\n",
                "4. **ë°°ì¹˜ ì²˜ë¦¬**: í”„ë¡¬í”„íŠ¸ê°€ ë§ì„ìˆ˜ë¡ vLLMì˜ ì´ì ì´ ì»¤ì§‘ë‹ˆë‹¤!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 6: ì¢…í•© ì‹¤ìŠµ\n",
                "\n",
                "### ìµœì¢… í”„ë¡œì íŠ¸: Text-to-SQL ì‹œìŠ¤í…œ êµ¬ì¶•\n",
                "\n",
                "ë°°ìš´ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì‹¤ì „ Text-to-SQL ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TextToSQLSystem:\n",
                "    def __init__(self, model_name):\n",
                "        self.llm = LLM(\n",
                "            model=model_name,\n",
                "            tensor_parallel_size=1,\n",
                "            gpu_memory_utilization=0.9,\n",
                "            trust_remote_code=True,\n",
                "        )\n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "        \n",
                "        self.system_prompt = \"\"\"\n",
                "You are an expert SQL query generator. Convert natural language questions to SQL queries.\n",
                "Generate only the SQL query without explanations.\n",
                "\"\"\"\n",
                "    \n",
                "    def generate_sql(self, question: str, schema_info: str = \"\") -> str:\n",
                "        \"\"\"ìì—°ì–´ ì§ˆì˜ë¥¼ SQLë¡œ ë³€í™˜\"\"\"\n",
                "        \n",
                "        user_content = f\"\"\"\n",
                "Database Schema:\n",
                "{schema_info if schema_info else 'Standard database tables'}\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "SQL Query:\n",
                "\"\"\"\n",
                "        \n",
                "        messages = [\n",
                "            {'role': 'system', 'content': self.system_prompt},\n",
                "            {'role': 'user', 'content': user_content}\n",
                "        ]\n",
                "        \n",
                "        prompt = format_chat_messages(messages)\n",
                "        \n",
                "        sampling_params = SamplingParams(\n",
                "            max_tokens=128,\n",
                "            temperature=0.1,\n",
                "            stop=[\";\", \"\\n\\n\"]\n",
                "        )\n",
                "        \n",
                "        outputs = self.llm.generate([prompt], sampling_params)\n",
                "        sql_query = outputs[0].outputs[0].text.strip()\n",
                "        \n",
                "        return sql_query\n",
                "\n",
                "# ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
                "sql_system = TextToSQLSystem(model_name)\n",
                "print(\"âœ… Text-to-SQL ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# í…ŒìŠ¤íŠ¸\n",
                "schema = \"\"\"\n",
                "Tables:\n",
                "- users (id, name, email, age, department_id)\n",
                "- departments (id, name, budget)\n",
                "- orders (id, user_id, product_id, amount, order_date)\n",
                "\"\"\"\n",
                "\n",
                "test_questions = [\n",
                "    \"Find all users older than 30\",\n",
                "    \"Count employees in each department\",\n",
                "    \"Show top 5 users by order amount\",\n",
                "    \"List departments with budget over 100000\",\n",
                "]\n",
                "\n",
                "print(\"ğŸ§ª Text-to-SQL í…ŒìŠ¤íŠ¸\\n\")\n",
                "\n",
                "for question in test_questions:\n",
                "    print(f\"â“ Question: {question}\")\n",
                "    sql = sql_system.generate_sql(question, schema)\n",
                "    print(f\"ğŸ’¡ Generated SQL: {sql}\")\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ’¡ ìµœì¢… ì‹¤ìŠµ ë¬¸ì œ\n",
                "\n",
                "**ê³¼ì œ**: `TextToSQLSystem`ì„ ê°œì„ í•´ë³´ì„¸ìš”!\n",
                "\n",
                "1. **Few-shot Learning ì¶”ê°€**:\n",
                "   - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì˜ˆì œ query-SQL í˜ì–´ ì¶”ê°€\n",
                "   - ì„±ëŠ¥ì´ ê°œì„ ë˜ë‚˜ìš”?\n",
                "\n",
                "2. **ì—ëŸ¬ ì²˜ë¦¬**:\n",
                "   - ì˜ëª»ëœ SQLì´ ìƒì„±ë˜ë©´ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ ì¶”ê°€\n",
                "   - SQL ìœ íš¨ì„± ê²€ì¦ ì¶”ê°€\n",
                "\n",
                "3. **ë°°ì¹˜ ì²˜ë¦¬**:\n",
                "   - ì—¬ëŸ¬ ì§ˆì˜ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ê¸°ëŠ¥ ì¶”ê°€\n",
                "   - ì„±ëŠ¥ ì°¨ì´ ì¸¡ì •\n",
                "\n",
                "4. **ì„±ëŠ¥ ìµœì í™”**:\n",
                "   - ë‹¤ì–‘í•œ sampling parameters ì‹¤í—˜\n",
                "   - ìµœì ì˜ ì„¤ì • ì°¾ê¸°"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ìµœì¢… ì‹¤ìŠµ ê³µê°„\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ì •ë¦¬ ë° ìš”ì•½\n",
                "\n",
                "### âœ… í•™ìŠµí•œ ë‚´ìš©\n",
                "\n",
                "1. **vLLM í•µì‹¬ ê°œë…**\n",
                "   - PagedAttention: ë¸”ë¡ ë‹¨ìœ„ ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
                "   - KV Cache: íš¨ìœ¨ì ì¸ ìºì‹± ë©”ì»¤ë‹ˆì¦˜\n",
                "   - ì—°ì† ë°°ì¹­: ë™ì  ë°°ì¹˜ ì²˜ë¦¬\n",
                "\n",
                "2. **ì‹¤ì „ ì‚¬ìš©ë²•**\n",
                "   - ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”\n",
                "   - Sampling parameters ì„¤ì •\n",
                "   - Chat í˜•ì‹ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬\n",
                "\n",
                "3. **LoRA í†µí•©**\n",
                "   - Runtime LoRA: ë™ì  ì ìš©\n",
                "   - Merged Model: ì‚¬ì „ í†µí•©\n",
                "\n",
                "4. **ì„±ëŠ¥ ìµœì í™”**\n",
                "   - Transformers ëŒ€ë¹„ 2-3ë°° ë¹ ë¥¸ ì†ë„\n",
                "   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ \n",
                "   - ë†’ì€ ì²˜ë¦¬ëŸ‰\n",
                "\n",
                "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
                "\n",
                "1. **í”„ë¡œë•ì…˜ ë°°í¬**\n",
                "   - FastAPIì™€ í†µí•©\n",
                "   - ë©€í‹° GPU ì„¤ì •\n",
                "   - ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…\n",
                "\n",
                "2. **ê³ ê¸‰ ê¸°ëŠ¥**\n",
                "   - Multi-LoRA: ì—¬ëŸ¬ adapter ë™ì‹œ ì‚¬ìš©\n",
                "   - Prefix Caching: ê³µí†µ í”„ë¡¬í”„íŠ¸ ìºì‹±\n",
                "   - Speculative Decoding: ì¶”ë¡  ê°€ì†í™”\n",
                "\n",
                "3. **ì‹¤ì „ ì‘ìš©**\n",
                "   - ì±—ë´‡ ì‹œìŠ¤í…œ\n",
                "   - ì½”ë“œ ìƒì„± ë„êµ¬\n",
                "   - ë¬¸ì„œ ìš”ì•½ ì„œë¹„ìŠ¤\n",
                "\n",
                "### ğŸ“š ì°¸ê³  ìë£Œ\n",
                "\n",
                "- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)\n",
                "- [PagedAttention ë…¼ë¬¸](https://arxiv.org/abs/2309.06180)\n",
                "- [LoRA ë…¼ë¬¸](https://arxiv.org/abs/2106.09685)\n",
                "\n",
                "---\n",
                "\n",
                "**ğŸ‰ ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤! ì´ì œ ì—¬ëŸ¬ë¶„ì€ vLLM ë§ˆìŠ¤í„°ì…ë‹ˆë‹¤!**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
