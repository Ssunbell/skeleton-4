"""
vLLM Offline Inference with LoRA Adapter
Model: naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B
"""

import os
import time
import torch
from typing import List, Dict, Optional, Tuple
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel


def merge_lora_to_base_model(
    base_model_name: str,
    lora_adapter_path: str,
    output_path: str,
    save_tokenizer: bool = True,
) -> str:
    """
    LoRA adapterë¥¼ base modelì— mergeí•˜ì—¬ í†µí•© ëª¨ë¸ ìƒì„±

    Args:
        base_model_name: ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” ì´ë¦„
        lora_adapter_path: LoRA adapter ê²½ë¡œ
        output_path: mergeëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        save_tokenizer: tokenizerë„ í•¨ê»˜ ì €ì¥í• ì§€ ì—¬ë¶€

    Returns:
        ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ
    """
    print(f"ğŸ”„ LoRA merge ì‹œì‘...")
    print(f"  Base Model: {base_model_name}")
    print(f"  LoRA Adapter: {lora_adapter_path}")

    # Base model ë¡œë“œ
    print("  1) Base model ë¡œë”© ì¤‘...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        trust_remote_code=True,
        torch_dtype="auto",
    )

    # LoRA adapter ë¡œë“œ
    print("  2) LoRA adapter ë¡œë”© ì¤‘...")
    model_with_lora = PeftModel.from_pretrained(base_model, lora_adapter_path)

    # Merge ìˆ˜í–‰
    print("  3) LoRA weightsë¥¼ base modelì— merge ì¤‘...")
    merged_model = model_with_lora.merge_and_unload()

    # ì €ì¥
    print(f"  4) Merged model ì €ì¥ ì¤‘: {output_path}")
    os.makedirs(output_path, exist_ok=True)
    merged_model.save_pretrained(output_path, safe_serialization=True)

    if save_tokenizer:
        print("  5) Tokenizer ì €ì¥ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        tokenizer.save_pretrained(output_path)

    print(f"âœ… LoRA merge ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {output_path}\n")
    return output_path


class VLLMInferenceWithLoRA:
    def __init__(
        self,
        base_model_name: str = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B",
        lora_adapter_path: str = None,
        tensor_parallel_size: int = 1,
        gpu_memory_utilization: float = 0.9,
    ):
        """
        vLLM ì˜¤í”„ë¼ì¸ ì¶”ë¡  ì´ˆê¸°í™”

        Args:
            base_model_name: ê¸°ë³¸ ëª¨ë¸ ì´ë¦„
            lora_adapter_path: LoRA adapter ê²½ë¡œ (mergeí•  ê²½ìš°)
            tensor_parallel_size: GPU ë³‘ë ¬í™” í¬ê¸°
            gpu_memory_utilization: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        """
        self.base_model_name = base_model_name
        self.lora_adapter_path = lora_adapter_path

        # Tokenizer ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)

        # vLLM ëª¨ë¸ ë¡œë“œ
        if lora_adapter_path:
            # LoRA adapterì™€ í•¨ê»˜ ë¡œë“œ (vLLMì´ ìë™ìœ¼ë¡œ merge)
            self.llm = LLM(
                model=base_model_name,
                enable_lora=True,
                max_lora_rank=64,  # LoRA rank ì„¤ì • (adapterì— ë§ê²Œ ì¡°ì •)
                tensor_parallel_size=tensor_parallel_size,
                gpu_memory_utilization=gpu_memory_utilization,
                trust_remote_code=True,
            )
        else:
            # ê¸°ë³¸ ëª¨ë¸ë§Œ ë¡œë“œ
            self.llm = LLM(
                model=base_model_name,
                tensor_parallel_size=tensor_parallel_size,
                gpu_memory_utilization=gpu_memory_utilization,
                trust_remote_code=True,
            )

    def format_chat_messages(self, messages: List[Dict[str, str]]) -> str:
        """
        Chat ë©”ì‹œì§€ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Args:
            messages: [{'role': 'system/user/assistant', 'content': '...'}, ...]

        Returns:
            í¬ë§·íŒ…ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        if self.tokenizer.chat_template:
            # Tokenizerì— chat_templateì´ ìˆìœ¼ë©´ ì‚¬ìš©
            formatted_prompt = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
        else:
            # ì—†ìœ¼ë©´ ìˆ˜ë™ìœ¼ë¡œ í¬ë§·íŒ…
            formatted_prompt = ""
            for message in messages:
                role = message["role"]
                content = message["content"]

                if role == "system":
                    formatted_prompt += f"System: {content}\n\n"
                elif role == "user":
                    formatted_prompt += f"User: {content}\n\n"
                elif role == "assistant":
                    formatted_prompt += f"Assistant: {content}\n\n"

            # ìƒˆë¡œìš´ ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
            if messages[-1]["role"] != "assistant":
                formatted_prompt += "Assistant: "

        return formatted_prompt

    def generate(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50,
        repetition_penalty: float = 1.0,
        stop_tokens: List[str] = None,
    ) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            messages: Chat í˜•ì‹ì˜ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            top_p: Nucleus sampling íŒŒë¼ë¯¸í„°
            top_k: Top-k sampling íŒŒë¼ë¯¸í„°
            repetition_penalty: ë°˜ë³µ í˜ë„í‹°
            stop_tokens: ìƒì„± ì¤‘ë‹¨ í† í° ë¦¬ìŠ¤íŠ¸

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        # ë©”ì‹œì§€ í¬ë§·íŒ…
        prompt = self.format_chat_messages(messages)

        # Sampling íŒŒë¼ë¯¸í„° ì„¤ì •
        sampling_params = SamplingParams(
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=repetition_penalty,
            stop=stop_tokens if stop_tokens else [],
        )

        # LoRA adapter ì‚¬ìš© ì‹œ
        if self.lora_adapter_path:
            # vLLMì—ì„œ LoRA request ìƒì„±
            from vllm.lora.request import LoRARequest

            lora_request = LoRARequest("lora_adapter", 1, self.lora_adapter_path)

            outputs = self.llm.generate(
                [prompt], sampling_params, lora_request=lora_request
            )
        else:
            outputs = self.llm.generate([prompt], sampling_params)

        # ê²°ê³¼ ì¶”ì¶œ
        generated_text = outputs[0].outputs[0].text
        return generated_text

    def batch_generate(
        self,
        messages_list: List[List[Dict[str, str]]],
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50,
    ) -> List[str]:
        """
        ë°°ì¹˜ ì¶”ë¡ 

        Args:
            messages_list: Chat ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸
            ê¸°íƒ€ íŒŒë¼ë¯¸í„°: generate()ì™€ ë™ì¼

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        prompts = [self.format_chat_messages(msgs) for msgs in messages_list]

        sampling_params = SamplingParams(
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
        )

        if self.lora_adapter_path:
            from vllm.lora.request import LoRARequest

            lora_request = LoRARequest("lora_adapter", 1, self.lora_adapter_path)
            outputs = self.llm.generate(
                prompts, sampling_params, lora_request=lora_request
            )
        else:
            outputs = self.llm.generate(prompts, sampling_params)

        return [output.outputs[0].text for output in outputs]


class PerformanceBenchmark:
    """Transformers vs vLLM ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬"""

    def __init__(self, model_name: str):
        self.model_name = model_name

    def get_gpu_memory_mb(self) -> float:
        """í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024 / 1024
        return 0.0

    def benchmark_transformers(
        self,
        prompts: List[str],
        max_new_tokens: int = 128,
    ) -> Dict[str, float]:
        """
        Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì¶”ë¡  ì„±ëŠ¥ ì¸¡ì •

        Returns:
            ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        print("\n" + "=" * 60)
        print("ğŸ”µ Transformers ë²¤ì¹˜ë§ˆí¬")
        print("=" * 60)

        # ëª¨ë¸ ë¡œë“œ
        print("  ëª¨ë¸ ë¡œë”© ì¤‘...")
        start_load = time.time()

        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
        )
        model.eval()

        load_time = time.time() - start_load
        memory_after_load = self.get_gpu_memory_mb()

        print(f"  âœ… ë¡œë”© ì™„ë£Œ: {load_time:.2f}s, {memory_after_load:.0f} MB")

        # ì¶”ë¡  ì¸¡ì •
        print(f"\n  ì¶”ë¡  ì‹œì‘ ({len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸)...")

        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()

        total_inference_time = 0
        total_tokens = 0
        first_token_latencies = []

        for i, prompt in enumerate(prompts):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            input_ids = inputs.input_ids

            # First token ì¸¡ì •
            start_first = time.time()

            with torch.no_grad():
                # ì²« í† í° ìƒì„±
                outputs = model(input_ids)
                next_token_logits = outputs.logits[:, -1, :]
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

            first_token_latency = time.time() - start_first
            first_token_latencies.append(first_token_latency)

            # ì „ì²´ ìƒì„± (autoregressive)
            start_full = time.time()

            generated_ids = input_ids
            generated_count = 0

            for _ in range(max_new_tokens - 1):
                generated_ids = torch.cat([generated_ids, next_token], dim=1)
                generated_count += 1

                with torch.no_grad():
                    outputs = model(generated_ids)
                    next_token_logits = outputs.logits[:, -1, :]
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

                if next_token.item() == tokenizer.eos_token_id:
                    break

            inference_time = time.time() - start_full
            total_inference_time += inference_time
            total_tokens += generated_count

            if i == 0:
                decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                print(f"\n  [ìƒ˜í”Œ ì¶œë ¥] {decoded[:100]}...")

        peak_memory = self.get_gpu_memory_mb()
        avg_first_token_latency = sum(first_token_latencies) / len(
            first_token_latencies
        )
        tokens_per_sec = (
            total_tokens / total_inference_time if total_inference_time > 0 else 0
        )

        # ê²°ê³¼ ì¶œë ¥
        print(f"\n  âœ… ì¶”ë¡  ì™„ë£Œ")
        print(f"  ğŸ“Š First Token Latency: {avg_first_token_latency*1000:.2f}ms (í‰ê· )")
        print(f"  ğŸ“Š Token/sec: {tokens_per_sec:.2f}")
        print(f"  ğŸ“Š ì´ ì¶”ë¡  ì‹œê°„: {total_inference_time:.2f}s")
        print(f"  ğŸ“Š í”¼í¬ GPU ë©”ëª¨ë¦¬: {peak_memory:.0f} MB")

        # ì •ë¦¬
        del model
        del tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return {
            "load_time": load_time,
            "total_inference_time": total_inference_time,
            "avg_first_token_latency_ms": avg_first_token_latency * 1000,
            "tokens_per_sec": tokens_per_sec,
            "total_tokens": total_tokens,
            "peak_memory_mb": peak_memory,
            "throughput_prompts_per_sec": len(prompts) / total_inference_time,
        }

    def benchmark_vllm(
        self,
        prompts: List[str],
        max_tokens: int = 128,
    ) -> Dict[str, float]:
        """
        vLLMìœ¼ë¡œ ì¶”ë¡  ì„±ëŠ¥ ì¸¡ì •

        Returns:
            ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        print("\n" + "=" * 60)
        print("ğŸŸ¢ vLLM ë²¤ì¹˜ë§ˆí¬")
        print("=" * 60)

        # ëª¨ë¸ ë¡œë“œ
        print("  ëª¨ë¸ ë¡œë”© ì¤‘...")
        start_load = time.time()

        llm = LLM(
            model=self.model_name,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.9,
            trust_remote_code=True,
        )

        load_time = time.time() - start_load
        memory_after_load = self.get_gpu_memory_mb()

        print(f"  âœ… ë¡œë”© ì™„ë£Œ: {load_time:.2f}s, {memory_after_load:.0f} MB")

        # ì¶”ë¡  ì¸¡ì •
        print(f"\n  ì¶”ë¡  ì‹œì‘ ({len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸)...")

        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()

        sampling_params = SamplingParams(
            max_tokens=max_tokens,
            temperature=0.0,  # greedy decoding for fair comparison
        )

        # First token latency ì¸¡ì •ì„ ìœ„í•´ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ë¨¼ì € ì‹¤í–‰
        start_first = time.time()
        single_output = llm.generate([prompts[0]], sampling_params)
        first_token_latency = time.time() - start_first

        # ì „ì²´ ë°°ì¹˜ ì¶”ë¡ 
        start_inference = time.time()
        outputs = llm.generate(prompts, sampling_params)
        total_inference_time = time.time() - start_inference

        peak_memory = self.get_gpu_memory_mb()

        # í† í° í†µê³„
        total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)
        tokens_per_sec = (
            total_tokens / total_inference_time if total_inference_time > 0 else 0
        )

        # ìƒ˜í”Œ ì¶œë ¥
        if outputs:
            print(f"\n  [ìƒ˜í”Œ ì¶œë ¥] {outputs[0].outputs[0].text[:100]}...")

        # ê²°ê³¼ ì¶œë ¥
        print(f"\n  âœ… ì¶”ë¡  ì™„ë£Œ")
        print(f"  ğŸ“Š First Token Latency: {first_token_latency*1000:.2f}ms")
        print(f"  ğŸ“Š Token/sec: {tokens_per_sec:.2f}")
        print(f"  ğŸ“Š ì´ ì¶”ë¡  ì‹œê°„: {total_inference_time:.2f}s")
        print(f"  ğŸ“Š í”¼í¬ GPU ë©”ëª¨ë¦¬: {peak_memory:.0f} MB")

        return {
            "load_time": load_time,
            "total_inference_time": total_inference_time,
            "first_token_latency_ms": first_token_latency * 1000,
            "tokens_per_sec": tokens_per_sec,
            "total_tokens": total_tokens,
            "peak_memory_mb": peak_memory,
            "throughput_prompts_per_sec": len(prompts) / total_inference_time,
        }

    def compare(self, prompts: List[str], max_tokens: int = 128):
        """
        Transformersì™€ vLLM ì„±ëŠ¥ ë¹„êµ
        """
        print("\n" + "=" * 60)
        print("ğŸš€ Transformers vs vLLM ì„±ëŠ¥ ë¹„êµ")
        print("=" * 60)
        print(f"ëª¨ë¸: {self.model_name}")
        print(f"í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(prompts)}")
        print(f"Max Tokens: {max_tokens}")

        # Transformers ë²¤ì¹˜ë§ˆí¬
        tf_results = self.benchmark_transformers(prompts, max_tokens)

        # ë©”ëª¨ë¦¬ ì •ë¦¬ ëŒ€ê¸°
        time.sleep(3)

        # vLLM ë²¤ì¹˜ë§ˆí¬
        vllm_results = self.benchmark_vllm(prompts, max_tokens)

        # ë¹„êµ ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“Š ìµœì¢… ë¹„êµ ê²°ê³¼")
        print("=" * 60)

        print("\nâš¡ First Token Latency (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ):")
        tf_ttft = tf_results.get("avg_first_token_latency_ms", 0)
        vllm_ttft = vllm_results.get("first_token_latency_ms", 0)
        print(f"  Transformers: {tf_ttft:.2f}ms")
        print(f"  vLLM:         {vllm_ttft:.2f}ms")
        if vllm_ttft > 0:
            improvement = ((tf_ttft - vllm_ttft) / tf_ttft) * 100
            print(
                f"  {'ğŸš€ ê°œì„ ' if improvement > 0 else 'âš ï¸ ì°¨ì´'}: {abs(improvement):.1f}%"
            )

        print("\nğŸ”¥ Token/sec (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ):")
        print(f"  Transformers: {tf_results['tokens_per_sec']:.2f} tokens/sec")
        print(f"  vLLM:         {vllm_results['tokens_per_sec']:.2f} tokens/sec")
        speedup = (
            vllm_results["tokens_per_sec"] / tf_results["tokens_per_sec"]
            if tf_results["tokens_per_sec"] > 0
            else 0
        )
        print(f"  ğŸš€ vLLM í–¥ìƒ: {speedup:.2f}x")

        print("\nâ±ï¸ ì´ ì¶”ë¡  ì‹œê°„:")
        print(f"  Transformers: {tf_results['total_inference_time']:.2f}s")
        print(f"  vLLM:         {vllm_results['total_inference_time']:.2f}s")
        time_speedup = (
            tf_results["total_inference_time"] / vllm_results["total_inference_time"]
        )
        print(f"  ğŸš€ ì†ë„ í–¥ìƒ: {time_speedup:.2f}x")

        print("\nğŸ’¾ í”¼í¬ GPU ë©”ëª¨ë¦¬:")
        print(f"  Transformers: {tf_results['peak_memory_mb']:.0f} MB")
        print(f"  vLLM:         {vllm_results['peak_memory_mb']:.0f} MB")
        memory_diff = tf_results["peak_memory_mb"] - vllm_results["peak_memory_mb"]
        memory_saving_pct = (
            (memory_diff / tf_results["peak_memory_mb"]) * 100
            if tf_results["peak_memory_mb"] > 0
            else 0
        )
        print(
            f"  ğŸ’¡ ë©”ëª¨ë¦¬ {'ì ˆê°' if memory_diff > 0 else 'ì¦ê°€'}: {abs(memory_diff):.0f} MB ({abs(memory_saving_pct):.1f}%)"
        )

        print("\nğŸ¯ ì²˜ë¦¬ëŸ‰ (Throughput):")
        print(
            f"  Transformers: {tf_results['throughput_prompts_per_sec']:.2f} prompts/sec"
        )
        print(
            f"  vLLM:         {vllm_results['throughput_prompts_per_sec']:.2f} prompts/sec"
        )

        print("\n" + "=" * 60)

        return {
            "transformers": tf_results,
            "vllm": vllm_results,
        }


def run_benchmark():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"

    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    prompts = [
        "You are a SQL expert. Convert this to SQL: Find all users with age greater than 25",
        "You are a SQL expert. Convert this to SQL: Count total employees in sales department",
        "You are a SQL expert. Convert this to SQL: Show top 10 products by revenue",
        "You are a SQL expert. Convert this to SQL: Delete inactive user accounts",
        "You are a SQL expert. Convert this to SQL: Update email addresses for all admins",
    ]

    benchmark = PerformanceBenchmark(model_name)
    results = benchmark.compare(prompts, max_tokens=128)

    return results


def main_with_merged_model():
    """
    ë°©ë²• 1: LoRAë¥¼ ë¯¸ë¦¬ mergeí•œ ëª¨ë¸ë¡œ ì¶”ë¡ 
    - LoRA adapterë¥¼ base modelì— mergeí•˜ì—¬ í†µí•© ëª¨ë¸ ìƒì„±
    - vLLMì—ì„œ mergeëœ ëª¨ë¸ì„ ì§ì ‘ ë¡œë“œ
    """
    base_model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"
    lora_adapter_path = "./lora_adapter"
    merged_model_path = "./merged_model"

    # Step 1: LoRAë¥¼ base modelì— merge (í•œ ë²ˆë§Œ ì‹¤í–‰)
    if not os.path.exists(merged_model_path):
        if os.path.exists(lora_adapter_path):
            print("=" * 60)
            print("Step 1: LoRAë¥¼ Base Modelì— Merge")
            print("=" * 60)
            merge_lora_to_base_model(
                base_model_name=base_model_name,
                lora_adapter_path=lora_adapter_path,
                output_path=merged_model_path,
                save_tokenizer=True,
            )
        else:
            print(f"âš ï¸  LoRA adapterë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lora_adapter_path}")
            print("   Base modelë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n")
            merged_model_path = base_model_name
    else:
        print(f"âœ… ì´ë¯¸ mergeëœ ëª¨ë¸ì´ ì¡´ì¬í•©ë‹ˆë‹¤: {merged_model_path}\n")

    # Step 2: Mergeëœ ëª¨ë¸ë¡œ vLLM ì¶”ë¡ 
    print("=" * 60)
    print("Step 2: Merged Modelë¡œ vLLM ì¶”ë¡ ")
    print("=" * 60)
    print("ğŸš€ vLLM ëª¨ë¸ ë¡œë”© ì¤‘...")

    # Mergeëœ ëª¨ë¸ì„ base modelì²˜ëŸ¼ ì§ì ‘ ë¡œë“œ (LoRA path ì—†ì´)
    inferencer = VLLMInferenceWithLoRA(
        base_model_name=merged_model_path,  # mergeëœ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©
        lora_adapter_path=None,  # LoRAëŠ” ì´ë¯¸ mergeë˜ì—ˆìœ¼ë¯€ë¡œ None
        tensor_parallel_size=1,
        gpu_memory_utilization=0.9,
    )
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")

    # ì¶”ë¡  ì‹¤í–‰
    messages = [
        {
            "content": "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query.",
            "role": "system",
        },
        {
            "content": "Given the <USER_QUERY>, generate the corresponding SQL command to retrieve the desired data, considering the query's syntax, semantics, and schema constraints.\n\n<USER_QUERY>\nHow many heads of the departments are older than 56 ?\n</USER_QUERY>",
            "role": "user",
        },
    ]

    print("ğŸ“ ì…ë ¥ í”„ë¡¬í”„íŠ¸:")
    for msg in messages:
        print(f"  [{msg['role'].upper()}] {msg['content'][:100]}...")
    print()

    print("ğŸ¤– ì¶”ë¡  ì‹œì‘...")
    generated_text = inferencer.generate(
        messages=messages,
        max_tokens=256,
        temperature=0.1,
        top_p=0.95,
    )

    print("âœ¨ ìƒì„± ê²°ê³¼:")
    print(generated_text)
    print()


def main():
    """
    ë°©ë²• 2: vLLMì˜ ëŸ°íƒ€ì„ LoRA ê¸°ëŠ¥ ì‚¬ìš©
    - vLLMì´ ì¶”ë¡  ì‹œì ì— LoRAë¥¼ ë™ì ìœ¼ë¡œ ì ìš©
    """
    # LoRA adapter ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”)
    # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ ê¸°ë³¸ ëª¨ë¸ë§Œ ì‚¬ìš©
    lora_adapter_path = "./lora_adapter"  # ë˜ëŠ” None

    # ëª¨ë¸ ì´ˆê¸°í™”
    print("=" * 60)
    print("ë°©ë²• 2: vLLM Runtime LoRA")
    print("=" * 60)
    print("ğŸš€ vLLM ëª¨ë¸ ë¡œë”© ì¤‘...")
    inferencer = VLLMInferenceWithLoRA(
        base_model_name="naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B",
        lora_adapter_path=(
            lora_adapter_path if os.path.exists(lora_adapter_path or "") else None
        ),
        tensor_parallel_size=1,
        gpu_memory_utilization=0.9,
    )
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")

    # ì˜ˆì œ í”„ë¡¬í”„íŠ¸ (Text-to-SQL)
    messages = [
        {
            "content": "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query.",
            "role": "system",
        },
        {
            "content": "Given the <USER_QUERY>, generate the corresponding SQL command to retrieve the desired data, considering the query's syntax, semantics, and schema constraints.\n\n<USER_QUERY>\nHow many heads of the departments are older than 56 ?\n</USER_QUERY>",
            "role": "user",
        },
        {"content": "SELECT count(*) FROM head WHERE age  >  56", "role": "assistant"},
    ]

    print("ğŸ“ ì…ë ¥ í”„ë¡¬í”„íŠ¸:")
    for msg in messages:
        print(f"  [{msg['role'].upper()}] {msg['content'][:100]}...")
    print()

    # ì¶”ë¡  ì‹¤í–‰
    print("ğŸ¤– ì¶”ë¡  ì‹œì‘...")
    generated_text = inferencer.generate(
        messages=messages,
        max_tokens=256,
        temperature=0.1,  # SQL ìƒì„±ì´ë¯€ë¡œ ë‚®ì€ temperature ì‚¬ìš©
        top_p=0.95,
    )

    print("âœ¨ ìƒì„± ê²°ê³¼:")
    print(generated_text)
    print()

    # ì¶”ê°€ ì˜ˆì œ: ìƒˆë¡œìš´ ì§ˆì˜
    new_query_messages = [
        {
            "content": "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query.",
            "role": "system",
        },
        {
            "content": "Given the <USER_QUERY>, generate the corresponding SQL command to retrieve the desired data, considering the query's syntax, semantics, and schema constraints.\n\n<USER_QUERY>\nWhat are the names of all employees in the Sales department?\n</USER_QUERY>",
            "role": "user",
        },
    ]

    print("ğŸ“ ìƒˆë¡œìš´ ì§ˆì˜:")
    print(f"  [USER] {new_query_messages[1]['content'][:100]}...")
    print()

    print("ğŸ¤– ì¶”ë¡  ì‹œì‘...")
    new_generated_text = inferencer.generate(
        messages=new_query_messages,
        max_tokens=256,
        temperature=0.1,
    )

    print("âœ¨ ìƒì„± ê²°ê³¼:")
    print(new_generated_text)


if __name__ == "__main__":
    import sys

    print("\n" + "=" * 60)
    print("vLLM Offline Inference with LoRA")
    print("=" * 60)
    print("\nì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("  1) Merged Model ë°©ì‹ - LoRAë¥¼ ë¯¸ë¦¬ mergeí•œ ëª¨ë¸ ì‚¬ìš©")
    print("  2) Runtime LoRA ë°©ì‹ - vLLMì´ ì¶”ë¡  ì‹œ LoRA ë™ì  ì ìš©")
    print("  3) ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ - Transformers vs vLLM ë¹„êµ (ê¶Œì¥)")
    print("\n")

    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ìê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    if len(sys.argv) > 1:
        choice = sys.argv[1]
    else:
        choice = input("ì„ íƒ (1, 2, ë˜ëŠ” 3, ê¸°ë³¸ê°’=3): ").strip() or "3"

    print("\n")

    if choice == "1":
        main_with_merged_model()
    elif choice == "2":
        main()
    elif choice == "3":
        run_benchmark()
    else:
        print("âš ï¸  ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        run_benchmark()
