{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# vLLM ì™„ë²½ ê°€ì´ë“œ: ì´ë¡ ë¶€í„° ì‹¤ì „ê¹Œì§€\n",
                "\n",
                "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” vLLMì„ ì´ìš©í•œ íš¨ìœ¨ì ì¸ LLM ì¶”ë¡ ì„ ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
                "\n",
                "## í•™ìŠµ ëª©í‘œ\n",
                "1. âœ… vLLMì˜ í•µì‹¬ ê°œë… ì´í•´ (PagedAttention, KV Cache)\n",
                "2. âœ… vLLM ê¸°ë³¸ ì‚¬ìš©ë²• ì‹¤ìŠµ\n",
                "3. âœ… LoRA adapter í†µí•© ë°©ë²•\n",
                "4. âœ… Transformers vs vLLM ì„±ëŠ¥ ë¹„êµ\n",
                "5. âœ… ì‹¤ì „ ì‘ìš© (Text-to-SQL)\n",
                "\n",
                "## ì‚¬ì „ ì¤€ë¹„\n",
                "```bash\n",
                "# uv ì„¤ì¹˜ (í•„ìš”í•œ ê²½ìš°)\n",
                "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
                "\n",
                "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
                "uv pip install vllm==0.13.0 transformers torch peft\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 1: vLLM ê¸°ì´ˆ ê°œë…\n",
                "\n",
                "### 1.1 ì™œ vLLMì¸ê°€?\n",
                "\n",
                "ì¼ë°˜ì ì¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë¬¸ì œì :\n",
                "- **ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨**: KV Cacheë¥¼ ì—°ì†ëœ ë©”ëª¨ë¦¬ì— ì €ì¥ â†’ ë‹¨í¸í™” ë°œìƒ\n",
                "- **ëŠë¦° ì†ë„**: ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë¹„íš¨ìœ¨ì \n",
                "- **ë‚®ì€ ì²˜ë¦¬ëŸ‰**: ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ ì œí•œ\n",
                "\n",
                "vLLMì˜ í•´ê²°ì±…:\n",
                "- **PagedAttention**: OSì˜ ê°€ìƒ ë©”ëª¨ë¦¬ì²˜ëŸ¼ KV Cacheë¥¼ ë¸”ë¡ ë‹¨ìœ„ë¡œ ê´€ë¦¬\n",
                "- **ì—°ì† ë°°ì¹­**: ìš”ì²­ì„ ë™ì ìœ¼ë¡œ ë°°ì¹­í•˜ì—¬ ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”\n",
                "- **ìµœì í™”ëœ CUDA ì»¤ë„**: GPU í™œìš©ë„ í–¥ìƒ\n",
                "\n",
                "### 1.2 í•µì‹¬ ê°œë… 1: PagedAttention (ë°”ì¸ë” ë…¸íŠ¸ ë¹„ìœ )\n",
                "\n",
                "ì½”ë“œë¥¼ ë³´ê¸° ì „ì— ë”± í•˜ë‚˜ë§Œ ê¸°ì–µí•©ì‹œë‹¤.\n",
                "\n",
                "**ê¸°ì¡´ ë°©ì‹ (ìŠ¤í”„ë§ ë…¸íŠ¸)**:\n",
                "- í˜ì´ì§€ë¥¼ ì°¢ê±°ë‚˜ ì¤‘ê°„ì— ë¼ì›Œ ë„£ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\n",
                "- 10ì¥ì„ ì“¸ì§€ ëª¨ë¥´ê³  100ì¥ì§œë¦¬ ë…¸íŠ¸ë¥¼ ì‚¬ë©´ 90ì¥ì€ ë²„ë¦¬ëŠ” ê²ë‹ˆë‹¤\n",
                "- ì—°ì†ëœ ë©”ëª¨ë¦¬ í•„ìˆ˜ â†’ ë‚­ë¹„ ë°œìƒ\n",
                "\n",
                "```\n",
                "KV Cache: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  â† ë¯¸ë¦¬ í° ë©”ëª¨ë¦¬ í• ë‹¹, ë‚­ë¹„ ë°œìƒ\n",
                "```\n",
                "\n",
                "**PagedAttention (ë°”ì¸ë”/Loose-leaf ë…¸íŠ¸)**:\n",
                "- ë‚±ì¥(Block) ë‹¨ìœ„ë¡œ ì¢…ì´ë¥¼ êº¼ë‚´ì„œ ì•„ë¬´ ë°ë‚˜ ì² í•´ë‘¡ë‹ˆë‹¤\n",
                "- 'ëª©ì°¨(Block Table)'ì— ìˆœì„œë§Œ ì ì–´ë‘ë©´ ë©ë‹ˆë‹¤\n",
                "- ì¢…ì´ ë‚­ë¹„ê°€ ì—†ìŠµë‹ˆë‹¤!\n",
                "\n",
                "```\n",
                "Block 0: [â–ˆâ–ˆâ–ˆâ–ˆ]\n",
                "Block 3: [â–ˆâ–ˆâ–ˆâ–ˆ]  â† í•„ìš”í•  ë•Œë§Œ í• ë‹¹, ë¹„ì—°ì†ì  ì €ì¥ ê°€ëŠ¥\n",
                "Block 7: [â–ˆâ–ˆâ–‘â–‘]\n",
                "Block Table: [0, 3, 7]  â† ë§¤í•‘ í…Œì´ë¸”ë¡œ ê´€ë¦¬\n",
                "```\n",
                "\n",
                "### 1.3 í•µì‹¬ ê°œë… 2: KV Cache (ì¼ê¸°ì¥ ë¹„ìœ )\n",
                "\n",
                "**\"ë‚˜ëŠ” ì–´ì œ í•™êµì— ê°€ì„œ ë°¥ì„ ë¨¹ì—ˆë‹¤\"** ë¼ëŠ” ë¬¸ì¥ì„ ì“´ë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤.\n",
                "\n",
                "**KV Cacheê°€ ì—†ëŠ” ê²½ìš° (ì¹˜ë§¤ ê±¸ë¦° ì‘ê°€)**:\n",
                "1. \"ë‚˜ëŠ”\" (ì”€)\n",
                "2. (ê¸°ì–µ ë¦¬ì…‹) \"ë‚˜ëŠ”\" + \"ì–´ì œ\" (ë‹¤ì‹œ ì½ê³  ì”€)\n",
                "3. (ê¸°ì–µ ë¦¬ì…‹) \"ë‚˜ëŠ” ì–´ì œ\" + \"í•™êµì—\" (ë‹¤ì‹œ ì½ê³  ì”€)\n",
                "4. (ê¸°ì–µ ë¦¬ì…‹) \"ë‚˜ëŠ” ì–´ì œ í•™êµì—\" + \"ê°€ì„œ\" ...\n",
                "\n",
                "â†’ **ë¬¸ì œì **: ë‹¨ì–´ í•˜ë‚˜ ì“¸ ë•Œë§ˆë‹¤ ì²˜ìŒë¶€í„° ëê¹Œì§€ ë‹¤ì‹œ ê³„ì‚°. O(NÂ²) ë³µì¡ë„!\n",
                "\n",
                "**KV Cacheê°€ ìˆëŠ” ê²½ìš° (ìŠ¤ë§ˆíŠ¸í•œ ì‘ê°€)**:\n",
                "1. \"ë‚˜ëŠ”\" (ì”€. 'ë‚˜ëŠ”'ì— ëŒ€í•œ ì •ë³´ë¥¼ í¬ìŠ¤íŠ¸ì‡ì— ë©”ëª¨)\n",
                "2. \"ì–´ì œ\" (ì•„ê¹Œ ë©”ëª¨ í™•ì¸, 'ì–´ì œ' ì •ë³´ ë©”ëª¨)\n",
                "3. \"í•™êµì—\" (ë©”ëª¨ë“¤ í™•ì¸, 'í•™êµì—' ì •ë³´ ë©”ëª¨)\n",
                "\n",
                "â†’ **í•µì‹¬**: ê³¼ê±° ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ê³„ì‚°í•˜ì§€ ì•Šê³ , ì €ì¥ëœ **ë©”ëª¨(Key, Value)**ë§Œ ì°¸ì¡°. O(N) ë³µì¡ë„!\n",
                "\n",
                "**í•˜ì§€ë§Œ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤**:\n",
                "- KV Cacheê°€ ê³„ì† ì»¤ì§ â†’ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)\n",
                "- **ë°”ë¡œ ì—¬ê¸°ì„œ PagedAttentionì´ ë“±ì¥!**\n",
                "- \"ì–´ì°¨í”¼ ì»¤ì§ˆ `kv_cache`ë¼ë©´, ë¹ˆ ê³µê°„ ì—†ì´ ìª¼ê°œì„œ(Blocking) íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ì\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
                "import os\n",
                "import time\n",
                "import torch\n",
                "from vllm import LLM, SamplingParams\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")\n",
                "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
                "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 1.5: PagedAttention ì‹œë®¬ë ˆì´í„° ì‹¤ìŠµ\n",
                "\n",
                "ì´ë¡ ë§Œìœ¼ë¡œëŠ” ì´í•´ê°€ ì–´ë µìŠµë‹ˆë‹¤. íŒŒì´ì¬ìœ¼ë¡œ **PagedAttention ì‹œë®¬ë ˆì´í„°**ë¥¼ ë§Œë“¤ì–´ì„œ GPU ë©”ëª¨ë¦¬ í• ë‹¹ ê³¼ì •ì„ ì§ì ‘ ì²´í—˜í•´ë´…ì‹œë‹¤!\n",
                "\n",
                "### ì‹œë®¬ë ˆì´í„° êµ¬ì„±ìš”ì†Œ\n",
                "- **Logical Memory**: ì‚¬ìš©ìê°€ ë³´ëŠ” ë¬¸ì¥ (ì—°ì†ë¨)\n",
                "- **Physical Memory**: ì‹¤ì œ ë°ì´í„°ê°€ ì €ì¥ë˜ëŠ” ê³³ (ë’¤ì£½ë°•ì£½ ì„ì„)\n",
                "- **Block Table**: ì´ ë‘˜ì„ ì—°ê²°í•´ì£¼ëŠ” ì§€ë„"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "from typing import List, Dict, Optional\n",
                "\n",
                "class PhysicalBlock:\n",
                "    \"\"\"\n",
                "    ë¬¼ë¦¬ì ì¸ ë©”ëª¨ë¦¬ ë¸”ë¡ (GPUì˜ ì‘ì€ ì¡°ê°)\n",
                "    ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ í•˜ë‚˜ì˜ ë¸”ë¡ì— 4ê°œì˜ í† í°ì„ ì €ì¥í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤ (BLOCK_SIZE = 4).\n",
                "    \"\"\"\n",
                "    def __init__(self, block_id: int, block_size: int = 4):\n",
                "        self.block_id = block_id\n",
                "        self.size = block_size\n",
                "        self.data = [None] * block_size  # ì²˜ìŒì—” ë¹„ì–´ìˆìŒ\n",
                "        self.filled_count = 0\n",
                "\n",
                "    def append(self, token: str) -> bool:\n",
                "        if self.filled_count < self.size:\n",
                "            self.data[self.filled_count] = token\n",
                "            self.filled_count += 1\n",
                "            return True  # ì €ì¥ ì„±ê³µ\n",
                "        return False  # ê½‰ ì°¨ì„œ ì‹¤íŒ¨\n",
                "\n",
                "    def __repr__(self):\n",
                "        return f\"[Block {self.block_id}]: {self.data}\"\n",
                "\n",
                "class VLLMMemoryManager:\n",
                "    \"\"\"\n",
                "    vLLMì˜ í•µì‹¬ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì (OS ì—­í• )\n",
                "    \"\"\"\n",
                "    def __init__(self, total_blocks: int = 16, block_size: int = 4):\n",
                "        self.block_size = block_size\n",
                "        # 1. ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ ê³µê°„ ìƒì„± (GPU VRAM)\n",
                "        self.physical_memory = [PhysicalBlock(i, block_size) for i in range(total_blocks)]\n",
                "        \n",
                "        # 2. ì‚¬ìš© ê°€ëŠ¥í•œ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ (Free List)\n",
                "        # ì‹¤ì œë¡œëŠ” ë¬¼ë¦¬ì ìœ¼ë¡œ ë–¨ì–´ì ¸ ìˆì–´ë„ ìƒê´€ì—†ìŒì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì„ìŠµë‹ˆë‹¤.\n",
                "        self.free_blocks = list(range(total_blocks))\n",
                "        random.shuffle(self.free_blocks) \n",
                "        \n",
                "        # 3. ìš”ì²­ë³„ ë¸”ë¡ ë§¤í•‘ í…Œì´ë¸” (Request ID -> List[Physical Block IDs])\n",
                "        self.block_tables: Dict[str, List[int]] = {}\n",
                "\n",
                "    def allocate_block(self) -> Optional[int]:\n",
                "        \"\"\"ë¹ˆ ë¬¼ë¦¬ ë¸”ë¡ í•˜ë‚˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
                "        if not self.free_blocks:\n",
                "            return None # OOM (Out of Memory)\n",
                "        return self.free_blocks.pop()\n",
                "\n",
                "    def append_token(self, request_id: str, token: str):\n",
                "        \"\"\"\n",
                "        í•µì‹¬ ë¡œì§: í† í°ì„ ìƒì„±í•´ì„œ KV Cacheì— ì¶”ê°€í•˜ëŠ” ê³¼ì •\n",
                "        \"\"\"\n",
                "        # 1. í•´ë‹¹ ìš”ì²­ì˜ ë¸”ë¡ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±\n",
                "        if request_id not in self.block_tables:\n",
                "            print(f\"--- [New Request] '{request_id}' ì‹œì‘ ---\")\n",
                "            self.block_tables[request_id] = []\n",
                "\n",
                "        # 2. í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë§ˆì§€ë§‰ ë¬¼ë¦¬ ë¸”ë¡ í™•ì¸\n",
                "        current_block_id = None\n",
                "        if self.block_tables[request_id]:\n",
                "            current_block_id = self.block_tables[request_id][-1]\n",
                "        \n",
                "        # 3. ë¸”ë¡ì´ ì—†ê±°ë‚˜ ê½‰ ì°¼ìœ¼ë©´, ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ í• ë‹¹ (PagedAttentionì˜ í•µì‹¬!)\n",
                "        if current_block_id is None or \\\n",
                "           self.physical_memory[current_block_id].filled_count >= self.block_size:\n",
                "            \n",
                "            new_block_id = self.allocate_block()\n",
                "            if new_block_id is None:\n",
                "                raise Exception(\"GPU Memory Full!\")\n",
                "            \n",
                "            self.block_tables[request_id].append(new_block_id)\n",
                "            current_block_id = new_block_id\n",
                "            print(f\"ğŸ‘‰ [Alloc] ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ {current_block_id}ë²ˆ í• ë‹¹ë¨\")\n",
                "\n",
                "        # 4. ë¬¼ë¦¬ ë¸”ë¡ì— ë°ì´í„° ì €ì¥\n",
                "        self.physical_memory[current_block_id].append(token)\n",
                "        print(f\"   [Write] í† í° '{token}' -> ë¬¼ë¦¬ ë¸”ë¡ {current_block_id}ë²ˆì— ì €ì¥\")\n",
                "\n",
                "    def print_state(self, request_id: str):\n",
                "        \"\"\"í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ ì‹œê°í™”í•´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.\"\"\"\n",
                "        print(f\"\\nğŸ“Š [{request_id}] ì˜ PagedAttention ìƒíƒœ\")\n",
                "        table = self.block_tables.get(request_id, [])\n",
                "        \n",
                "        # 1. Logical View (ì‚¬ìš©ìê°€ ë³´ëŠ” ë¬¸ì¥)\n",
                "        logical_text = []\n",
                "        for block_id in table:\n",
                "            block = self.physical_memory[block_id]\n",
                "            logical_text.extend([t for t in block.data if t is not None])\n",
                "        print(f\"  1) ë…¼ë¦¬ì  ë·° (Logical): {logical_text}\")\n",
                "\n",
                "        # 2. Block Table (ë§¤í•‘ ì •ë³´)\n",
                "        print(f\"  2) ë¸”ë¡ í…Œì´ë¸” (Mapping): {table}\")\n",
                "\n",
                "        # 3. Physical View (ì‹¤ì œ ì €ì¥ ìœ„ì¹˜)\n",
                "        print(f\"  3) ë¬¼ë¦¬ì  ë·° (Physical):\")\n",
                "        for block_id in table:\n",
                "            print(f\"     {self.physical_memory[block_id]}\")\n",
                "        print(\"-\" * 50)\n",
                "\n",
                "print(\"âœ… PagedAttention ì‹œë®¬ë ˆì´í„° í´ë˜ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PagedAttention ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\n",
                "# LLMì´ í•œ í† í°ì”© ìƒì„±í•˜ëŠ” ìƒí™©ì„ í‰ë‚´ëƒ…ë‹ˆë‹¤\n",
                "\n",
                "# 1. ë§¤ë‹ˆì € ì´ˆê¸°í™” (Block Size = 4)\n",
                "vllm_manager = VLLMMemoryManager(total_blocks=10, block_size=4)\n",
                "\n",
                "# 2. ë¬¸ì¥ ìƒì„± ì‹œë®¬ë ˆì´ì…˜\n",
                "req_id = \"User_A\"\n",
                "tokens = [\"Deep\", \"Learn\", \"ing\", \"is\", \"very\", \"fun\", \"to\", \"study\"]\n",
                "\n",
                "print(\"ğŸš€ í† í° ìƒì„± ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\\n\")\n",
                "for token in tokens:\n",
                "    vllm_manager.append_token(req_id, token)\n",
                "\n",
                "# 3. ìµœì¢… ìƒíƒœ í™•ì¸\n",
                "vllm_manager.print_state(req_id)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ì‹¤í–‰ ê²°ê³¼ í•´ì„\n",
                "\n",
                "ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì¤‘ìš”í•œ í˜„ìƒì„ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
                "\n",
                "#### ğŸ”‘ í•µì‹¬ í¬ì¸íŠ¸ 3ê°€ì§€\n",
                "\n",
                "**1. ë¹„ì—°ì†ì„± (Non-contiguous)**\n",
                "- ë…¼ë¦¬ì ìœ¼ë¡œëŠ” `'is'` ë‹¤ìŒì— `'very'`ê°€ ì˜µë‹ˆë‹¤\n",
                "- í•˜ì§€ë§Œ ë¬¼ë¦¬ì ìœ¼ë¡œëŠ” ëœ¬ê¸ˆì—†ëŠ” ë¸”ë¡ ë²ˆí˜¸ì— ì €ì¥ë©ë‹ˆë‹¤\n",
                "- **ì˜ì˜**: ë©”ëª¨ë¦¬ì˜ ë¹ˆ ê³µê°„ì´ ì–´ë””ì— ìˆë“  ìƒê´€ì—†ì´ ì‚¬ìš© ê°€ëŠ¥ (ë©”ëª¨ë¦¬ ë‹¨í¸í™” í•´ê²°)\n",
                "\n",
                "**2. ë™ì  í• ë‹¹ (On-demand Allocation)**\n",
                "- ë¸”ë¡ì´ ê½‰ ì°° ë•Œ(4ê°œ í† í°)ë§Œ ìƒˆ ë¸”ë¡ í• ë‹¹\n",
                "- ë§Œì•½ ë¬¸ì¥ì´ `'Deep Learning is'`ì—ì„œ ëë‚¬ë‹¤ë©´? ë‘ ë²ˆì§¸ ë¸”ë¡ì€ ì•„ì˜ˆ í• ë‹¹ë˜ì§€ ì•ŠìŒ\n",
                "- **ì˜ì˜**: ë¯¸ë¦¬ ì¡ì•„ë‘” ê³µê°„ì˜ ë‚­ë¹„(Reserved slots waste)ê°€ 'Zero'\n",
                "\n",
                "**3. ë¸”ë¡ í…Œì´ë¸” (The Map)**\n",
                "- vLLMì€ `Block Table: [7, 2]` ê°™ì€ ì •ë³´ë§Œ ìœ ì§€\n",
                "- \"5ë²ˆì§¸ í† í°(`very`)ì„ ê°€ì ¸ì™€ë¼\" ìš”ì²­ ì‹œ:\n",
                "  - `5 // 4 = 1` (ì¸ë±ìŠ¤ 1ë²ˆ ë¸”ë¡ = Physical Block 2)\n",
                "  - `5 % 4 = 1` (ë¸”ë¡ ë‚´ 1ë²ˆì§¸ ì˜¤í”„ì…‹ = 'very')\n",
                "- **ì˜ì˜**: ì´ ê³„ì‚°ì´ ë§¤ìš° ë¹ ë¥´ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ ì €í•˜ ì—†ìŒ\n",
                "\n",
                "### ğŸ’¡ ì™œ ì´ê²Œ ì¤‘ìš”í•œê°€?\n",
                "\n",
                "PagedAttentionì´ GPU ë©”ëª¨ë¦¬ë¥¼ 'í…ŒíŠ¸ë¦¬ìŠ¤' í•˜ë“¯ì´ ë¹ˆí‹ˆì—†ì´ ì±„ì›Œ ë„£ìŠµë‹ˆë‹¤. ì´ê²ƒì´ vLLMì´ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë³´ë‹¤ **ì••ë„ì ìœ¼ë¡œ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì´ìœ **ì…ë‹ˆë‹¤!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 2: vLLM ê¸°ë³¸ ì‚¬ìš©ë²•\n",
                "\n",
                "### 2.1 ëª¨ë¸ ë¡œë”©"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: vLLM ëª¨ë¸ ì´ˆê¸°í™”\n",
                "model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B\"\n",
                "\n",
                "print(\"ğŸš€ vLLM ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "llm = LLM(\n",
                "    model=model_name,\n",
                "    tensor_parallel_size=1,       # ë‹¨ì¼ GPU ì‚¬ìš©\n",
                "    gpu_memory_utilization=0.9,   # GPU ë©”ëª¨ë¦¬ 90% ì‚¬ìš©\n",
                "    trust_remote_code=True,       # ì»¤ìŠ¤í…€ ëª¨ë¸ ì½”ë“œ ì‹ ë¢°\n",
                ")\n",
                "\n",
                "load_time = time.time() - start_time\n",
                "print(f\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ({load_time:.2f}ì´ˆ)\")\n",
                "\n",
                "# Tokenizer ë¡œë“œ\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "print(f\"ğŸ“ Tokenizer ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 ê°„ë‹¨í•œ ì¶”ë¡  ì‹¤ìŠµ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2: Sampling íŒŒë¼ë¯¸í„° ì„¤ì •\n",
                "sampling_params = SamplingParams(\n",
                "    max_tokens=128,        # ìµœëŒ€ ìƒì„± í† í° ìˆ˜\n",
                "    temperature=0.7,       # ìƒ˜í”Œë§ ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€)\n",
                "    top_p=0.9,            # Nucleus sampling\n",
                "    top_k=50,             # Top-k sampling\n",
                ")\n",
                "\n",
                "print(\"âš™ï¸  Sampling íŒŒë¼ë¯¸í„°:\")\n",
                "print(f\"  - Max Tokens: {sampling_params.max_tokens}\")\n",
                "print(f\"  - Temperature: {sampling_params.temperature}\")\n",
                "print(f\"  - Top-p: {sampling_params.top_p}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3: ì¶”ë¡  ì‹¤í–‰\n",
                "prompts = [\n",
                "    \"Explain quantum computing in simple terms:\",\n",
                "    \"What is the capital of France?\",\n",
                "    \"Write a haiku about programming:\",\n",
                "]\n",
                "\n",
                "print(\"ğŸ¤– ì¶”ë¡  ì‹œì‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "outputs = llm.generate(prompts, sampling_params)\n",
                "\n",
                "inference_time = time.time() - start_time\n",
                "\n",
                "print(f\"\\nâœ… ì¶”ë¡  ì™„ë£Œ! ({inference_time:.2f}ì´ˆ)\")\n",
                "print(f\"ğŸ“Š ì²˜ë¦¬ëŸ‰: {len(prompts) / inference_time:.2f} prompts/sec\\n\")\n",
                "\n",
                "# ê²°ê³¼ ì¶œë ¥\n",
                "for i, output in enumerate(outputs):\n",
                "    generated_text = output.outputs[0].text\n",
                "    token_count = len(output.outputs[0].token_ids)\n",
                "    \n",
                "    print(f\"[í”„ë¡¬í”„íŠ¸ {i+1}] {prompts[i]}\")\n",
                "    print(f\"[ìƒì„± ê²°ê³¼] {generated_text}\")\n",
                "    print(f\"[í† í° ìˆ˜] {token_count}\")\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2.5: KV Cache ì„±ëŠ¥ ë¹„êµ ì‹œë®¬ë ˆì´í„°\n",
                "\n",
                "ì´ì œ **ì™œ KV Cacheê°€ í•„ìš”í•œì§€** ì—°ì‚° íšŸìˆ˜ë¥¼ ì¹´ìš´íŒ…í•˜ì—¬ ì§ì ‘ í™•ì¸í•´ë´…ì‹œë‹¤!\n",
                "\n",
                "ì´ ì‹œë®¬ë ˆì´í„°ëŠ” ì‹¤ì œ í–‰ë ¬ ì—°ì‚° ëŒ€ì‹ , **'ì—°ì‚°ì„ ëª‡ ë²ˆ ìˆ˜í–‰í–ˆëŠ”ê°€'**ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ModelSimulator:\n",
                "    \"\"\"LLM ì¶”ë¡  ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
                "    def __init__(self):\n",
                "        self.op_count = 0  # ì—°ì‚° íšŸìˆ˜ ì¹´ìš´í„°\n",
                "\n",
                "    def compute_token_representation(self, token):\n",
                "        \"\"\"\n",
                "        í† í°ì„ ë²¡í„°ë¡œ ë³€í™˜ (Embedding + Q, K, V ìƒì„±)\n",
                "        ê³ ë¹„ìš© ì—°ì‚°ì´ë¼ê³  ê°€ì •\n",
                "        \"\"\"\n",
                "        self.op_count += 1\n",
                "        return f\"Vector({token})\"\n",
                "\n",
                "    def attention_calculation(self, current_token, past_vectors):\n",
                "        \"\"\"\n",
                "        í˜„ì¬ í† í°ê³¼ ê³¼ê±° í† í°ë“¤ ê°„ì˜ ê´€ê³„ ê³„ì‚° (Attention)\n",
                "        \"\"\"\n",
                "        self.op_count += len(past_vectors)\n",
                "\n",
                "print(\"âœ… ModelSimulator ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# ì‹œë‚˜ë¦¬ì˜¤ 1: KV Cache ì—†ì´ ìƒì„± (No Cache)\n",
                "# ==========================================\n",
                "print(\"ğŸš« [Scenario 1] KV Cache ë¯¸ì‚¬ìš© (ë§¤ë²ˆ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ê³„ì‚°)\")\n",
                "model_no_cache = ModelSimulator()\n",
                "generated_tokens = []\n",
                "input_prompt = [\"Deep\", \"Learning\", \"is\", \"Fun\"]\n",
                "\n",
                "for step in range(5):\n",
                "    current_context = input_prompt + generated_tokens\n",
                "    \n",
                "    # ë§¤ë²ˆ ëª¨ë“  í† í°ì„ ë‹¤ì‹œ ê³„ì‚° (ë¹„íš¨ìœ¨!)\n",
                "    vectors = []\n",
                "    for token in current_context:\n",
                "        vec = model_no_cache.compute_token_representation(token)\n",
                "        vectors.append(vec)\n",
                "    \n",
                "    model_no_cache.attention_calculation(\"New_Token\", vectors)\n",
                "    \n",
                "    generated_tokens.append(f\"Gen_{step}\")\n",
                "    print(f\"  Step {step+1}: ë¬¸ì¥ ê¸¸ì´ {len(current_context)} -> ëˆ„ì  ì—°ì‚°: {model_no_cache.op_count}\")\n",
                "\n",
                "final_cost_no_cache = model_no_cache.op_count\n",
                "print(f\"\\nì´ ì—°ì‚° íšŸìˆ˜: {final_cost_no_cache}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# ì‹œë‚˜ë¦¬ì˜¤ 2: KV Cache ì‚¬ìš© (With Cache)\n",
                "# ==========================================\n",
                "print(\"\\nâœ… [Scenario 2] KV Cache ì‚¬ìš© (ê³¼ê±°ëŠ” ì €ì¥í•´ë‘ê³  ì¬ì‚¬ìš©)\")\n",
                "model_with_cache = ModelSimulator()\n",
                "generated_tokens = []\n",
                "kv_cache = []  # ì´ê²ƒì´ ë°”ë¡œ KV Cache!\n",
                "\n",
                "# Prefill: ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ (í•œ ë²ˆë§Œ)\n",
                "print(\"  [Prefill] ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì¤‘...\")\n",
                "for token in input_prompt:\n",
                "    vec = model_with_cache.compute_token_representation(token)\n",
                "    kv_cache.append(vec)\n",
                "\n",
                "# Decode: í† í° ìƒì„±\n",
                "for step in range(5):\n",
                "    # ìƒˆë¡œìš´ í† í° í•˜ë‚˜ë§Œ ì²˜ë¦¬!\n",
                "    current_vec = model_with_cache.compute_token_representation(\"New_Token\")\n",
                "    \n",
                "    # ê³¼ê±° cache + í˜„ì¬ ë²¡í„°ë¡œ attention\n",
                "    model_with_cache.attention_calculation(\"New_Token\", kv_cache)\n",
                "    \n",
                "    # ìºì‹œì— ì €ì¥\n",
                "    kv_cache.append(current_vec)\n",
                "    generated_tokens.append(f\"Gen_{step}\")\n",
                "    \n",
                "    print(f\"  Step {step+1}: ìºì‹œ í¬ê¸° {len(kv_cache)} -> ëˆ„ì  ì—°ì‚°: {model_with_cache.op_count}\")\n",
                "\n",
                "final_cost_with_cache = model_with_cache.op_count\n",
                "print(f\"\\nì´ ì—°ì‚° íšŸìˆ˜: {final_cost_with_cache}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ìµœì¢… ë¹„êµ\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"ğŸ“Š ìµœì¢… ì—°ì‚° ë¹„ìš© ë¹„êµ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„)\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"1. No Cache Cost  : {final_cost_no_cache} (O(NÂ²) - ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€)\")\n",
                "print(f\"2. With Cache Cost: {final_cost_with_cache} (O(N) - ì„ í˜•ì  ì¦ê°€)\")\n",
                "print(f\"ğŸš€ íš¨ìœ¨ í–¥ìƒ: ì•½ {final_cost_no_cache / final_cost_with_cache:.1f}ë°° ë” ì ì€ ì—°ì‚°!\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ê²°ê³¼ í•´ì„\n",
                "\n",
                "**KV Cache**ê°€ ìˆê³  ì—†ê³ ì˜ ì°¨ì´ê°€ ì—„ì²­ë‚©ë‹ˆë‹¤!\n",
                "\n",
                "- **No Cache**: O(NÂ²) ë¹„ìš©. ë¬¸ì¥ì´ ê¸¸ì–´ì§€ë©´ ëŠë ¤í„°ì§\n",
                "- **KV Cache**: O(N) ë¹„ìš©. ëŒ€ì‹  ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì”€ (ë©”ëª¨ë¦¬ â†” ì†ë„ êµí™˜)\n",
                "- **vLLM**: KV Cacheë¥¼ OSì²˜ëŸ¼ ê´€ë¦¬í•´ì„œ ë©”ëª¨ë¦¬ ë‚­ë¹„ë¥¼ ì¤„ì´ê³ , ë” ë§ì€ KV Cacheë¥¼ ì‘¤ì…” ë„£ìŒ!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 3: Chat í˜•ì‹ í”„ë¡¬í”„íŠ¸\n",
                "\n",
                "ì‹¤ì „ì—ì„œëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ìœ ì € ë©”ì‹œì§€, ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ ì¡°í•©í•œ chat í˜•ì‹ì„ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_chat_messages(messages):\n",
                "    \"\"\"\n",
                "    Chat ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜\n",
                "    \n",
                "    Args:\n",
                "        messages: [{'role': 'system/user/assistant', 'content': '...'}, ...]\n",
                "    \"\"\"\n",
                "    if tokenizer.chat_template:\n",
                "        # Tokenizerì— chat templateì´ ìˆìœ¼ë©´ ì‚¬ìš©\n",
                "        return tokenizer.apply_chat_template(\n",
                "            messages,\n",
                "            tokenize=False,\n",
                "            add_generation_prompt=True\n",
                "        )\n",
                "    else:\n",
                "        # ìˆ˜ë™ í¬ë§·íŒ…\n",
                "        formatted = \"\"\n",
                "        for msg in messages:\n",
                "            role = msg['role'].capitalize()\n",
                "            content = msg['content']\n",
                "            formatted += f\"{role}: {content}\\n\\n\"\n",
                "        \n",
                "        if messages[-1]['role'] != 'assistant':\n",
                "            formatted += \"Assistant: \"\n",
                "        \n",
                "        return formatted\n",
                "\n",
                "print(\"âœ… Chat í¬ë§·íŒ… í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Text-to-SQL ì˜ˆì œ\n",
                "messages = [\n",
                "    {\n",
                "        'role': 'system',\n",
                "        'content': 'You are a SQL expert. Convert natural language queries to SQL.'\n",
                "    },\n",
                "    {\n",
                "        'role': 'user',\n",
                "        'content': 'Find all users with age greater than 30'\n",
                "    }\n",
                "]\n",
                "\n",
                "# í”„ë¡¬í”„íŠ¸ ë³€í™˜\n",
                "prompt = format_chat_messages(messages)\n",
                "print(\"ğŸ“ ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸:\")\n",
                "print(prompt)\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# ì¶”ë¡ \n",
                "sampling_params_sql = SamplingParams(\n",
                "    max_tokens=64,\n",
                "    temperature=0.1,  # SQL ìƒì„±ì€ ë‚®ì€ temperature ì‚¬ìš©\n",
                ")\n",
                "\n",
                "outputs = llm.generate([prompt], sampling_params_sql)\n",
                "print(\"\\nğŸ’¡ ìƒì„±ëœ SQL:\")\n",
                "print(outputs[0].outputs[0].text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 4: LoRA Adapter ì‚¬ìš©í•˜ê¸°\n",
                "\n",
                "### 4.1 LoRAë€?\n",
                "\n",
                "**LoRA (Low-Rank Adaptation)**:\n",
                "- ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ fine-tuningí•˜ëŠ” ëŒ€ì‹ , ì‘ì€ adapterë§Œ í•™ìŠµ\n",
                "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥\n",
                "- ì—¬ëŸ¬ taskë³„ adapterë¥¼ ì‰½ê²Œ êµì²´ ê°€ëŠ¥\n",
                "\n",
                "### 4.2 vLLMì—ì„œ LoRA ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•\n",
                "\n",
                "#### ë°©ë²• 1: Runtime LoRA (ë™ì  ì ìš©)\n",
                "- ì¶”ë¡  ì‹œì ì— adapterë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ\n",
                "- ì—¬ëŸ¬ adapterë¥¼ ë¹ ë¥´ê²Œ êµì²´ ê°€ëŠ¥"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Runtime LoRA ì˜ˆì œ (adapterê°€ ìˆëŠ” ê²½ìš°)\n",
                "lora_adapter_path = \"./lora_adapter\"  # ì‹¤ì œ adapter ê²½ë¡œë¡œ ë³€ê²½\n",
                "\n",
                "if os.path.exists(lora_adapter_path):\n",
                "    from vllm.lora.request import LoRARequest\n",
                "    \n",
                "    # LoRA ì§€ì› ëª¨ë¸ ë¡œë“œ\n",
                "    llm_with_lora = LLM(\n",
                "        model=model_name,\n",
                "        enable_lora=True,\n",
                "        max_lora_rank=64,\n",
                "        tensor_parallel_size=1,\n",
                "        gpu_memory_utilization=0.9,\n",
                "        trust_remote_code=True,\n",
                "    )\n",
                "    \n",
                "    # LoRA request ìƒì„±\n",
                "    lora_request = LoRARequest(\"my_adapter\", 1, lora_adapter_path)\n",
                "    \n",
                "    # ì¶”ë¡  (LoRA adapter ì ìš©)\n",
                "    outputs = llm_with_lora.generate(\n",
                "        [prompt],\n",
                "        sampling_params,\n",
                "        lora_request=lora_request\n",
                "    )\n",
                "    \n",
                "    print(\"âœ… LoRA adapter ì ìš© ì™„ë£Œ\")\n",
                "else:\n",
                "    print(\"âš ï¸ LoRA adapter ì—†ìŒ - ê±´ë„ˆë›°ê¸°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### ë°©ë²• 2: Merged Model (ì‚¬ì „ í†µí•©)\n",
                "- LoRA weightsë¥¼ base modelì— ë¯¸ë¦¬ merge\n",
                "- ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„\n",
                "- ë°°í¬ì— ìœ ë¦¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM\n",
                "from peft import PeftModel\n",
                "\n",
                "def merge_lora_to_base(base_model_name, lora_path, output_path):\n",
                "    \"\"\"\n",
                "    LoRA adapterë¥¼ base modelì— merge\n",
                "    \"\"\"\n",
                "    print(\"ğŸ”„ LoRA merge ì‹œì‘...\")\n",
                "    \n",
                "    # 1. Base model ë¡œë“œ\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\n",
                "        base_model_name,\n",
                "        trust_remote_code=True,\n",
                "        torch_dtype=\"auto\"\n",
                "    )\n",
                "    \n",
                "    # 2. LoRA adapter ë¡œë“œ\n",
                "    model_with_lora = PeftModel.from_pretrained(base_model, lora_path)\n",
                "    \n",
                "    # 3. Merge\n",
                "    merged_model = model_with_lora.merge_and_unload()\n",
                "    \n",
                "    # 4. ì €ì¥\n",
                "    os.makedirs(output_path, exist_ok=True)\n",
                "    merged_model.save_pretrained(output_path, safe_serialization=True)\n",
                "    \n",
                "    # Tokenizerë„ ì €ì¥\n",
                "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
                "    tokenizer.save_pretrained(output_path)\n",
                "    \n",
                "    print(f\"âœ… Merge ì™„ë£Œ: {output_path}\")\n",
                "    return output_path\n",
                "\n",
                "# ì˜ˆì œ (adapterê°€ ìˆëŠ” ê²½ìš°)\n",
                "if os.path.exists(lora_adapter_path):\n",
                "    merged_path = merge_lora_to_base(\n",
                "        base_model_name=model_name,\n",
                "        lora_path=lora_adapter_path,\n",
                "        output_path=\"./merged_model\"\n",
                "    )\n",
                "    \n",
                "    # Merged ëª¨ë¸ë¡œ ì¶”ë¡ \n",
                "    llm_merged = LLM(\n",
                "        model=merged_path,\n",
                "        tensor_parallel_size=1,\n",
                "        gpu_memory_utilization=0.9,\n",
                "        trust_remote_code=True,\n",
                "    )\n",
                "    print(\"âœ… Merged ëª¨ë¸ë¡œ ì¶”ë¡  ì¤€ë¹„ ì™„ë£Œ\")\n",
                "else:\n",
                "    print(\"âš ï¸ LoRA adapter ì—†ìŒ - ê±´ë„ˆë›°ê¸°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 5: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ - Transformers vs vLLM\n",
                "\n",
                "vLLMì´ ì–¼ë§ˆë‚˜ ë¹ ë¥¸ì§€ ì§ì ‘ ì¸¡ì •í•´ë´…ì‹œë‹¤!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë²¤ì¹˜ë§ˆí¬ ì¤€ë¹„\n",
                "test_prompts = [\n",
                "    \"You are a SQL expert. Convert this to SQL: Find all users\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Count employees\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Show top 10 sales\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Delete inactive accounts\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Update user emails\",\n",
                "]\n",
                "\n",
                "max_tokens = 64\n",
                "\n",
                "print(f\"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì„¤ì •\")\n",
                "print(f\"  - í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(test_prompts)}\")\n",
                "print(f\"  - Max Tokens: {max_tokens}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Transformers ë²¤ì¹˜ë§ˆí¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "print(\"ğŸ”µ Transformers ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\")\n",
                "print(\"  ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
                "\n",
                "# ëª¨ë¸ ë¡œë“œ\n",
                "tf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tf_model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "tf_model.eval()\n",
                "\n",
                "# GPU ë©”ëª¨ë¦¬ ë¦¬ì…‹\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.reset_peak_memory_stats()\n",
                "\n",
                "# ì¶”ë¡ \n",
                "tf_start = time.time()\n",
                "tf_total_tokens = 0\n",
                "tf_first_token_latencies = []\n",
                "\n",
                "for prompt in test_prompts:\n",
                "    inputs = tf_tokenizer(prompt, return_tensors=\"pt\").to(tf_model.device)\n",
                "    \n",
                "    # First token ì¸¡ì •\n",
                "    ft_start = time.time()\n",
                "    with torch.no_grad():\n",
                "        outputs = tf_model(**inputs)\n",
                "    first_token_time = time.time() - ft_start\n",
                "    tf_first_token_latencies.append(first_token_time)\n",
                "    \n",
                "    # ì „ì²´ ìƒì„±\n",
                "    with torch.no_grad():\n",
                "        generated = tf_model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_tokens,\n",
                "            do_sample=False,\n",
                "        )\n",
                "    tf_total_tokens += generated.shape[1] - inputs.input_ids.shape[1]\n",
                "\n",
                "tf_time = time.time() - tf_start\n",
                "tf_peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0\n",
                "tf_avg_first_token = sum(tf_first_token_latencies) / len(tf_first_token_latencies)\n",
                "\n",
                "print(f\"\\nâœ… Transformers ì™„ë£Œ\")\n",
                "print(f\"  â±ï¸  ì´ ì‹œê°„: {tf_time:.2f}s\")\n",
                "print(f\"  âš¡ First Token: {tf_avg_first_token*1000:.2f}ms\")\n",
                "print(f\"  ğŸ”¥ Token/sec: {tf_total_tokens / tf_time:.2f}\")\n",
                "print(f\"  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: {tf_peak_memory:.0f} MB\")\n",
                "\n",
                "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
                "del tf_model\n",
                "del tf_tokenizer\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 vLLM ë²¤ì¹˜ë§ˆí¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nğŸŸ¢ vLLM ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\")\n",
                "\n",
                "# GPU ë©”ëª¨ë¦¬ ë¦¬ì…‹\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.reset_peak_memory_stats()\n",
                "\n",
                "# Sampling íŒŒë¼ë¯¸í„°\n",
                "benchmark_params = SamplingParams(\n",
                "    max_tokens=max_tokens,\n",
                "    temperature=0.0,  # greedy decoding\n",
                ")\n",
                "\n",
                "# First token latency ì¸¡ì •\n",
                "vllm_ft_start = time.time()\n",
                "_ = llm.generate([test_prompts[0]], benchmark_params)\n",
                "vllm_first_token = time.time() - vllm_ft_start\n",
                "\n",
                "# ì „ì²´ ì¶”ë¡ \n",
                "vllm_start = time.time()\n",
                "vllm_outputs = llm.generate(test_prompts, benchmark_params)\n",
                "vllm_time = time.time() - vllm_start\n",
                "\n",
                "vllm_peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0\n",
                "vllm_total_tokens = sum(len(out.outputs[0].token_ids) for out in vllm_outputs)\n",
                "\n",
                "print(f\"\\nâœ… vLLM ì™„ë£Œ\")\n",
                "print(f\"  â±ï¸  ì´ ì‹œê°„: {vllm_time:.2f}s\")\n",
                "print(f\"  âš¡ First Token: {vllm_first_token*1000:.2f}ms\")\n",
                "print(f\"  ğŸ”¥ Token/sec: {vllm_total_tokens / vllm_time:.2f}\")\n",
                "print(f\"  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: {vllm_peak_memory:.0f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 ê²°ê³¼ ë¹„êµ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"ğŸ“Š ìµœì¢… ë¹„êµ ê²°ê³¼\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# First Token Latency\n",
                "print(f\"\\nâš¡ First Token Latency:\")\n",
                "print(f\"  Transformers: {tf_avg_first_token*1000:.2f}ms\")\n",
                "print(f\"  vLLM:         {vllm_first_token*1000:.2f}ms\")\n",
                "ft_improvement = ((tf_avg_first_token - vllm_first_token) / tf_avg_first_token) * 100\n",
                "print(f\"  {'ğŸš€ ê°œì„ ' if ft_improvement > 0 else 'âš ï¸ ì°¨ì´'}: {abs(ft_improvement):.1f}%\")\n",
                "\n",
                "# Token/sec\n",
                "tf_tps = tf_total_tokens / tf_time\n",
                "vllm_tps = vllm_total_tokens / vllm_time\n",
                "print(f\"\\nğŸ”¥ Token/sec:\")\n",
                "print(f\"  Transformers: {tf_tps:.2f} tokens/sec\")\n",
                "print(f\"  vLLM:         {vllm_tps:.2f} tokens/sec\")\n",
                "print(f\"  ğŸš€ vLLM í–¥ìƒ: {vllm_tps / tf_tps:.2f}x\")\n",
                "\n",
                "# ì´ ì‹œê°„\n",
                "print(f\"\\nâ±ï¸ ì´ ì¶”ë¡  ì‹œê°„:\")\n",
                "print(f\"  Transformers: {tf_time:.2f}s\")\n",
                "print(f\"  vLLM:         {vllm_time:.2f}s\")\n",
                "print(f\"  ğŸš€ ì†ë„ í–¥ìƒ: {tf_time / vllm_time:.2f}x\")\n",
                "\n",
                "# ë©”ëª¨ë¦¬\n",
                "print(f\"\\nğŸ’¾ í”¼í¬ GPU ë©”ëª¨ë¦¬:\")\n",
                "print(f\"  Transformers: {tf_peak_memory:.0f} MB\")\n",
                "print(f\"  vLLM:         {vllm_peak_memory:.0f} MB\")\n",
                "memory_diff = tf_peak_memory - vllm_peak_memory\n",
                "if memory_diff > 0:\n",
                "    print(f\"  ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆê°: {memory_diff:.0f} MB ({memory_diff/tf_peak_memory*100:.1f}%)\")\n",
                "else:\n",
                "    print(f\"  ğŸ“ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©: {abs(memory_diff):.0f} MB ë” ì‚¬ìš©\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 6: ì¢…í•© ì‹¤ìŠµ\n",
                "\n",
                "### ìµœì¢… í”„ë¡œì íŠ¸: Text-to-SQL ì‹œìŠ¤í…œ êµ¬ì¶•\n",
                "\n",
                "ë°°ìš´ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì‹¤ì „ Text-to-SQL ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TextToSQLSystem:\n",
                "    def __init__(self, model_name):\n",
                "        self.llm = LLM(\n",
                "            model=model_name,\n",
                "            tensor_parallel_size=1,\n",
                "            gpu_memory_utilization=0.9,\n",
                "            trust_remote_code=True,\n",
                "        )\n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "        \n",
                "        self.system_prompt = \"\"\"\n",
                "You are an expert SQL query generator. Convert natural language questions to SQL queries.\n",
                "Generate only the SQL query without explanations.\n",
                "\"\"\"\n",
                "    \n",
                "    def generate_sql(self, question: str, schema_info: str = \"\") -> str:\n",
                "        \"\"\"ìì—°ì–´ ì§ˆì˜ë¥¼ SQLë¡œ ë³€í™˜\"\"\"\n",
                "        \n",
                "        user_content = f\"\"\"\n",
                "Database Schema:\n",
                "{schema_info if schema_info else 'Standard database tables'}\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "SQL Query:\n",
                "\"\"\"\n",
                "        \n",
                "        messages = [\n",
                "            {'role': 'system', 'content': self.system_prompt},\n",
                "            {'role': 'user', 'content': user_content}\n",
                "        ]\n",
                "        \n",
                "        prompt = format_chat_messages(messages)\n",
                "        \n",
                "        sampling_params = SamplingParams(\n",
                "            max_tokens=128,\n",
                "            temperature=0.1,\n",
                "            stop=[\";\", \"\\n\\n\"]\n",
                "        )\n",
                "        \n",
                "        outputs = self.llm.generate([prompt], sampling_params)\n",
                "        sql_query = outputs[0].outputs[0].text.strip()\n",
                "        \n",
                "        return sql_query\n",
                "\n",
                "# ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
                "sql_system = TextToSQLSystem(model_name)\n",
                "print(\"âœ… Text-to-SQL ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# í…ŒìŠ¤íŠ¸\n",
                "schema = \"\"\"\n",
                "Tables:\n",
                "- users (id, name, email, age, department_id)\n",
                "- departments (id, name, budget)\n",
                "- orders (id, user_id, product_id, amount, order_date)\n",
                "\"\"\"\n",
                "\n",
                "test_questions = [\n",
                "    \"Find all users older than 30\",\n",
                "    \"Count employees in each department\",\n",
                "    \"Show top 5 users by order amount\",\n",
                "    \"List departments with budget over 100000\",\n",
                "]\n",
                "\n",
                "print(\"ğŸ§ª Text-to-SQL í…ŒìŠ¤íŠ¸\\n\")\n",
                "\n",
                "for question in test_questions:\n",
                "    print(f\"â“ Question: {question}\")\n",
                "    sql = sql_system.generate_sql(question, schema)\n",
                "    print(f\"ğŸ’¡ Generated SQL: {sql}\")\n",
                "    print(\"-\" * 60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
